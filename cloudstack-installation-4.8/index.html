<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Apache CloudStack 安装文档 4.8.0 </title>
    
    <link rel="stylesheet" href="./_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="./_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="./_static/css/badge_only.css" type="text/css" />
    <link rel="stylesheet" href="./_static/bootswatch-3.2.0/spacelab/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="./_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '4.8.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="./_static/jquery.js"></script>
    <script type="text/javascript" src="./_static/underscore.js"></script>
    <script type="text/javascript" src="./_static/doctools.js"></script>
    <script type="text/javascript" src="./_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="./_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="./_static/bootstrap-3.2.0/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="./_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="None" href="./index.html#document-index" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  
<!-- RTD Extra Head -->

<!-- 
Always link to the latest version, as canonical.
http://docs.readthedocs.org/en/latest/canonical.html
-->
<link rel="canonical" href="http://docs.cloudstack.apache.org/projects/cloudstack-installation/en/4.9/" />

<link rel="stylesheet" href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />

<script type="text/javascript" src="./_static/readthedocs-data.js"></script>

<!-- Add page-specific data, which must exist in the page js, not global -->
<script type="text/javascript">
READTHEDOCS_DATA['page'] = 'index'
</script>

<script type="text/javascript" src="./_static/readthedocs-dynamic-include.js"></script>

<!-- end RTD <extrahead> --></head>
  <body role="document">

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html#document-index">
          安装指南</a>
        <span class="navbar-text navbar-version pull-left"><b>4.8</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
                <a role="button"
                   id="dLabelGlobalToc"
                   data-toggle="dropdown"
                   data-target="#"
                   href="index.html#document-index">章节 <b class="caret"></b></a>
                <ul class="dropdown-menu globaltoc"
                    role="menu"
                    aria-labelledby="dLabelGlobalToc">
                     <ul class="simple">
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-choosing_deployment_architecture">选择部署架构</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-qig">CentOS 6 快速安装指南</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-building_from_source">源码打包部署</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-overview/index">安装概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-management-server/index">Management Server 安装</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-configuration">配置 CloudStack</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/hyperv">Hyper-V 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/kvm">KVM 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/lxc">LXC 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/vsphere">VMware vSphere 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/xenserver">Citrix XenServer 安装</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-network_setup">网络配置</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup">存储配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup#small-scale-setup">小规模集群配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup#large-scale-setup">大规模集群配置p</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup#storage-architecture">存储架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup#network-configuration-for-storage">存储网络配置</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-optional_installation">其它安装选项</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-encryption">关于 Password 和 Key Encryption</a></li>
</ul>


                </ul>
              </li>
              
                
              
            
            
              
                
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="./search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul class="simple">
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-choosing_deployment_architecture">选择部署架构</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-qig">CentOS 6 快速安装指南</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-building_from_source">源码打包部署</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-overview/index">安装概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-management-server/index">Management Server 安装</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-configuration">配置 CloudStack</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/hyperv">Hyper-V 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/kvm">KVM 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/lxc">LXC 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/vsphere">VMware vSphere 安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-hypervisor/xenserver">Citrix XenServer 安装</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-network_setup">网络配置</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup">存储配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="./index.html#document-storage_setup#small-scale-setup">小规模集群配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup#large-scale-setup">大规模集群配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup#storage-architecture">存储架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage_setup#network-configuration-for-storage">存储网络配置</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-optional_installation">其它安装选项</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-encryption">关于 Password 和 Key Encryption</a></li>
</ul>


        </div>
      </div>
    <div class="col-md-9">
      
  <div class="section" id="cloudstack-installation-documentation">
<h1>CloudStack 安装文档<a class="headerlink" href="#cloudstack-installation-documentation" title="Permalink to this headline">¶</a></h1>
<div class="figure align-center">
<a class="reference external image-reference" href="http://cloudstack.apache.org/"><img alt="_images/acslogo.png" src="_images/acslogo.png" /></a>
</div>
<p>本文为 Apache CloudStack 安装指南, 文档中心,
管理员指南或Release-Notes 参见:</p>
<ul class="simple">
<li><a class="reference external" href="http://docs.cloudstack.apache.org">文档中心</a></li>
<li><a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-administration">管理员指南</a></li>
<li><a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-release-notes">Release Notes</a></li>
</ul>
<div class="admonition note">
<p class="first admonition-title">注意</p>
<p class="last">在本文档中，我们首先通过一些设计和架构 <a class="reference internal" href="#choices">选择</a> 来创建一个云。
 然后我们会深入一个单节点快速开始 <a class="reference internal" href="#guide">指南</a> 来引导您理解安装过程。
 源码安装 <a class="reference internal" href="#steps">步骤</a> 会在后续章节中给出，用于帮助开发者发布自己的版本。
 除此以外，您可以使用社区维护的包存储库进行一般 <a class="reference internal" href="#installation">安装</a>。
 文档的其它部分为 <a class="reference internal" href="#configuration">配置</a> 数据中心和
 <a class="reference internal" href="#network">网络</a>, <a class="reference internal" href="#storage">存储</a> 以及 <a class="reference internal" href="#hypervisors">计算节点</a>。</p>
</div>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="choosing-a-deployment-architecture">
<span id="choices"></span><h2>选择部署架构<a class="headerlink" href="#choosing-a-deployment-architecture" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-choosing_deployment_architecture"></span><div class="section" id="choosing-a-deployment-architecture">
<h3>选择部署架构<a class="headerlink" href="#choosing-a-deployment-architecture" title="Permalink to this headline">¶</a></h3>
<p>架构选择取决于规模和功能。 <br>本章包含架构以下部署实例：用于测试和实验的小规模架构和用于生产环境的完全冗余的大规模架构。</p>
<div class="section" id="small-scale-deployment">
<h4>小规模集群部署<a class="headerlink" href="#small-scale-deployment" title="Permalink to this headline">¶</a></h4>
<p><img alt="Small-Scale Deployment" src="_images/small-scale-deployment.png" /></p>
<p>本图描述了一个小规模Cloudtack环境的网络架构。</p>
<ul class="simple">
<li>防火墙提供了到Internet的访问。
该防火墙配置为NAT模式。
防火墙转发从Internet到Management Server的HTTP请求和API请求。
Management Server连接在管理网络上。</li>
<li>一个二层交换机连接了所有物理服务器和存储。</li>
<li>一个NFS Server作为主存储和二级存储。</li>
<li>一个管理服务器连接到管理网络上。</li>
</ul>
</div>
<div class="section" id="large-scale-redundant-setup">
<h4>大规模冗余配置<a class="headerlink" href="#large-scale-setup" title="Permalink to this headline">¶</a></h4>
<p><img alt="Large-Scale Redundant Setup" src="_images/large-scale-redundant-setup.png" /></p>
<p>本图描述了一个大规模Cloudtack环境的网络架构。</p>
<ul class="simple">
<li>一个三层交换作为数据中心的核心。
 部署一个类似VRRP的路由冗余协议。
 通常高端核心路由都包含防火墙模块。
 如果三层交换机没有集成防火墙功能，也会使用单独的防火墙设备。 
 防火墙配置为NAT模式，提供如下功能：<ul>
<li>转发来自Internet到Management Server的 HTTP 请求和 API请求。Management Server连接在管理网络上。</li>
<li>当云扩展到多zone的时候，防火墙需要开启端对端的VPN以便不同zone的server可以互相访问。</li>
</ul>
</li>
<li>每个pod有一个二层交换机。
 多个交换机堆叠来扩展端口数。
 无论那种情况，二层交换机都应该再部署一台作为冗余。</li>
<li>Management Server 集群 (包括前端负载均衡,
Management Server 节点, 和MySQL 数据库) 通过负载均衡连接到管理网络上。</li>
<li>二级存储服务器连接到管理网路上。</li>
<li>每个pod包含主存储和计算服务器。每个主存储和计算服务应该有冗余的网卡连接到不同的二层交换机上。</li>
</ul>
</div>
<div class="section" id="separate-storage-network">
<h4>独立存储网络<a class="headerlink" href="#separate-storage-network" title="Permalink to this headline">¶</a></h4>
<p>在前文描述的大规模冗余配置中，存储通信可能会导致管理网络超载。此时可以选择一个独立的存储网络。
 存储协议例如iSCSI 对网络延时很敏感。一个独立的存储网络可以保证客户网络通信不影响存储性能。</p>
</div>
<div class="section" id="multi-node-management-server">
<h4>多节点管理服务<a class="headerlink" href="#multi-node-management-server" title="Permalink to this headline">¶</a></h4>
<p>CloudStack Management Server 部署为一个或多个前端服务连接同一个MySQL 数据库。
 
 Optionally a pair of
hardware load balancers distributes requests from the web. A backup
management server set may be deployed using MySQL replication at a
remote site to add DR capabilities.</p>
<p><img alt="Multi-Node Management Server" src="_images/multi-node-management-server.png" /></p>
<p>管理员需要决定以下几项。</p>
<ul class="simple">
<li>是否启用负载均衡。</li>
<li>部署几台management server。</li>
<li>是否启用MySQL replication 来进行容灾。</li>
</ul>
</div>
<div class="section" id="multi-site-deployment">
<h4>多地区部署<a class="headerlink" href="#multi-site-deployment" title="Permalink to this headline">¶</a></h4>
<p>CloudStack 平台可以通过使用zone的方式很容易的扩展到多地区部署。下图展示了一个多地区部署的案例。</p>
<p><img alt="Example Of A Multi-Site Deployment" src="_images/multi-site-deployment.png" /></p>
<p>数据中心 1 部署了primary Management Server 和 zone 1. MySQL 数据库实时同步到位于Data Center 2的secondary
Management Server。</p>
<p><img alt="Separate Storage Network" src="_images/separate-storage-network.png" /></p>
<p>该图表述了一个独立存储网络的部署。每个server有4个网卡，2个连接到pod的网络交换机上，另外两个连接到存储网络的交换机上。</p>
<p>有两种方式来配置存储网络：</p>
<ul class="simple">
<li>为NFS做网卡Bonding，部署冗余交换机。
在NFS部署中，冗余交换机和绑定的网卡仍然连接到同一个网络 (只有一个CIDR 和默认网关).</li>
<li>iSCSI 可以发挥两个不同网络的优势(两个 CIDR分别拥有各自的网关). 多路径iSCSI 客户端可以在不同的存储网络中做容灾和负载均衡</li>
</ul>
<p><img alt="NIC Bonding And Multipath I/O" src="_images/nic-bonding-and-multipath-io.png" /></p>
<p>该图描述了网卡bonding和多路I/O（MPIO）的区别。
网卡bonding 配置只涉及到一个网络。MPIO涉及到两个独立的网络。</p>
</div>
<div class="section" id="choosing-a-hypervisor">
<h4>选择 Hypervisor<a class="headerlink" href="#choosing-a-hypervisor" title="Permalink to this headline">¶</a></h4>
<p>CloudStack 支持多种主流的hypervisor. 您的云环境可以考虑单一hypervisor或者多种hypervisor。
每个cluster中的主机必须运行相同的hypervisor.</p>
<p>您可能以及有了一个安装好正在运行的hypervisor，在这种情况下，您的选择以及确定了。
如果您正在开始选型，您需要决定哪种hypervisor最适合您的需要。
本文档不对hypervisor进行详细比较。但是，了解Cloudstack对于每种hypervisor的支持会更有利于您的选型。
下表提供此类信息。</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="35%" />
<col width="11%" />
<col width="15%" />
<col width="13%" />
<col width="5%" />
<col width="8%" />
<col width="13%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Feature</th>
<th class="head">XenServer</th>
<th class="head">vSphere</th>
<th class="head">KVM - RHEL</th>
<th class="head">LXC</th>
<th class="head">HyperV</th>
<th class="head">Bare Metal</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Network Throttling</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>?</td>
<td>N/A</td>
</tr>
<tr class="row-odd"><td>基本网络模式的安全组</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>?</td>
<td>No</td>
</tr>
<tr class="row-even"><td>iSCSI</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr class="row-odd"><td>FibreChannel</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr class="row-even"><td>Local Disk</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>HA</td>
<td>Yes</td>
<td>Yes (Native)</td>
<td>Yes</td>
<td>?</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr class="row-even"><td>Snapshots of local disk</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>?</td>
<td>?</td>
<td>N/A</td>
</tr>
<tr class="row-odd"><td>Local disk as data disk</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr class="row-even"><td>Work load balancing</td>
<td>No</td>
<td>DRS</td>
<td>No</td>
<td>No</td>
<td>?</td>
<td>N/A</td>
</tr>
<tr class="row-odd"><td>手动触发 live migration</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>?</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr class="row-even"><td>通过link local网络和v-Router通信来节约管理网络IP</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>?</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<div class="section" id="hypervisor-support-for-primary-storage">
<h5>Hypervisor 支持的Primary Storage<a class="headerlink" href="#hypervisor-support-for-primary-storage" title="Permalink to this headline">¶</a></h5>
<p>下表展示了不同hypervisor的存储选项和属性。</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="33%" />
<col width="13%" />
<col width="15%" />
<col width="16%" />
<col width="16%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Primary Storage Type</th>
<th class="head">XenServer</th>
<th class="head">vSphere</th>
<th class="head">KVM - RHEL</th>
<th class="head">LXC</th>
<th class="head">HyperV</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>磁盘，模板和快照的格式</td>
<td>VHD</td>
<td>VMDK</td>
<td>QCOW2</td>
<td>&nbsp;</td>
<td>VHD</td>
</tr>
<tr class="row-odd"><td>iSCSI support</td>
<td>CLVM</td>
<td>VMFS</td>
<td>Yes via Shared
Mountpoint</td>
<td>Yes via Shared
Mountpoint</td>
<td>No</td>
</tr>
<tr class="row-even"><td>Fiber Channel support</td>
<td>Yes, Via
existing SR</td>
<td>VMFS</td>
<td>Yes via Shared
Mountpoint</td>
<td>Yes via Shared
Mountpoint</td>
<td>No</td>
</tr>
<tr class="row-odd"><td>NFS support</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="row-even"><td>Local storage support</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>Storage over-provisioning</td>
<td>NFS</td>
<td>NFS and iSCSI</td>
<td>NFS</td>
<td>&nbsp;</td>
<td>No</td>
</tr>
<tr class="row-even"><td>SMB/CIFS</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p>XenServer 在iSCSI和Fiber Channel卷上使用一种集群存储LVM 来存储VM 镜像并且不支持over-provision。
但是这个存储本身可以支持thin-provisioning。因此CloudStack可以通过使用thin-provisioned 存储卷来支持over-provision。</p>
<p>KVM 支持&#8220;Shared Mountpoint（共享挂载点）&#8221; 存储. 一个shared mountpoint是一个挂载到集群所有server上的文件存储路径。
这个路径在集群所有机器上必须是完全相同的，例如 /mnt/primary1.
该shared mountpoint是一个集群存储文集系统，例如OCFS2。
这种情况下，CloudStack不再像操作NFS 一样去挂载或者卸载存储。
CloudStack 需要管理员来保证存储可用。</p>
<p>使用NFS 存储, CloudStack 可以实现overprovisioning. 在这种情况下全局配置的属性storage.overprovisioning.factor
是用来控制超分配的倍数。这个和hypervisor的类型无关。</p>
<p>Local storage（本地存储）是主存储的一种，用于vSphere, XenServer,
和KVM。当启用本地存储的时候，会在所有主机上自动创建一个本地存储池。
如果要使用本地存储来存放系统虚拟机（例如Virtual Router）需要再全局设置里面设置system.vm.use.local.storage为true。</p>
<p>CloudStack支持一个集群中有多个主存储。例如，可以部署2个NFS 作为主存储。或者先部署一个iSCSI LUN，成功后在添加第二个。</p>
</div>
</div>
<div class="section" id="best-practices">
<h4>最佳实践<a class="headerlink" href="#best-practices" title="Permalink to this headline">¶</a></h4>
<p>部署一个云平台是一件很有挑战性的事情。需要做很多不同的技术选型，CloudStack可以灵活配置来适应不同的技术选型组合。
本章节内容包含云平台实施的一些建议和需求。</p>
<p>这些内容仅作为建议而非特定方案。
如果指南中没有合适的方案，我们建议您通过mailing list来寻求指导或者建议。</p>
<div class="section" id="process-best-practices">
<h5>最佳实践<a class="headerlink" href="#process-best-practices" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>强烈建议使用对生产环境进行建模的分期系统。在定制化的系统上安装部署CloudStack是有很大风险的。</li>
<li>要有充足的时间来进行安装，测试和学习该系统。
安装基本网络环境需要大约一小时。
安装高级网络环境一开始可能需要数天时间，复杂安装可能持续更久。
安装一个完整的生产环境，需要大约4-8周来对所有功能进行集成测试。
您可以在通过cloudstack-users mailing list向社区研究人员求助。</li>
</ul>
</div>
<div class="section" id="setup-best-practices">
<h5>配置最佳实践<a class="headerlink" href="#setup-best-practices" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>每台主机都应该配置一个连接用来访问CloudStack Management Server 或者您的网络监控软件。</li>
<li>Use multiple clusters per pod if you need to achieve a certain switch
density.</li>
<li>Primary storage mountpoints 或LUNs 应该保持在 6 TB 以内。
更好的方案是在一个集群中使用多个小的主存储而不是一个大的主存储。</li>
<li>When exporting shares on primary storage, avoid data loss by
restricting the range of IP addresses that can access the storage.
参见&#8220;Linux NFS on Local Disks and DAS&#8221; 或者&#8220;Linux NFS on iSCSI&#8221;.</li>
<li>可以设置NIC bonding来提升效率和增强容灾能力。</li>
<li>在一个高配服务器需要支持较多VM 的环境中，推荐使用万兆交换机来配置存储网络。</li>
<li>服务器容量一般只通过虚拟机的内存来进行计算。
存储和CPU 是可以超分配的。内存不可以。
所以一般会用内存来作为服务器容量的计算因素。</li>
<li>(XenServer)可以通过修改XenServer dom0配置来为dom0分配更多内存，以便XenServer可以管理大规模虚拟机。
我们为dom0分配2940 MB内存。操作步骤参见
<a class="reference external" href="http://support.citrix.com/article/CTX126531">http://support.citrix.com/article/CTX126531</a>.
本文针对XenServer 5.6版本，同样适用于XenServer 6.0.</li>
</ul>
</div>
<div class="section" id="maintenance-best-practices">
<h5>维护最佳实践<a class="headerlink" href="#maintenance-best-practices" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>监控主机磁盘空间。很多主机因log占满根目录导致系统无法运转而宕机。</li>
<li>监控每个集群的虚拟机总数，禁止向达到hypervisor可管理虚拟机上限的集群分配新的虚拟机。
确保预留充足的资源在一台或多台主机宕机的情况下做容灾，以便VM可以在其它机器上重启。
可以查看hypervisor的相关文档来确定没台主机能支持的最大VM数量，然后在CloudStack的全局配置中进行设置，作为默认限制。
监控每个集群的活跃的VM，保证最大VM数低于安全值，避免偶尔发生的主机宕机。
例如：一个集群中有N 台服务器，您关闭一台之后，整个集群能提供的VM 容量为(N-1) *
(per-host-limit)。一旦集群容量达到这个数值，则需要通过CloudStack UI 来禁止继续在此集群分配VM。</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">没有升级至最新补丁包可能导致数据损坏或丢失虚拟机。</p>
</div>
<p>安装所有hypervisor官方提供的补丁包。订阅hypervisor提供商的支持页面来跟踪所发布的补丁，在发布之后尽快更新。
CloudStack本身不会订阅或管理hypervisor的补丁。
主机更新到最新补丁是很有必要的。Hypervisor的提供商一般不提供旧版本的支持。</p>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="quick-installation-guide">
<span id="guide"></span><h2>快速安装指南<a class="headerlink" href="#quick-installation-guide" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-qig"></span><div class="section" id="quick-installation-guide-for-centos-6">
<h3>基于CentOS 6的快速安装指南<a class="headerlink" href="#quick-installation-guide-for-centos-6" title="Permalink to this headline">¶</a></h3>
<div class="section" id="overview">
<h4>概述<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h4>
<div class="section" id="what-exactly-are-we-building">
<h5>我们在部署什么？<a class="headerlink" href="#what-exactly-are-we-building" title="Permalink to this headline">¶</a></h5>
<p>Infrastructure-as-a-Service (IaaS：基础设施即服务) 云是一个很复杂工程，
因为设计上有特别多的选项，即便是有经验的系统管理员在刚接触云平台的时候，也可能会导致迷惑。
本手册的目标是提供一组指导，帮助您尽快上手CloudStack并绕过一些可能的问题。</p>
</div>
<div class="section" id="high-level-overview-of-the-process">
<h5>安装过程高级概述<a class="headerlink" href="#high-level-overview-of-the-process" title="Permalink to this headline">¶</a></h5>
<p>该手册主题为创建一个CloudStack云，该云使用CentOS 6.5上部署的KVM 和三层网络隔离出来的NFS 存储(又名安全组), 并且将其都部署在统一台服务器上。</p>
<p>KVM（Kernel-based Virtual Machine）是一个基于Linux Kernel的虚拟化技术。
 KVM 在硬件虚拟化扩展的基础上支持本地虚拟化。</p>
<p>安全组作为分布式防火墙来控制一组虚拟机的访问。</p>
</div>
<div class="section" id="prerequisites">
<h5>先决条件<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h5>
<p>完成该手册内容你需要准备以下几项：</p>
<ol class="arabic simple">
<li>至少一台电脑支持并开启硬件虚拟化。</li>
<li><a class="reference external" href="http://mirrors.kernel.org/centos/6/isos/x86_64/">CentOS 6.5 x86_64 minimal install CD</a></li>
<li>24位掩码的网络，网关为xxx.xxx.xxx.1, 不需要DHCP并且CloudStack没有任何节点需要动态IP地址。
这是为了简单起见。</li>
</ol>
</div>
</div>
<div class="section" id="environment">
<h4>环境<a class="headerlink" href="#environment" title="Permalink to this headline">¶</a></h4>
<p>在Cloudtack安装开始之前，需要先准备安装好部署环境。接下来我们逐步进行准备。</p>
<div class="section" id="operating-system">
<h5>操作系统<a class="headerlink" href="#operating-system" title="Permalink to this headline">¶</a></h5>
<p>使用CentOS 6.5 x86_64 minimal install ISO安装操作系统。默认安装即可。</p>
<p>安装完成后，使用root账户通过SSH 连接到新的系统。注意不能在生产环境中允许root登陆，在安装配置完成后需要关闭root的远程登陆。</p>
<div class="section" id="configuring-the-network">
<span id="conf-network"></span><h6>配置网络<a class="headerlink" href="#configuring-the-network" title="Permalink to this headline">¶</a></h6>
<p>默认网卡不会启动，您需要配置你的网络环境来启动网卡。
我们设计为本环境中没有DHCP服务器，所以我们要手动配置网络设置。
在此次配置中，eth0是唯一的可用的网卡。</p>
<p>使用root账户登陆Console. 查看文件
/etc/sysconfig/network-scripts/ifcfg-eth0, 如下所示，默认为:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">DEVICE</span><span class="o">=</span><span class="s2">&quot;eth0&quot;</span>
<span class="n">HWADDR</span><span class="o">=</span><span class="s2">&quot;52:54:00:B9:A6:C0&quot;</span>
<span class="n">NM_CONTROLLED</span><span class="o">=</span><span class="s2">&quot;yes&quot;</span>
<span class="n">ONBOOT</span><span class="o">=</span><span class="s2">&quot;no&quot;</span>
</pre></div>
</div>
<p>该配置无法连接网络，也不适合CloudStack。我们来配置此文件来指定IP 地址，掩码等，如下所示：</p>
<div class="admonition note">
<p class="first admonition-title">提示</p>
<p class="last">在配置中不要使用示例中的Hardware Address (也称为MAC address)。这是网卡特定属性，应该保持和物理网卡信息一致。</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span>DEVICE=eth0
HWADDR=52:54:00:B9:A6:C0
NM_CONTROLLED=no
ONBOOT=yes
BOOTPROTO=none
IPADDR=172.16.10.2
NETMASK=255.255.255.0
GATEWAY=172.16.10.1
DNS1=8.8.8.8
DNS2=8.8.4.4
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">提示</p>
<p class="last">IP Addressing - 本文档中，我们假设您只有一个24位掩码的网络。
可以是任何符合RFC 1918标准的网络。
但是我们假设您使用和我们一样的地址，从而我们使用172.16.10.2 但是您可能使用192.168.55.0/24网络并且IP 为192.168.55.2</p>
</div>
<p>现在配置文件已经完成，接下来运行以下命令启动网络：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># chkconfig network on</span>

<span class="c1"># service network start</span>
</pre></div>
</div>
</div>
<div class="section" id="hostname">
<span id="conf-hostname"></span><h6>Hostname<a class="headerlink" href="#hostname" title="Permalink to this headline">¶</a></h6>
<p>CloudStack需要已经正确设置过的hostname。默认安装的系统中，hostname默认为
localhost.localdomain. 可以通过运行以下命令来查看：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># hostname --fqdn</span>
</pre></div>
</div>
<p>返回值为：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>localhost
</pre></div>
</div>
<p>修改hostname，可以按和文件中相同的格式来修改/etc/hosts文件，例如：
</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
172.16.10.2 srvr1.cloud.priv
</pre></div>
</div>
<p>修改好配置之后，重启网络：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service network restart</span>
</pre></div>
</div>
<p>再次执行 hostname &#8211;fqdn 来查看hostname，确认返回了一个FQDN结果。</p>
</div>
<div class="section" id="selinux">
<span id="conf-selinux"></span><h6>SELinux<a class="headerlink" href="#selinux" title="Permalink to this headline">¶</a></h6>
<p>安装部署CloudStack，需要设置SELinux为permissive模式。
我们需要设置修改当前环境和修改配置以便重启后依然生效。</p>
<p>当前环境中配置SELinux命令为：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># setenforce 0</span>
</pre></div>
</div>
<p>修改永久生效的SElinux配置需要修改配置文件/etc/selinux/config，如下：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># This file controls the state of SELinux on the system.</span>
<span class="c1"># SELINUX= can take one of these three values:</span>
<span class="c1"># enforcing - SELinux security policy is enforced.</span>
<span class="c1"># permissive - SELinux prints warnings instead of enforcing.</span>
<span class="c1"># disabled - No SELinux policy is loaded.</span>
<span class="nv">SELINUX</span><span class="o">=</span>permissive
<span class="c1"># SELINUXTYPE= can take one of these two values:</span>
<span class="c1"># targeted - Targeted processes are protected,</span>
<span class="c1"># mls - Multi Level Security protection.</span>
<span class="nv">SELINUXTYPE</span><span class="o">=</span>targeted
</pre></div>
</div>
</div>
<div class="section" id="ntp">
<span id="conf-ntp"></span><h6>NTP<a class="headerlink" href="#ntp" title="Permalink to this headline">¶</a></h6>
<p>NTP 是为了保证环境中所有的server在时间上一致。但是，NTP默认没有安装。
所以我们需要手动安装并配置NTP服务。步骤如下：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum -y install ntp</span>
</pre></div>
</div>
<p>默认配置已经可以满足我们的目标了，我们只需要启动并设置为自动启动即可，步骤如下：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># chkconfig ntpd on</span>
<span class="c1"># service ntpd start</span>
</pre></div>
</div>
</div>
<div class="section" id="configuring-the-cloudstack-package-repository">
<span id="qigconf-pkg-repo"></span><h6>配置CloudStack 安装源<a class="headerlink" href="#configuring-the-cloudstack-package-repository" title="Permalink to this headline">¶</a></h6>
<p>我们需要配置机器使用一个CloudStack 安装源。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Apache CloudStack官方发布的release是一个源码包，并没有官方的二进制包。
完整的安装指南包含如何获取源码并且打包生成RPM包，创建yum源。
本指南尽量保持简单，所以我们使用一个社区提供的yum源。</p>
</div>
<p>想要创建一个CloudStack yum源, 需要创建/etc/yum.repos.d/cloudstack.repo 输入以下内容：</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>[cloudstack]
name=cloudstack
baseurl=http://cloudstack.apt-get.eu/centos/6/4.8/
enabled=1
gpgcheck=0
</pre></div>
</div>
</div>
</div>
<div class="section" id="nfs">
<h5>NFS<a class="headerlink" href="#nfs" title="Permalink to this headline">¶</a></h5>
<p>本指南配置使用一个NFS同时作为主存储和二级存储。
我们先来设置两个NFS共享目录。
首先安装nfs-utils.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum -y install nfs-utils</span>
</pre></div>
</div>
<p>接下来配置NFS来支持两个共享目录。
简单的配置/etc/exports 即可，输入以下内容：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/secondary *<span class="o">(</span>rw,async,no_root_squash,no_subtree_check<span class="o">)</span>
/primary *<span class="o">(</span>rw,async,no_root_squash,no_subtree_check<span class="o">)</span>
</pre></div>
</div>
<p>您可能注意到了，我们使用了两个并不存在的目录。现在我们通过以下命令创建这两个目录：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mkdir /primary</span>
<span class="c1"># mkdir /secondary</span>
</pre></div>
</div>
<p>CentOS 6.x 默认使用NFSv4。NFSv4需要在所有client端设置域名。本例中，域名为cloud.priv,
所以，请修改域名设置文件/etc/idmapd.conf，取消注释并如下设置：
Domain = cloud.priv</p>
<p>取消
/etc/sysconfig/nfs以下行的注释</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">LOCKD_TCPPORT</span><span class="o">=</span>32803
<span class="nv">LOCKD_UDPPORT</span><span class="o">=</span>32769
<span class="nv">MOUNTD_PORT</span><span class="o">=</span>892
<span class="nv">RQUOTAD_PORT</span><span class="o">=</span>875
<span class="nv">STATD_PORT</span><span class="o">=</span>662
<span class="nv">STATD_OUTGOING_PORT</span><span class="o">=</span>2020
</pre></div>
</div>
<p>配置防火墙来允许NFS连接。
编辑文件/etc/sysconfig/iptables</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>-A INPUT -s 172.16.10.0/24 -m state --state NEW -p udp --dport <span class="m">111</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p tcp --dport <span class="m">111</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p tcp --dport <span class="m">2049</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p tcp --dport <span class="m">32803</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p udp --dport <span class="m">32769</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p tcp --dport <span class="m">892</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p udp --dport <span class="m">892</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p tcp --dport <span class="m">875</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p udp --dport <span class="m">875</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p tcp --dport <span class="m">662</span> -j ACCEPT
-A INPUT -s 172.16.10.0/24 -m state --state NEW -p udp --dport <span class="m">662</span> -j ACCEPT
</pre></div>
</div>
<p>重启防火墙：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service iptables restart</span>
</pre></div>
</div>
<p>设置NFS开机自启动：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service rpcbind start</span>
<span class="c1"># service nfs start</span>
<span class="c1"># chkconfig rpcbind on</span>
<span class="c1"># chkconfig nfs on</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="management-server-installation">
<h4>安装Management Server<a class="headerlink" href="#management-server-installation" title="Permalink to this headline">¶</a></h4>
<p>下面我们开始安装CloudStack 管理服务和周边工具。</p>
<div class="section" id="database-installation-and-configuration">
<h5>数据库安装及配置<a class="headerlink" href="#database-installation-and-configuration" title="Permalink to this headline">¶</a></h5>
<p>安装MySQL并修改部分配置以运行CloudStack。</p>
<p>安装命令：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum -y install mysql-server</span>
</pre></div>
</div>
<p>安装完成后，我们需要修改/etc/my.cnf几项配置。我们需要在[mysqld]标签下插入以下几项：</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">innodb_rollback_on_timeout</span><span class="o">=</span><span class="mi">1</span>
<span class="n">innodb_lock_wait_timeout</span><span class="o">=</span><span class="mi">600</span>
<span class="n">max_connections</span><span class="o">=</span><span class="mi">350</span>
<span class="n">log</span><span class="o">-</span><span class="nb">bin</span><span class="o">=</span><span class="n">mysql</span><span class="o">-</span><span class="nb">bin</span>
<span class="n">binlog</span><span class="o">-</span><span class="n">format</span> <span class="o">=</span> <span class="s1">&#39;ROW&#39;</span>
</pre></div>
</div>
<p>MySQL配置完成，接下来执行以下命令启动MySQL并设置为开机自启动：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service mysqld start</span>
<span class="c1"># chkconfig mysqld on</span>
</pre></div>
</div>
</div>
<div class="section" id="installation">
<h5>安装<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h5>
<p>现在开始安装management server. 步骤如下：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum -y install cloudstack-management</span>
</pre></div>
</div>
<p>安装完成后，执行以下命令配置数据库：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># cloudstack-setup-databases cloud:password@localhost --deploy-as=root</span>
</pre></div>
</div>
<p>执行完成后，可以看到类似信息&#8220;CloudStack has
successfully initialized the database.&#8221;</p>
<p>数据库创建完成后，执行最后一步来设置management server：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># cloudstack-setup-management</span>
</pre></div>
</div>
<p>如果使用Tomcat7 需要增加 &#8211;tomcat7 参数。</p>
</div>
<div class="section" id="system-template-setup">
<h5>系统虚拟机模板设置<a class="headerlink" href="#system-template-setup" title="Permalink to this headline">¶</a></h5>
<p>CloudStack使用系统虚拟机来提供虚拟机console的访问，提供多种网络服务，管理多种存储。
启动CloudStack的时候，这一步骤将会获取虚拟机镜像。</p>
<p>现在我们需要将系统虚拟机模板下载下来并部署到刚刚挂载的共享目录中。
Management server及其脚本会操作这些系统虚拟机模板。</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt <span class="se">\</span>
-m /secondary <span class="se">\</span>
-u http://cloudstack.apt-get.eu/systemvm/4.6/systemvm64template-4.6.0-kvm.qcow2.bz2 <span class="se">\</span>
-h kvm -F
</pre></div>
</div>
<p>至此，management server安装基本结束。CloudStack还需要其它配置，我们会在设置完hypervisor之后继续配置CloudStack。</p>
</div>
</div>
<div class="section" id="kvm-setup-and-installation">
<h4>KVM安装及配置<a class="headerlink" href="#kvm-setup-and-installation" title="Permalink to this headline">¶</a></h4>
<p>我们选择KVM作为hypervisor - 我们会讲解如何初始化KVM 及如何初始化安装agent，您可以根据此步骤安装其它KVM节点。</p>
<div class="section" id="id1">
<h5>先决条件<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>我们用management server同时作为计算节点，这样，我们已经完成了一部分先决条件的设置，但是为了清晰，我们依然再次列出来。
步骤如下：</p>
<ol class="arabic simple">
<li><a class="reference internal" href="#conf-network"><span>Configuring the network</span></a></li>
<li><a class="reference internal" href="#conf-hostname"><span>Hostname</span></a></li>
<li><a class="reference internal" href="#conf-selinux"><span>SELinux</span></a></li>
<li><a class="reference internal" href="#conf-ntp"><span>NTP</span></a></li>
<li><a class="reference internal" href="#qigconf-pkg-repo"><span>Configuring the CloudStack Package Repository</span></a></li>
</ol>
<p>当然，您已经不需要再次执行在management server安装过程中已经完成的步骤，但是添加其它节点的时候，还是需要完成以上步骤。</p>
</div>
<div class="section" id="id2">
<h5>安装<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<p>Installation of the KVM agent is trivial with just a single command, but
afterwards we&#8217;ll need to configure a few things.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum -y install cloudstack-agent</span>
</pre></div>
</div>
</div>
<div class="section" id="kvm-configuration">
<h5>KVM 配置<a class="headerlink" href="#kvm-configuration" title="Permalink to this headline">¶</a></h5>
<p>我们需要对KVM进行两方面的配置，libvirt, 和QEMU.</p>
<div class="section" id="qemu-configuration">
<h6>QEMU 配置<a class="headerlink" href="#qemu-configuration" title="Permalink to this headline">¶</a></h6>
<p>KVM单节点配置是相对比较简单的。我们需要修改QEMU VNC配置。
修改/etc/libvirt/qemu.conf文件，删掉下列行的注释并修改为以下内容。</p>
<blockquote>
<div>vnc_listen=0.0.0.0</div></blockquote>
</div>
<div class="section" id="libvirt-configuration">
<h6>Libvirt 配置<a class="headerlink" href="#libvirt-configuration" title="Permalink to this headline">¶</a></h6>
<p>CloudStack通过libvirt来管理虚拟机。正确配置libvirt是很重要的。
Libvirt是cloud-agent的一个依赖，需要提前安装。</p>
<ol class="arabic">
<li><p class="first">为了支持live migration，libvirt需要监听非安全的TCP连接。
我们需要禁止libvirt使用Multicast DNS 广播。这些配置选项都在/etc/libvirt/libvirtd.conf文件中。</p>
<p>设置以下选项：</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">listen_tls</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">listen_tcp</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">tcp_port</span> <span class="o">=</span> <span class="s2">&quot;16059&quot;</span>
<span class="n">auth_tcp</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
<span class="n">mdns_adv</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</li>
<li><p class="first">仅仅在libvirtd.conf中打开 &#8220;listen_tcp&#8221; 是不够的, 我们还需要修改/etc/sysconfig/libvirtd文件中的选项：</p>
<p>打开下列行的注释：</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1">#LIBVIRTD_ARGS=&quot;--listen&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">重启libvirt</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service libvirtd restart</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="kvm-configuration-complete">
<h6>KVM配置完成<a class="headerlink" href="#kvm-configuration-complete" title="Permalink to this headline">¶</a></h6>
<dl class="docutils">
<dt>执行以下命令检查KVM是否完成配置并正确运行：</dt>
<dd><div class="first last highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># lsmod | grep kvm</span>
kvm_intel              <span class="m">55496</span>  0
kvm                   <span class="m">337772</span>  <span class="m">1</span> kvm_intel
</pre></div>
</div>
</dd>
</dl>
<p>现在KVM安装和配置已经完成，我们接下来开始使用CloudStack UI来部署我们的云平台。</p>
</div>
</div>
</div>
<div class="section" id="configuration">
<h4>配置<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h4>
<p>As we noted before we will be using security groups to provide isolation and
by default that implies that we&#8217;ll be using a flat layer-2 network. It also
means that the simplicity of our setup means that we can use the quick
installer.</p>
<div class="section" id="ui-access">
<h5>UI 访问<a class="headerlink" href="#ui-access" title="Permalink to this headline">¶</a></h5>
<p>登陆CloudStack的web管理界面，打开浏览器，登陆网址：
<a class="reference external" href="http://172.16.10.2:8080/client">http://172.16.10.2:8080/client</a>默认用户名为&#8216;admin&#8217;
默认密码为：&#8216;password&#8217;。您可以看到一个页面让您选择几种选项来设置您的CloudStack。
选择Continue with Basic Setup option.</p>
<p>您可以看到一个提示，要求您修改admin密码。可以按需执行。</p>
</div>
<div class="section" id="setting-up-a-zone">
<h5>配置 Zone<a class="headerlink" href="#setting-up-a-zone" title="Permalink to this headline">¶</a></h5>
<p>Zone是CloudStack中最大的管理单元 - 我们通过当前页面来创建一个Zone。接下来，我们需要5个部分的信息。</p>
<ol class="arabic simple">
<li>Name - 设置一个有描述性的名字&#8216;Zone1&#8217; </li>
<li>Public DNS 1 - 设置为 &#8216;8.8.8.8&#8217; </li>
<li>Public DNS 2 - 设置为 &#8216;8.8.4.4&#8217; </li>
<li>Internal DNS1 - 同样设置为 &#8216;8.8.8.8&#8217; </li>
<li>Internal DNS2 - 同样设置为 &#8216;8.8.4.4&#8217; </li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">CloudStack 区分内部和外部DNS。内部DNS只用来解析内网主机名，例如NFS server的DNS名。
外部DNS用于给客户虚拟机提供对公网域名的解析。
您可以使用相同类型的DNS server，但是如果这样配置，则必须要保证内网和外网IP都可以路由到这个DNS server。
在我们当前配置中，并没有用到内网域名，为了不再设置一个内网DNA server，我们把内外网设为同一个。</p>
</div>
</div>
<div class="section" id="pod-configuration">
<h5>Pod 配置<a class="headerlink" href="#pod-configuration" title="Permalink to this headline">¶</a></h5>
<p>Zone已经创建完成了，接下来我们注册一个pod，输入以下信息：</p>
<ol class="arabic simple">
<li>Name - 设置为Pod1</li>
<li>Gateway - 设置为 172.16.10.1 </li>
<li>Netmask - 设置为 255.255.255.0</li>
<li>Start/end 系统内部使用 IP段 - 设置为 172.16.10.10-172.16.10.20</li>
<li>Guest gateway - 设置为 172.16.10.1</li>
<li>Guest netmask - 设置为 255.255.255.0</li>
<li>Guest start/end IP - 设置为 172.16.10.30-172.16.10.200</li>
</ol>
</div>
<div class="section" id="cluster">
<h5>Cluster<a class="headerlink" href="#cluster" title="Permalink to this headline">¶</a></h5>
<p>Zone已经创建完成，我们接下来只需要添加几个配置来创建cluster。</p>
<ol class="arabic simple">
<li>Name - 设置为 Cluster1</li>
<li>Hypervisor - 选择KVM</li>
</ol>
<p>输入以下信息来添加cluster的第一台主机。</p>
<ol class="arabic simple">
<li>Hostname - 填入IP地址 172.16.10.2，因为我们没有设置内网的DNS server.</li>
<li>Username - &#8216;root&#8217;</li>
<li>Password - 系统的root密码</li>
</ol>
<div class="section" id="primary-storage">
<h6>Primary Storage<a class="headerlink" href="#primary-storage" title="Permalink to this headline">¶</a></h6>
<p>Cluster设置完成 - 会有一个提示，提示添加primary storage。
选择NFS作为存储类型并且输入以下信息：</p>
<ol class="arabic simple">
<li>Name - 设置为&#8216;Primary1&#8217;</li>
<li>Server - 设置为IP 172.16.10.2</li>
<li>Path - 设置为/primary</li>
</ol>
</div>
<div class="section" id="secondary-storage">
<h6>Secondary Storage<a class="headerlink" href="#secondary-storage" title="Permalink to this headline">¶</a></h6>
<p>在一个新的zone，需要指定二级存储信息 - 一般如下所示：</p>
<ol class="arabic simple">
<li>NFS server - 设置为 IP 172.16.10.2</li>
<li>Path - 设置为 /secondary</li>
</ol>
<p>现在，点击Launch，云开始进行设置 - 这会持续几分钟时间，具体取决于您的网络速度。</p>
<p>您已经完成了Apache CloudStack 云平台的安装。 </p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="source-installation">
<span id="steps"></span><h2>源码安装<a class="headerlink" href="#source-installation" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-building_from_source"></span><div class="section" id="building-from-source">
<h3>从源码打包<a class="headerlink" href="#building-from-source" title="Permalink to this headline">¶</a></h3>
<p>官方的CloudStack release通常是以源码的形式进行发布。
您可能更期望能获得更加方便的&#8220;二进制安装包&#8221; 但社区以源码包作为发布的规范。
本章中，我们将会介绍如何获取源码并且编译打包，以便于您可以使用maven或者创建Debian或者RPM包。</p>
<p>注意：从源码编译安装一个IaaS云平台并非最有效率的方式。但是，我们还是会介绍从源码编译打包RPM或Debian包的方法。</p>
<p>该指南是针对特定版本的。从源码编译4.7.x系列的方法和编译4.2.x系列并不相同。</p>
<p>如果您在使用一个没有公开发布的CloudStack版本，参见根目录下的INSTALL.md。</p>
<div class="section" id="getting-the-release">
<h4>获取发布版本<a class="headerlink" href="#getting-the-release" title="Permalink to this headline">¶</a></h4>
<p>可以在<a class="reference external" href="http://cloudstack.apache.org/downloads.html">Apache
CloudStack 下载页</a>获取最新发布版本。</p>
<p>历史发布可以在archive.apache.org找到。可以在下载页面找到更多信息。</p>
<p>在&#8216;Latest release&#8217; 部分可以找到以下几个链接. 一个以<code class="docutils literal"><span class="pre">tar.bz2</span></code>结尾，
和一个PGP/GPG 签名, MD5,以及 SHA512 file.</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">tar.bz2</span></code> 文件是Bzip2打包的源码。</li>
<li><code class="docutils literal"><span class="pre">.asc</span></code> 文件是一个加密的签名文件，用来确认该发布版本的真实性。</li>
<li><code class="docutils literal"><span class="pre">.md5</span></code> 文件是一个MD5的哈希值，用来确认是否下载完成。</li>
<li><code class="docutils literal"><span class="pre">.sha</span></code> 文件是一个SHA512的哈希值，用来确认是否下载完成。</li>
</ul>
</div>
<div class="section" id="verifying-the-downloaded-release">
<h4>校验下载的版本<a class="headerlink" href="#verifying-the-downloaded-release" title="Permalink to this headline">¶</a></h4>
<p>有很多种机制用来校验下载的版本是否真实有效。</p>
<div class="section" id="getting-the-keys">
<h5>获取 KEYS<a class="headerlink" href="#getting-the-keys" title="Permalink to this headline">¶</a></h5>
<p>要使用GPG签名进行校验，需要下载
<a class="reference external" href="http://www.apache.org/dist/cloudstack/KEYS">KEYS</a> 文件。</p>
<p>接下来，执行以下命令导入这些key：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ wget http://www.apache.org/dist/cloudstack/KEYS
$ gpg --import KEYS
</pre></div>
</div>
</div>
<div class="section" id="gpg">
<h5>GPG<a class="headerlink" href="#gpg" title="Permalink to this headline">¶</a></h5>
<p>CloudStack提供了一个独立的GPG签名，可执行以下命令校验这个签名：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ gpg --verify apache-cloudstack-4.8.0-src.tar.bz2.asc
</pre></div>
</div>
<p>如果签名有效，您会看到如下输出：
&#8216;Good signature&#8217;.</p>
</div>
<div class="section" id="md5">
<h5>MD5<a class="headerlink" href="#md5" title="Permalink to this headline">¶</a></h5>
<p>在加密的签名之外CloudStack还提供了一个MD5校验值，您可以通过此校验值校验下载是否正确。
命令如下：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ gpg --print-md MD5 apache-cloudstack-4.8.0-src.tar.bz2 <span class="p">|</span> diff - apache-cloudstack-4.8.0-src.tar.bz2.md5
</pre></div>
</div>
<p>如果成功则不会有返回值。
如果有任何输出，则说明本地生成的哈希值和社区发布不一致。</p>
</div>
<div class="section" id="sha512">
<h5>SHA512<a class="headerlink" href="#sha512" title="Permalink to this headline">¶</a></h5>
<p>MD5之外，CloudStack还提供了一个SHA512加密的哈希值，用来确保下载有效。可以使用如下命令进行判断：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ gpg --print-md SHA512 apache-cloudstack-4.8.0-src.tar.bz2 <span class="p">|</span> diff - apache-cloudstack-4.8.0-src.tar.bz2.sha
</pre></div>
</div>
<p>如果成功则不会有返回值。
如果有任何输出，则说明本地生成的哈希值和社区发布不一致。</p>
</div>
</div>
<div class="section" id="prerequisites-for-building-apache-cloudstack">
<h4>编译Apache CloudStack的先决条件<a class="headerlink" href="#prerequisites-for-building-apache-cloudstack" title="Permalink to this headline">¶</a></h4>
<p>编译CloudStack需要很多的预设值。本文档假定编译是在使用RPM或DEB安装包的Linux系统上进行的，并且打包为RPM或在DEB。</p>
<p>你需要以下组件来进行编译：</p>
<ol class="arabic simple">
<li>Maven (version 3)</li>
<li>Java (Java 7/OpenJDK 1.7)</li>
<li>Apache Web Services Common Utilities (ws-commons-util)</li>
<li>MySQL</li>
<li>MySQLdb (provides Python database API)</li>
<li>Tomcat 6 (not 6.0.35)</li>
<li>genisoimage</li>
<li>rpmbuild or dpkg-dev</li>
</ol>
</div>
<div class="section" id="extracting-source">
<h4>解压源码<a class="headerlink" href="#extracting-source" title="Permalink to this headline">¶</a></h4>
<p>执行以下命令解压源码：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ tar -jxvf apache-cloudstack-4.8.0-src.tar.bz2
</pre></div>
</div>
<p>切换到源码目录中：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> ./apache-cloudstack-4.8.0-src
</pre></div>
</div>
</div>
<div class="section" id="building-deb-packages">
<h4>打包为 DEB<a class="headerlink" href="#building-deb-packages" title="Permalink to this headline">¶</a></h4>
<p>在上文的依赖之外，还需要安装其它的依赖。我们推荐使用Maven 3。</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ sudo apt-get update
$ sudo apt-get install python-software-properties
$ sudo apt-get update
$ sudo apt-get install ant debhelper openjdk-7-jdk tomcat6 libws-commons-util-java genisoimage python-mysqldb libcommons-codec-java libcommons-httpclient-java liblog4j1.2-java maven
</pre></div>
</div>
<p>按照前文预设，我们已经安装好了基本的编译环境，接下来我们编译过程中会有很多依赖jar。
 CloudStack用maven作为依赖管理。
 可以执行以下命令来处理编译期依赖：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ mvn -P deps
</pre></div>
</div>
<p>解决完编译依赖之后，我们可以开始执行命令编译CloudStack并打包成DEB，命令如下：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ dpkg-buildpackage -uc -us
</pre></div>
</div>
<p>该命令会打包成如下DEB安装包：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-common-4.8.0.amd64.deb
cloudstack-management-4.8.0.amd64.deb
cloudstack-agent-4.8.0.amd64.deb
cloudstack-usage-4.8.0.amd64.deb
cloudstack-awsapi-4.8.0.amd64.deb
cloudstack-cli-4.8.0.amd64.deb
cloudstack-docs-4.8.0.amd64.deb
</pre></div>
</div>
<div class="section" id="setting-up-an-apt-repo">
<h5>配置APT 源<a class="headerlink" href="#setting-up-an-apt-repo" title="Permalink to this headline">¶</a></h5>
<p>创建安装包完成后，您可能想要把这些包复制到一个能提供HTTP服务的系统中。
首先您需要创建一个目录，然后执行<code class="docutils literal"><span class="pre">dpkg-scanpackages</span></code>命令来创建
<code class="docutils literal"><span class="pre">Packages.gz</span></code>, 该包中包含所有包的结构。
最后，您可以将这个源添加到您的系统中，以便使用APT方式来安装这些包。</p>
<p>首先确认您已经安装了<strong>dpkg-dev</strong>。
 This should have been installed when you pulled in the
<strong>debhelper</strong> application previously, but if you&#8217;re generating
<code class="docutils literal"><span class="pre">Packages.gz</span></code> on a different system, be sure that it&#8217;s installed there
as well.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ sudo apt-get install dpkg-dev
</pre></div>
</div>
<p>接下来，将DEB复制到能提供HTTP的目录中。本例中，我们使用<code class="docutils literal"><span class="pre">/var/www/cloudstack/repo</span></code>
，您可以按需修改。</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ sudo mkdir -p /var/www/cloudstack/repo/binary
$ sudo cp *.deb /var/www/cloudstack/repo/binary
$ <span class="nb">cd</span> /var/www/cloudstack/repo/binary
$ sudo sh -c <span class="s1">&#39;dpkg-scanpackages . /dev/null | tee Packages | gzip -9 &gt; Packages.gz&#39;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">注意</p>
<p class="last">如果提示缺失文件，可忽略。</p>
</div>
<p>现在所有的DEB包已经编译完成并且<code class="docutils literal"><span class="pre">Packages.gz</span></code>已经生成至
<code class="docutils literal"><span class="pre">binary</span></code> 目录，可以使用HTTP访问。（进行下一步之前，您可以使用
<code class="docutils literal"><span class="pre">wget</span></code> 或 <code class="docutils literal"><span class="pre">curl</span></code> 来测试。）</p>
</div>
<div class="section" id="configuring-your-machines-to-use-the-apt-repository">
<h5>配置服务器来使用该APT源<a class="headerlink" href="#configuring-your-machines-to-use-the-apt-repository" title="Permalink to this headline">¶</a></h5>
<p>
现在我们已经有了一个APT源，您需要进行相应配置来启用这个源。您可以在 <code class="docutils literal"><span class="pre">/etc/apt/sources.list.d</span></code>
文件中加入相关内容。也可以创建一个新的文件<code class="docutils literal"><span class="pre">/etc/apt/sources.list.d/cloudstack.list</span></code> 
写入该内容：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>deb http://server.url/cloudstack/repo/binary ./
</pre></div>
</div>
<p>现在APT源已经配置完成，运行update命令使APT获得CloudStack安装包的路径。</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ sudo apt-get update
</pre></div>
</div>
<p>现在可以跳转至Install on Ubuntu.</p>
</div>
</div>
<div class="section" id="building-rpms-from-source">
<h4>从源码编译RPM<a class="headerlink" href="#building-rpms-from-source" title="Permalink to this headline">¶</a></h4>
<p>如前文<a class="reference external" href="#prerequisites-for-building-apache-cloudstack">“Prerequisites for building Apache CloudStack”</a>所示，
在编译之前，需要先做一些环境预设。
我们假定您的编译环境是64位CentOS 或者Red Hat Enterprise
Linux.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum groupinstall &quot;Development Tools&quot;</span>
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum install java-1.7.0-openjdk-devel.x86_64 genisoimage mysql mysql-server ws-commons-util MySQL-python tomcat6 createrepo</span>
</pre></div>
</div>
<p>接下来，需要安装运行时的maven依赖。我们使用的是maven3来管理依赖，所以您需要下载<a class="reference external" href="http://maven.apache.org/download.cgi">Maven 3.0.5 (Binary tar.gz)</a> 并解压到您的根目录（或其它您习惯的目录）：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> ~
$ tar zxvf apache-maven-3.0.5-bin.tar.gz
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ <span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span>~/apache-maven-3.0.5/bin:<span class="nv">$PATH</span>
</pre></div>
</div>
<p>Maven也需要配置Java环境变量（JAVA_HOME）</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ <span class="nb">export</span> <span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/lib/jvm/java-1.7.0-openjdk.x86_64
</pre></div>
</div>
<p>验证Maven是否安装成功：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ mvn --version
</pre></div>
</div>
<p>如果您需要保证环境变量重启后依然有效，需要更新<code class="docutils literal"><span class="pre">~/.bashrc</span></code> 写入PATH 和
JAVA_HOME。</p>
<p>编译RPB很简单。源码已经下载完成并解压到本地目录后，您只需要几分钟即可编译完成。</p>
<div class="admonition note">
<p class="first admonition-title">注意</p>
<p class="last">打包方式和之前有变化。如果您曾编译过其它的CloudStack版本，您需要知道
在使用Apache Maven之后，打包方式有了很大变化。请务必跟随本章节步骤进行打包。</p>
</div>
<div class="section" id="generating-rpms">
<h5>生成RPM<a class="headerlink" href="#generating-rpms" title="Permalink to this headline">¶</a></h5>
<p>环境预设和源码下载已经完成，切换到
<cite>packaging/</cite> 目录。</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> packaging/
</pre></div>
</div>
<p>通过<code class="docutils literal"><span class="pre">package.sh</span></code> 脚本来创建CloudStack包：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ./package.sh -d centos6
</pre></div>
</div>
<p>该脚本会运行一段时间，然后将打包完成的安装包放到
<code class="docutils literal"><span class="pre">dist/rpmbuild/RPMS/x86_64/</span></code>。</p>
<p>您可以在该目录中看到以下RPM：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-agent-4.8.0.el6.x86_64.rpm
cloudstack-awsapi-4.8.0.el6.x86_64.rpm
cloudstack-cli-4.8.0.el6.x86_64.rpm
cloudstack-common-4.8.0.el6.x86_64.rpm
cloudstack-docs-4.8.0.el6.x86_64.rpm
cloudstack-management-4.8.0.el6.x86_64.rpm
cloudstack-usage-4.8.0.el6.x86_64.rpm
</pre></div>
</div>
<div class="section" id="creating-a-yum-repo">
<h6>创建yum源<a class="headerlink" href="#creating-a-yum-repo" title="Permalink to this headline">¶</a></h6>
<p>RPM打包完成后 - 创建Yum网络源就很简单了。按以下步骤创建一个Yum 源：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ mkdir -p ~/tmp/repo
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> ../..
$ cp dist/rpmbuild/RPMS/x86_64/*rpm ~/tmp/repo/
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ createrepo ~/tmp/repo
</pre></div>
</div>
<p>在 <code class="docutils literal"><span class="pre">~/tmp/repo</span></code>中的目录和文件
现在可以上传到一个web server或在yum 源server。</p>
</div>
<div class="section" id="configuring-your-systems-to-use-your-new-yum-repository">
<h6>配置系统使用yum 源<a class="headerlink" href="#configuring-your-systems-to-use-your-new-yum-repository" title="Permalink to this headline">¶</a></h6>
<p>现在yum源已经有了RPM和元数据信息，我们需要配置机器使用此yum源来安装CloudStack。
添加文件<code class="docutils literal"><span class="pre">/etc/yum.repos.d/cloudstack.repo</span></code> 
写入以下信息：</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="o">[</span>apache-cloudstack<span class="o">]</span>
<span class="nv">name</span><span class="o">=</span>Apache CloudStack
<span class="nv">baseurl</span><span class="o">=</span>http://webserver.tld/path/to/repo
<span class="nv">enabled</span><span class="o">=</span>1
<span class="nv">gpgcheck</span><span class="o">=</span>0
</pre></div>
</div>
<p>完成之后，您可以通过yum来网络安装CloudStack了。</p>
</div>
</div>
</div>
<div class="section" id="building-non-oss">
<h4>编译Non-OSS模式<a class="headerlink" href="#building-non-oss" title="Permalink to this headline">¶</a></h4>
<p>如果您的系统需要支持VMware, NetApp, F5, NetScaler, SRX, 或者其它非开源组件（non-Open Source Software【nonoss】），
您需要下载一些其它组件，执行以下步骤（与之前略有不同）。</p>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">CloudStack因License限制，有些可支持的插件不能作为开源产品发布。
其中，一些依赖包需要单独的所有权，两位一些可能需要不符合
<a class="reference external" href="http://www.apache.org/legal/resolved.html#category-x">Apache license标准的第三方包</a>。</p>
</div>
<ol class="arabic">
<li><p class="first">编译Non-OSS插件，需要先将依赖的JAR包放到<code class="docutils literal"><span class="pre">deps</span></code> 目录下。</p>
<p>因为这些模块不能随着CloudStack一起发布，所以需要单独下载。链接参见WIKI页面
 <a class="reference external" href="https://cwiki.apache.org/confluence/display/CLOUDSTACK/How+to+build+CloudStack">*How to build CloudStack*</a>
</p>
</li>
<li><p class="first">同样，您需要下载
<a class="reference external" href="http://download.cloud.com.s3.amazonaws.com/tools/vhd-util">vhd-util</a>,
该文件因license问题，已经从代码结构中移除。您需要手动将vhd-util移动到
<code class="docutils literal"><span class="pre">scripts/vm/hypervisor/xenserver/</span></code> 目录下。</p>
</li>
<li><p class="first">完成以上步骤后，您可以执行以下命令打包CloudStack的
<code class="docutils literal"><span class="pre">noredist</span></code>版本：</p>
</li>
</ol>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ mvn clean
$ mvn install -Dnoredist
</pre></div>
</div>
<ol class="arabic simple">
<li>Once you&#8217;ve built CloudStack with the <code class="docutils literal"><span class="pre">noredist</span></code> profile, you can
package it using the <a class="reference external" href="#building-rpms-from-source">“Building RPMs from Source”</a>
or <a class="reference external" href="#building-deb-packages">“Building DEB packages”</a> instructions.</li>
</ol>
</div>
</div>
</div>
</div>
<div class="section" id="general-installation">
<span id="installation"></span><h2>一般安装<a class="headerlink" href="#general-installation" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-overview/index"></span><div class="section" id="installation-overview">
<h3>安装概述<a class="headerlink" href="#installation-overview" title="Permalink to this headline">¶</a></h3>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="id2">简介</a><ul>
<li><a class="reference internal" href="#who-should-read-this" id="id3">面向用户</a></li>
<li><a class="reference internal" href="#installation-steps" id="id4">安装步骤</a></li>
</ul>
</li>
<li><a class="reference internal" href="#minimum-system-requirements" id="id5">最低系统需求</a><ul>
<li><a class="reference internal" href="#management-server-database-and-storage-system-requirements" id="id6">Management Server, 数据库, 存储系统需求</a></li>
<li><a class="reference internal" href="#host-hypervisor-system-requirements" id="id7">Host/Hypervisor 系统需求</a></li>
</ul>
</li>
<li><a class="reference internal" href="#package-repository" id="id8">package repository</a></li>
</ul>
</div>
<div class="section" id="introduction">
<h4><a class="toc-backref" href="#contents">简介</a><a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h4>
<div class="section" id="who-should-read-this">
<h5><a class="toc-backref" href="#contents">面向用户</a><a class="headerlink" href="#who-should-read-this" title="Permalink to this headline">¶</a></h5>
<p>已经完成设计阶段并且规划了一个比较复杂的实施环境，或者准备好要升级一个落后版本的人。
按照以下步骤，您可以使用CloudStack更为强大的功能，例如高级VLAN网络，HA，附加网络组件例如负载均衡和防火墙，
以及支持多种hypervisor，包括
Citrix XenServer, KVM, and VMware vSphere。</p>
</div>
<div class="section" id="installation-steps">
<h5><a class="toc-backref" href="#contents">安装步骤</a><a class="headerlink" href="#installation-steps" title="Permalink to this headline">¶</a></h5>
<p>For anything more than a simple trial installation, you will need
guidance for a variety of configuration choices. It is strongly
recommended that you read the following:</p>
<ul class="simple">
<li>选择部署架构</li>
<li>选择Hypervisor: 支持的功能</li>
<li>网络设置</li>
<li>存储设置</li>
<li>最佳实践</li>
</ul>
<ol class="arabic simple">
<li>Make sure you have the required hardware ready.
See <a class="reference internal" href="#minimum-system-requirements"><span>Minimum System Requirements</span></a></li>
<li>Install the Management Server (choose single-node or multi-node).
See <a class="reference internal" href="index.html#install-mgt"><span>Management Server Installation</span></a></li>
<li>Configure your cloud. See <a class="reference internal" href="index.html#configuring-your-cloudstack-installation"><span>Configuring your CloudStack Installation</span></a><ol class="arabic">
<li>Using CloudStack UI. See <a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-administration/en/latest/ui.html#log-in-to-the-ui">*User Interface*</a></li>
<li>Add a zone. Includes the first pod, cluster, and host. See <a class="reference internal" href="index.html#adding-a-zone"><span>Adding a Zone</span></a></li>
<li>Add more pods (optional). See <a class="reference internal" href="index.html#adding-a-pod"><span>Adding a Pod</span></a></li>
<li>Add more clusters (optional). See <a class="reference internal" href="index.html#adding-a-cluster"><span>Adding a Cluster</span></a></li>
<li>Add more hosts (optional). See <a class="reference internal" href="index.html#adding-a-host"><span>Adding a Host</span></a></li>
<li>Add more primary storage (optional). See <a class="reference internal" href="index.html#add-primary-storage"><span>Add Primary Storage</span></a></li>
<li>Add more secondary storage (optional). See <a class="reference internal" href="index.html#add-secondary-storage"><span>Add Secondary Storage</span></a></li>
</ol>
</li>
<li>Try using the cloud. See <a class="reference internal" href="index.html#initialize-and-test"><span>Initialize and Test</span></a></li>
</ol>
</div>
</div>
<div class="section" id="minimum-system-requirements">
<span id="id1"></span><h4><a class="toc-backref" href="#contents">Minimum System Requirements</a><a class="headerlink" href="#minimum-system-requirements" title="Permalink to this headline">¶</a></h4>
<div class="section" id="management-server-database-and-storage-system-requirements">
<h5><a class="toc-backref" href="#contents">Management Server, Database, and Storage System Requirements</a><a class="headerlink" href="#management-server-database-and-storage-system-requirements" title="Permalink to this headline">¶</a></h5>
<p>The machines that will run the Management Server and MySQL database must
meet the following requirements. The same machines can also be used to
provide primary and secondary storage, such as via localdisk or NFS. The
Management Server may be placed on a virtual machine.</p>
<ul class="simple">
<li>Operating system:<ul>
<li>Preferred: CentOS/RHEL 6.3+ or Ubuntu 14.04(.2)</li>
</ul>
</li>
<li>64-bit x86 CPU (more cores results in better performance)</li>
<li>4 GB of memory</li>
<li>250 GB of local disk (more results in better capability; 500 GB
recommended)</li>
<li>At least 1 NIC</li>
<li>Statically allocated IP address</li>
<li>Fully qualified domain name as returned by the hostname command</li>
</ul>
</div>
<div class="section" id="host-hypervisor-system-requirements">
<h5><a class="toc-backref" href="#contents">Host/Hypervisor System Requirements</a><a class="headerlink" href="#host-hypervisor-system-requirements" title="Permalink to this headline">¶</a></h5>
<p>The host is where the cloud services run in the form of guest virtual
machines. Each host is one machine that meets the following
requirements:</p>
<ul class="simple">
<li>Must support HVM (Intel-VT or AMD-V enabled).</li>
<li>64-bit x86 CPU (more cores results in better performance)</li>
<li>Hardware virtualization support required</li>
<li>4 GB of memory</li>
<li>36 GB of local disk</li>
<li>At least 1 NIC</li>
<li>Latest hotfixes applied to hypervisor software</li>
<li>When you deploy CloudStack, the hypervisor host must not have any VMs
already running</li>
<li>All hosts within a cluster must be homogeneous. The CPUs must be of
the same type, count, and feature flags.</li>
</ul>
<p>Hosts have additional requirements depending on the hypervisor. See the
requirements listed at the top of the Installation section for your
chosen hypervisor:</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Be sure you fulfill the additional hypervisor requirements and installation
steps provided in this Guide. Hypervisor hosts must be properly prepared to
work with CloudStack. For example, the requirements for XenServer are
listed under Citrix XenServer Installation.</p>
</div>
</div>
</div>
<div class="section" id="package-repository">
<h4><a class="toc-backref" href="#contents">package repository</a><a class="headerlink" href="#package-repository" title="Permalink to this headline">¶</a></h4>
<p>CloudStack is only distributed from source from the official mirrors.
However, members of the CloudStack community may build convenience
binaries so that users can install Apache CloudStack without needing to
build from source.</p>
<p>If you didn&#8217;t follow the steps to build your own packages from source in
the sections for <a class="reference external" href="building_from_source.html#building-rpms-from-source">“Building RPMs from Source”</a> or
<a class="reference external" href="building_from_source.html#building-deb-packages">“Building DEB packages”</a>
you may find pre-built DEB and RPM packages for your convenience linked from
the <a class="reference external" href="http://cloudstack.apache.org/downloads.html">downloads</a> page.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These repositories contain both the Management Server and KVM Hypervisor
packages.</p>
</div>
</div>
</div>
<span id="document-management-server/index"></span><div class="section" id="management-server-installation">
<span id="install-mgt"></span><h3>Management Server Installation<a class="headerlink" href="#management-server-installation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h4>
<p>This section describes installing the Management Server. There are two
slightly different installation flows, depending on how many Management
Server nodes will be in your cloud:</p>
<ul class="simple">
<li>A single Management Server node, with MySQL on the same node.</li>
<li>Multiple Management Server nodes, with MySQL on a node separate from
the Management Servers.</li>
</ul>
<p>In either case, each machine must meet the system requirements described
in <a class="reference internal" href="index.html#minimum-system-requirements"><span>Minimum System Requirements</span></a>.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">For the sake of security, be sure the public Internet can not access port
8096 or port 8250 on the Management Server.</p>
</div>
<p>The procedure for installing the Management Server is:</p>
<ol class="arabic simple">
<li>Prepare the Operating System</li>
<li>(XenServer only) Download and install vhd-util.</li>
<li>Install the First Management Server</li>
<li>Install and Configure the MySQL database</li>
<li>Prepare NFS Shares</li>
<li>Prepare and Start Additional Management Servers (optional)</li>
<li>Prepare the System VM Template</li>
</ol>
</div>
<div class="section" id="prepare-the-operating-system">
<h4>Prepare the Operating System<a class="headerlink" href="#prepare-the-operating-system" title="Permalink to this headline">¶</a></h4>
<p>The OS must be prepared to host the Management Server using the
following steps. These steps must be performed on each Management Server
node.</p>
<ol class="arabic">
<li><p class="first">Log in to your OS as root.</p>
</li>
<li><p class="first">Check for a fully qualified hostname.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>hostname --fqdn
</pre></div>
</div>
<p>This should return a fully qualified hostname such as
&#8220;management1.lab.example.org&#8221;. If it does not, edit /etc/hosts so
that it does.</p>
</li>
<li><p class="first">Make sure that the machine can reach the Internet.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>ping www.cloudstack.org
</pre></div>
</div>
</li>
<li><p class="first">Turn on NTP for time synchronization.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">NTP is required to synchronize the clocks of the servers in your cloud.</p>
</div>
<p>Install NTP.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>yum install ntp
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>sudo apt-get install openntpd
</pre></div>
</div>
</li>
<li><p class="first">Repeat all of these steps on every host where the Management Server
will be installed.</p>
</li>
</ol>
</div>
<div class="section" id="install-the-management-server-on-the-first-host">
<h4>Install the Management Server on the First Host<a class="headerlink" href="#install-the-management-server-on-the-first-host" title="Permalink to this headline">¶</a></h4>
<p>The first step in installation, whether you are installing the
Management Server on one host or many, is to install the software on a
single node.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you are planning to install the Management Server on multiple nodes for
high availability, do not proceed to the additional nodes yet. That step
will come later.</p>
</div>
<p>The CloudStack Management server can be installed using either RPM or
DEB packages. These packages will depend on everything you need to run
the Management server.</p>
<div class="section" id="configure-package-repository">
<h5>Configure package repository<a class="headerlink" href="#configure-package-repository" title="Permalink to this headline">¶</a></h5>
<p>CloudStack is only distributed from source from the official mirrors.
However, members of the CloudStack community may build convenience
binaries so that users can install Apache CloudStack without needing to
build from source.</p>
<p>If you didn&#8217;t follow the steps to build your own packages from source in
the sections for <a class="reference external" href="building_from_source.html#building-rpms-from-source">“Building RPMs from Source”</a> or
<a class="reference external" href="building_from_source.html#building-deb-packages">“Building DEB packages”</a>
you may find pre-built DEB and RPM packages for your convenience linked from
the <a class="reference external" href="http://cloudstack.apache.org/downloads.html">downloads</a> page.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These repositories contain both the Management Server and KVM Hypervisor
packages.</p>
</div>
<div class="section" id="rpm-package-repository">
<h6>RPM package repository<a class="headerlink" href="#rpm-package-repository" title="Permalink to this headline">¶</a></h6>
<p>There is a RPM package repository for CloudStack so you can easily
install on RHEL based platforms.</p>
<p>If you&#8217;re using an RPM-based system, you&#8217;ll want to add the Yum
repository so that you can install CloudStack with Yum.</p>
<p>Yum repository information is found under <code class="docutils literal"><span class="pre">/etc/yum.repos.d</span></code>. You&#8217;ll
see several <code class="docutils literal"><span class="pre">.repo</span></code> files in this directory, each one denoting a
specific repository.</p>
<p>To add the CloudStack repository, create
<code class="docutils literal"><span class="pre">/etc/yum.repos.d/cloudstack.repo</span></code> and insert the following
information.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="o">[</span>cloudstack<span class="o">]</span>
<span class="nv">name</span><span class="o">=</span>cloudstack
<span class="nv">baseurl</span><span class="o">=</span>http://cloudstack.apt-get.eu/centos/6/4.8/
<span class="nv">enabled</span><span class="o">=</span>1
<span class="nv">gpgcheck</span><span class="o">=</span>0
</pre></div>
</div>
<p>Now you should be able to install CloudStack using Yum.</p>
</div>
<div class="section" id="deb-package-repository">
<h6>DEB package repository<a class="headerlink" href="#deb-package-repository" title="Permalink to this headline">¶</a></h6>
<p>You can add a DEB package repository to your apt sources with the
following commands. Please note that only packages for Ubuntu 12.04 LTS
(precise) and Ubuntu 14.04 (trusty) are being built at this time.</p>
<p>Use your preferred editor and open (or create)
<code class="docutils literal"><span class="pre">/etc/apt/sources.list.d/cloudstack.list</span></code>. Add the community provided
repository to the file:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>deb http://cloudstack.apt-get.eu/ubuntu precise 4.8
</pre></div>
</div>
<p>We now have to add the public key to the trusted keys.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>sudo wget -O - http://cloudstack.apt-get.eu/release.asc<span class="p">|</span>apt-key add -
</pre></div>
</div>
<p>Now update your local apt cache.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>sudo apt-get update
</pre></div>
</div>
<p>Your DEB package repository should now be configured and ready for use.</p>
</div>
</div>
<div class="section" id="install-on-centos-rhel">
<h5>Install on CentOS/RHEL<a class="headerlink" href="#install-on-centos-rhel" title="Permalink to this headline">¶</a></h5>
<div class="highlight-bash"><div class="highlight"><pre><span></span>yum install cloudstack-management
</pre></div>
</div>
</div>
<div class="section" id="install-on-ubuntu">
<h5>Install on Ubuntu<a class="headerlink" href="#install-on-ubuntu" title="Permalink to this headline">¶</a></h5>
<div class="highlight-bash"><div class="highlight"><pre><span></span>sudo apt-get install cloudstack-management
</pre></div>
</div>
</div>
</div>
<div class="section" id="downloading-vhd-util">
<h4>Downloading vhd-util<a class="headerlink" href="#downloading-vhd-util" title="Permalink to this headline">¶</a></h4>
<p>This procedure is required only for installations where XenServer is
installed on the hypervisor hosts.</p>
<p>Before setting up the Management Server, download
<a class="reference external" href="http://download.cloud.com.s3.amazonaws.com/tools/vhd-util">vhd-util</a> from
<a class="reference external" href="http://download.cloud.com.s3.amazonaws.com/tools/vhd-util">http://download.cloud.com.s3.amazonaws.com/tools/vhd-util</a>.
and copy it into <code class="docutils literal"><span class="pre">/usr/share/cloudstack-common/scripts/vm/hypervisor/xenserver</span></code>
of the Management Server.</p>
</div>
<div class="section" id="install-the-database-server">
<h4>Install the database server<a class="headerlink" href="#install-the-database-server" title="Permalink to this headline">¶</a></h4>
<p>The CloudStack management server uses a MySQL database server to store
its data. When you are installing the management server on a single
node, you can install the MySQL server locally. For an installation that
has multiple management server nodes, we assume the MySQL database also
runs on a separate node.</p>
<p>CloudStack has been tested with MySQL 5.1 and 5.5. These versions are
included in RHEL/CentOS and Ubuntu.</p>
<div class="section" id="install-the-database-on-the-management-server-node">
<h5>Install the Database on the Management Server Node<a class="headerlink" href="#install-the-database-on-the-management-server-node" title="Permalink to this headline">¶</a></h5>
<p>This section describes how to install MySQL on the same machine with the
Management Server. This technique is intended for a simple deployment
that has a single Management Server node. If you have a multi-node
Management Server deployment, you will typically use a separate node for
MySQL. See <a class="reference internal" href="#install-database-on-separate-node"><span>Install the Database on a Separate Node</span></a>.</p>
<ol class="arabic">
<li><p class="first">Install MySQL from the package repository of your distribution:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>yum install mysql-server
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>sudo apt-get install mysql-server
</pre></div>
</div>
</li>
<li><p class="first">Open the MySQL configuration file. The configuration file is
<code class="docutils literal"><span class="pre">/etc/my.cnf</span></code> or <code class="docutils literal"><span class="pre">/etc/mysql/my.cnf</span></code>, depending on your OS.</p>
<p>Insert the following lines in the <code class="docutils literal"><span class="pre">[mysqld]</span></code> section.</p>
<p>You can put these lines below the datadir line. The max_connections
parameter should be set to 350 multiplied by the number of Management
Servers you are deploying. This example assumes one Management
Server.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">innodb_rollback_on_timeout</span><span class="o">=</span>1
<span class="nv">innodb_lock_wait_timeout</span><span class="o">=</span>600
<span class="nv">max_connections</span><span class="o">=</span>350
log-bin<span class="o">=</span>mysql-bin
binlog-format <span class="o">=</span> <span class="s1">&#39;ROW&#39;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also create a file <code class="docutils literal"><span class="pre">/etc/mysql/conf.d/cloudstack.cnf</span></code>
and add these directives there. Don&#8217;t forget to add <code class="docutils literal"><span class="pre">[mysqld]</span></code> on the
first line of the file.</p>
</div>
</li>
<li><p class="first">Start or restart MySQL to put the new configuration into effect.</p>
<p>On RHEL/CentOS, MySQL doesn&#8217;t automatically start after installation.
Start it manually.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>service mysqld start
</pre></div>
</div>
<p>On Ubuntu, restart MySQL.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>sudo service mysql restart
</pre></div>
</div>
</li>
<li><p class="first">(CentOS and RHEL only; not required on Ubuntu)</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">On RHEL and CentOS, MySQL does not set a root password by default. It is
very strongly recommended that you set a root password as a security
precaution.</p>
</div>
<p>Run the following command to secure your installation. You can answer &#8220;Y&#8221;
to all questions.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mysql_secure_installation
</pre></div>
</div>
</li>
<li><p class="first">CloudStack can be blocked by security mechanisms, such as SELinux.
Disable SELinux to ensure + that the Agent has all the required
permissions.</p>
<p>Configure SELinux (RHEL and CentOS):</p>
<ol class="arabic">
<li><p class="first">Check whether SELinux is installed on your machine. If not, you
can skip this section.</p>
<p>In RHEL or CentOS, SELinux is installed and enabled by default.
You can verify this with:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>rpm -qa <span class="p">|</span> grep selinux
</pre></div>
</div>
</li>
<li><p class="first">Set the SELINUX variable in <code class="docutils literal"><span class="pre">/etc/selinux/config</span></code> to
&#8220;permissive&#8221;. This ensures that the permissive setting will be
maintained after a system reboot.</p>
<p>In RHEL or CentOS:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vi /etc/selinux/config
</pre></div>
</div>
<p>Change the following line</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">SELINUX</span><span class="o">=</span>enforcing
</pre></div>
</div>
<p>to this:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">SELINUX</span><span class="o">=</span>permissive
</pre></div>
</div>
</li>
<li><p class="first">Set SELinux to permissive starting immediately, without requiring
a system reboot.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>setenforce permissive
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Set up the database. The following command creates the &#8220;cloud&#8221; user
on the database.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-setup-databases cloud:&lt;dbpassword&gt;@localhost <span class="se">\</span>
--deploy-as<span class="o">=</span>root:&lt;password&gt; <span class="se">\</span>
-e &lt;encryption_type&gt; <span class="se">\</span>
-m &lt;management_server_key&gt; <span class="se">\</span>
-k &lt;database_key&gt; <span class="se">\</span>
-i &lt;management_server_ip&gt;
</pre></div>
</div>
<ul class="simple">
<li>In dbpassword, specify the password to be assigned to the &#8220;cloud&#8221;
user. You can choose to provide no password although that is not
recommended.</li>
<li>In deploy-as, specify the username and password of the user
deploying the database. In the following command, it is assumed
the root user is deploying the database and creating the &#8220;cloud&#8221;
user.</li>
<li>(Optional) For encryption_type, use file or web to indicate the
technique used to pass in the database encryption password.
Default: file. See <a class="reference internal" href="index.html#about-password-key-encryption"><span>About Password and Key Encryption</span></a>.</li>
<li>(Optional) For management_server_key, substitute the default key
that is used to encrypt confidential parameters in the CloudStack
properties file. Default: password. It is highly recommended that
you replace this with a more secure value. See
<a class="reference internal" href="index.html#about-password-key-encryption"><span>About Password and Key Encryption</span></a>.</li>
<li>(Optional) For database_key, substitute the default key that is
used to encrypt confidential parameters in the CloudStack
database. Default: password. It is highly recommended that you
replace this with a more secure value. See
<a class="reference internal" href="index.html#about-password-key-encryption"><span>About Password and Key Encryption</span></a>.</li>
<li>(Optional) For management_server_ip, you may explicitly specify
cluster management server node IP. If not specified, the local IP
address will be used.</li>
</ul>
<p>When this script is finished, you should see a message like
“Successfully initialized the database.”</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the script is unable to connect to the MySQL database, check the
&#8220;localhost&#8221; loopback address in <code class="docutils literal"><span class="pre">/etc/hosts</span></code>. It should be pointing to
the IPv4 loopback address &#8220;127.0.0.1&#8221; and not the IPv6 loopback address
<code class="docutils literal"><span class="pre">::1</span></code>. Alternatively, reconfigure MySQL to bind to the IPv6 loopback
interface.</p>
</div>
</li>
<li><p class="first">If you are running the KVM hypervisor on the same machine with the
Management Server, edit /etc/sudoers and add the following line:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>Defaults:cloud !requiretty
</pre></div>
</div>
</li>
<li><p class="first">Now that the database is set up, you can finish configuring the OS
for the Management Server. This command will set up iptables,
sudoers, and start the Management Server.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-setup-management
</pre></div>
</div>
<p>You should get the output message “CloudStack Management Server setup is
done.”
If the servlet container is Tomcat7 the argument &#8211;tomcat7 must be used.</p>
</li>
</ol>
</div>
<div class="section" id="install-the-database-on-a-separate-node">
<span id="install-database-on-separate-node"></span><h5>Install the Database on a Separate Node<a class="headerlink" href="#install-the-database-on-a-separate-node" title="Permalink to this headline">¶</a></h5>
<p>This section describes how to install MySQL on a standalone machine,
separate from the Management Server. This technique is intended for a
deployment that includes several Management Server nodes. If you have a
single-node Management Server deployment, you will typically use the
same node for MySQL. See <a class="reference external" href="#install-the-database-on-the-management-server-node">“Install the Database on the Management Server Node”</a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The management server doesn&#8217;t require a specific distribution for the MySQL
node. You can use a distribution or Operating System of your choice. Using
the same distribution as the management server is recommended, but not
required. See <a class="reference external" href="#management-server-database-and-storage-system-requirements">“Management Server, Database, and Storage System Requirements”</a>.</p>
</div>
<ol class="arabic">
<li><p class="first">Install MySQL from the package repository from your distribution:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>yum install mysql-server
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>sudo apt-get install mysql-server
</pre></div>
</div>
</li>
<li><p class="first">Edit the MySQL configuration (/etc/my.cnf or /etc/mysql/my.cnf,
depending on your OS) and insert the following lines in the [mysqld]
section. You can put these lines below the datadir line. The
max_connections parameter should be set to 350 multiplied by the
number of Management Servers you are deploying. This example assumes
two Management Servers.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">On Ubuntu, you can also create /etc/mysql/conf.d/cloudstack.cnf file and
add these directives there. Don&#8217;t forget to add [mysqld] on the first
line of the file.</p>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">innodb_rollback_on_timeout</span><span class="o">=</span>1
<span class="nv">innodb_lock_wait_timeout</span><span class="o">=</span>600
<span class="nv">max_connections</span><span class="o">=</span>700
log-bin<span class="o">=</span>mysql-bin
binlog-format <span class="o">=</span> <span class="s1">&#39;ROW&#39;</span>
bind-address <span class="o">=</span> 0.0.0.0
</pre></div>
</div>
</li>
<li><p class="first">Start or restart MySQL to put the new configuration into effect.</p>
<p>On RHEL/CentOS, MySQL doesn&#8217;t automatically start after installation.
Start it manually.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>service mysqld start
</pre></div>
</div>
<p>On Ubuntu, restart MySQL.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>sudo service mysql restart
</pre></div>
</div>
</li>
<li><p class="first">(CentOS and RHEL only; not required on Ubuntu)</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">On RHEL and CentOS, MySQL does not set a root password by default. It is
very strongly recommended that you set a root password as a security
precaution. Run the following command to secure your installation. You
can answer &#8220;Y&#8221; to all questions except &#8220;Disallow root login remotely?&#8221;.
Remote root login is required to set up the databases.</p>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mysql_secure_installation
</pre></div>
</div>
</li>
<li><p class="first">If a firewall is present on the system, open TCP port 3306 so
external MySQL connections can be established.</p>
<p>On Ubuntu, UFW is the default firewall. Open the port with this
command:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>ufw allow mysql
</pre></div>
</div>
<p>On RHEL/CentOS:</p>
<ol class="arabic">
<li><p class="first">Edit the /etc/sysconfig/iptables file and add the following line
at the beginning of the INPUT chain.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>-A INPUT -p tcp --dport <span class="m">3306</span> -j ACCEPT
</pre></div>
</div>
</li>
<li><p class="first">Now reload the iptables rules.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>service iptables restart
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Return to the root shell on your first Management Server.</p>
</li>
<li><p class="first">Set up the database. The following command creates the cloud user on
the database.</p>
<ul class="simple">
<li>In dbpassword, specify the password to be assigned to the cloud
user. You can choose to provide no password.</li>
<li>In deploy-as, specify the username and password of the user
deploying the database. In the following command, it is assumed
the root user is deploying the database and creating the cloud
user.</li>
<li>(Optional) For encryption_type, use file or web to indicate the
technique used to pass in the database encryption password.
Default: file. See <a class="reference internal" href="index.html#about-password-key-encryption"><span>About Password and Key Encryption</span></a>.</li>
<li>(Optional) For management_server_key, substitute the default key
that is used to encrypt confidential parameters in the CloudStack
properties file. Default: password. It is highly recommended that
you replace this with a more secure value. See About Password and
Key Encryption.</li>
<li>(Optional) For database_key, substitute the default key that is
used to encrypt confidential parameters in the CloudStack
database. Default: password. It is highly recommended that you
replace this with a more secure value. See
<a class="reference internal" href="index.html#about-password-key-encryption"><span>About Password and Key Encryption</span></a>.</li>
<li>(Optional) For management_server_ip, you may explicitly specify
cluster management server node IP. If not specified, the local IP
address will be used.</li>
</ul>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-setup-databases cloud:&lt;dbpassword&gt;@&lt;ip address mysql server&gt; <span class="se">\</span>
--deploy-as<span class="o">=</span>root:&lt;password&gt; <span class="se">\</span>
-e &lt;encryption_type&gt; <span class="se">\</span>
-m &lt;management_server_key&gt; <span class="se">\</span>
-k &lt;database_key&gt; <span class="se">\</span>
-i &lt;management_server_ip&gt;
</pre></div>
</div>
<p>When this script is finished, you should see a message like
“Successfully initialized the database.”</p>
</li>
<li><p class="first">Now that the database is set up, you can finish configuring the OS
for the Management Server. This command will set up iptables,
sudoers, and start the Management Server.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-setup-management
</pre></div>
</div>
<p>You should get the output message “CloudStack Management Server setup is
done.”</p>
</li>
</ol>
</div>
</div>
<div class="section" id="prepare-nfs-shares">
<span id="id2"></span><h4>Prepare NFS Shares<a class="headerlink" href="#prepare-nfs-shares" title="Permalink to this headline">¶</a></h4>
<p>CloudStack needs a place to keep primary and secondary storage (see
Cloud Infrastructure Overview). Both of these can be NFS shares. This
section tells how to set up the NFS shares before adding the storage to
CloudStack.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">NFS is not the only option for primary or secondary storage. For example,
you may use Ceph RBD, GlusterFS, iSCSI, and others. The choice of storage
system will depend on the choice of hypervisor and whether you are dealing
with primary or secondary storage.</p>
</div>
<p>The requirements for primary and secondary storage are described in:</p>
<ul class="simple">
<li><a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html#about-primary-storage">“About Primary Storage”</a></li>
<li><a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html#about-secondary-storage">“About Secondary Storage”</a></li>
</ul>
<p>A production installation typically uses a separate NFS server.
See <a class="reference internal" href="#using-a-separage-nfs-server"><span>Using a Separate NFS Server</span></a>.</p>
<p>You can also use the Management Server node as the NFS server. This is
more typical of a trial installation, but is technically possible in a
larger deployment. See <a class="reference internal" href="#using-the-management-server-as-the-nfs-server"><span>Using the Management Server as the NFS Server</span></a>.</p>
<div class="section" id="using-a-separate-nfs-server">
<span id="using-a-separage-nfs-server"></span><h5>Using a Separate NFS Server<a class="headerlink" href="#using-a-separate-nfs-server" title="Permalink to this headline">¶</a></h5>
<p>This section tells how to set up NFS shares for secondary and
(optionally) primary storage on an NFS server running on a separate node
from the Management Server.</p>
<p>The exact commands for the following steps may vary depending on your
operating system version.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">(KVM only) Ensure that no volume is already mounted at your NFS mount point.</p>
</div>
<ol class="arabic">
<li><p class="first">On the storage server, create an NFS share for secondary storage and,
if you are using NFS for primary storage as well, create a second NFS
share. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mkdir -p /export/primary
mkdir -p /export/secondary
</pre></div>
</div>
</li>
<li><p class="first">To configure the new directories as NFS exports, edit /etc/exports.
Export the NFS share(s) with
rw,async,no_root_squash,no_subtree_check. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vi /etc/exports
</pre></div>
</div>
<p>Insert the following line.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/export  *<span class="o">(</span>rw,async,no_root_squash,no_subtree_check<span class="o">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Export the /export directory.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>exportfs -a
</pre></div>
</div>
</li>
<li><p class="first">On the management server, create a mount point for secondary storage.
For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mkdir -p /mnt/secondary
</pre></div>
</div>
</li>
<li><p class="first">Mount the secondary storage on your Management Server. Replace the
example NFS server name and NFS share paths below with your own.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mount -t nfs nfsservername:/nfs/share/secondary /mnt/secondary
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="using-the-management-server-as-the-nfs-server">
<span id="id3"></span><h5>Using the Management Server as the NFS Server<a class="headerlink" href="#using-the-management-server-as-the-nfs-server" title="Permalink to this headline">¶</a></h5>
<p>This section tells how to set up NFS shares for primary and secondary
storage on the same node with the Management Server. This is more
typical of a trial installation, but is technically possible in a larger
deployment. It is assumed that you will have less than 16TB of storage
on the host.</p>
<p>The exact commands for the following steps may vary depending on your
operating system version.</p>
<ol class="arabic">
<li><p class="first">On RHEL/CentOS systems, you&#8217;ll need to install the nfs-utils package:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>yum install nfs-utils
</pre></div>
</div>
</li>
<li><p class="first">On the Management Server host, create two directories that you will
use for primary and secondary storage. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mkdir -p /export/primary
mkdir -p /export/secondary
</pre></div>
</div>
</li>
<li><p class="first">To configure the new directories as NFS exports, edit /etc/exports.
Export the NFS share(s) with
rw,async,no_root_squash,no_subtree_check. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vi /etc/exports
</pre></div>
</div>
<p>Insert the following line.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/export  *<span class="o">(</span>rw,async,no_root_squash,no_subtree_check<span class="o">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Export the /export directory.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>exportfs -a
</pre></div>
</div>
</li>
<li><p class="first">Edit the /etc/sysconfig/nfs file.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vi /etc/sysconfig/nfs
</pre></div>
</div>
<p>Uncomment the following lines:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">LOCKD_TCPPORT</span><span class="o">=</span>32803
<span class="nv">LOCKD_UDPPORT</span><span class="o">=</span>32769
<span class="nv">MOUNTD_PORT</span><span class="o">=</span>892
<span class="nv">RQUOTAD_PORT</span><span class="o">=</span>875
<span class="nv">STATD_PORT</span><span class="o">=</span>662
<span class="nv">STATD_OUTGOING_PORT</span><span class="o">=</span>2020
</pre></div>
</div>
</li>
<li><p class="first">Edit the /etc/sysconfig/iptables file.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vi /etc/sysconfig/iptables
</pre></div>
</div>
<p>Add the following lines at the beginning of the INPUT chain, where
&lt;NETWORK&gt; is the network that you&#8217;ll be using:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p udp --dport <span class="m">111</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p tcp --dport <span class="m">111</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p tcp --dport <span class="m">2049</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p tcp --dport <span class="m">32803</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p udp --dport <span class="m">32769</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p tcp --dport <span class="m">892</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p udp --dport <span class="m">892</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p tcp --dport <span class="m">875</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p udp --dport <span class="m">875</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p tcp --dport <span class="m">662</span> -j ACCEPT
-A INPUT -s &lt;NETWORK&gt; -m state --state NEW -p udp --dport <span class="m">662</span> -j ACCEPT
</pre></div>
</div>
</li>
<li><p class="first">Run the following commands:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>service iptables restart
service iptables save
</pre></div>
</div>
</li>
<li><p class="first">If NFS v4 communication is used between client and server, add your
domain to /etc/idmapd.conf on both the hypervisor host and Management
Server.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vi /etc/idmapd.conf
</pre></div>
</div>
<p>Remove the character # from the beginning of the Domain line in
idmapd.conf and replace the value in the file with your own domain.
In the example below, the domain is company.com.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">Domain</span> <span class="o">=</span> company.com
</pre></div>
</div>
</li>
<li><p class="first">Reboot the Management Server host.</p>
<p>Two NFS shares called /export/primary and /export/secondary are now
set up.</p>
</li>
<li><p class="first">It is recommended that you test to be sure the previous steps have
been successful.</p>
<ol class="arabic">
<li><p class="first">Log in to the hypervisor host.</p>
</li>
<li><p class="first">Be sure NFS and rpcbind are running. The commands might be
different depending on your OS. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>service rpcbind start
service nfs start
chkconfig nfs on
chkconfig rpcbind on
reboot
</pre></div>
</div>
</li>
<li><p class="first">Log back in to the hypervisor host and try to mount the /export
directories. For example, substitute your own management server
name:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mkdir /primary
mount -t nfs &lt;management-server-name&gt;:/export/primary
umount /primary
mkdir /secondary
mount -t nfs &lt;management-server-name&gt;:/export/secondary
umount /secondary
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>
<div class="section" id="additional-management-servers">
<h4>Additional Management Servers<a class="headerlink" href="#additional-management-servers" title="Permalink to this headline">¶</a></h4>
<p>For your second and subsequent Management Servers, you will install the
Management Server software, connect it to the database, and set up the
OS for the Management Server.</p>
<ol class="arabic">
<li><p class="first">Perform the steps in <a class="reference external" href="#prepare-the-operating-system">“Prepare the Operating System”</a> and <a class="reference external" href="building_from_source.html#building-rpms-from-source">“Building RPMs from Source”</a> or
<a class="reference external" href="building_from_source.html#building-deb-packages">“Building DEB packages”</a> as appropriate.</p>
</li>
<li><p class="first">This step is required only for installations where XenServer is
installed on the hypervisor hosts.</p>
<p>Download vhd-util from
<a class="reference external" href="http://download.cloud.com.s3.amazonaws.com/tools/vhd-util">vhd-util</a></p>
<p>Copy vhd-util to
<code class="docutils literal"><span class="pre">/usr/share/cloudstack-common/scripts/vm/hypervisor/xenserver</span></code>.</p>
</li>
<li><p class="first">Ensure that necessary services are started and set to start on boot.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>service rpcbind start
service nfs start
chkconfig nfs on
chkconfig rpcbind on
</pre></div>
</div>
</li>
<li><p class="first">Configure the database client. Note the absence of the &#8211;deploy-as
argument in this case. (For more details about the arguments to this
command, see <a class="reference internal" href="#install-database-on-separate-node"><span>Install the Database on a Separate Node</span></a>.)</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-setup-databases cloud:dbpassword@dbhost <span class="se">\</span>
-e encryption_type <span class="se">\</span>
-m management_server_key <span class="se">\</span>
-k database_key <span class="se">\</span>
-i management_server_ip
</pre></div>
</div>
</li>
<li><p class="first">Configure the OS and start the Management Server:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-setup-management
</pre></div>
</div>
<p>The Management Server on this node should now be running.
If the servlet container is Tomcat7 the argument &#8211;tomcat7 must be used.</p>
</li>
<li><p class="first">Repeat these steps on each additional Management Server.</p>
</li>
<li><p class="first">Be sure to configure a load balancer for the Management Servers. See
<a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/administration_guide.html?highlight=management%20server%20load#management-server-load-balancing">“Management Server Load Balancing”</a>.</p>
</li>
</ol>
</div>
<div class="section" id="prepare-the-system-vm-template">
<h4>Prepare the System VM Template<a class="headerlink" href="#prepare-the-system-vm-template" title="Permalink to this headline">¶</a></h4>
<p>Secondary storage must be seeded with a template that is used for
CloudStack system VMs.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When copying and pasting a command, be sure the command has pasted as a
single line before executing. Some document viewers may introduce unwanted
line breaks in copied text.</p>
</div>
<ol class="arabic">
<li><p class="first">On the Management Server, run one or more of the following
<code class="docutils literal"><span class="pre">cloud-install-sys-tmplt</span></code> commands to retrieve and decompress the
system VM template. Run the command for each hypervisor type that you
expect end users to run in this Zone.</p>
<p>If your secondary storage mount point is not named <code class="docutils literal"><span class="pre">/mnt/secondary</span></code>,
substitute your own mount point name.</p>
<p>If you set the CloudStack database encryption type to &#8220;web&#8221; when you
set up the database, you must now add the parameter <code class="docutils literal"><span class="pre">-s</span>
<span class="pre">&lt;management-server-secret-key&gt;</span></code>. See <a class="reference internal" href="index.html#about-password-key-encryption"><span>About Password and Key Encryption</span></a>.</p>
<p>This process will require approximately 5 GB of free space on the
local file system and up to 30 minutes each time it runs.</p>
<ul>
<li><p class="first">For Hyper-V</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt <span class="se">\</span>
-m /mnt/secondary <span class="se">\</span>
-u http://cloudstack.apt-get.eu/systemvm/4.6/systemvm64template-4.6.0-hyperv.vhd.zip <span class="se">\</span>
-h hyperv <span class="se">\</span>
-s &lt;optional-management-server-secret-key&gt; <span class="se">\</span>
-F
</pre></div>
</div>
</li>
<li><p class="first">For XenServer:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt <span class="se">\</span>
-m /mnt/secondary <span class="se">\</span>
-u http://cloudstack.apt-get.eu/systemvm/4.6/systemvm64template-4.6.0-xen.vhd.bz2 <span class="se">\</span>
-h xenserver <span class="se">\</span>
-s &lt;optional-management-server-secret-key&gt; <span class="se">\</span>
-F
</pre></div>
</div>
</li>
<li><p class="first">For vSphere:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt <span class="se">\</span>
-m /mnt/secondary <span class="se">\</span>
-u http://cloudstack.apt-get.eu/systemvm/4.6/systemvm64template-4.6.0-vmware.ova <span class="se">\</span>
-h vmware <span class="se">\</span>
-s &lt;optional-management-server-secret-key&gt; <span class="se">\</span>
-F
</pre></div>
</div>
</li>
<li><p class="first">For KVM:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt <span class="se">\</span>
-m /mnt/secondary <span class="se">\</span>
-u http://cloudstack.apt-get.eu/systemvm/4.6/systemvm64template-4.6.0-kvm.qcow2.bz2 <span class="se">\</span>
-h kvm <span class="se">\</span>
-s &lt;optional-management-server-secret-key&gt; <span class="se">\</span>
-F
</pre></div>
</div>
</li>
<li><p class="first">For LXC:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt <span class="se">\</span>
-m /mnt/secondary <span class="se">\</span>
-u http://cloudstack.apt-get.eu/systemvm/4.6/systemvm64template-4.6.0-kvm.qcow2.bz2 <span class="se">\</span>
-h lxc <span class="se">\</span>
-s &lt;optional-management-server-secret-key&gt; <span class="se">\</span>
-F
</pre></div>
</div>
</li>
<li><p class="first">For OVM3:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt <span class="se">\</span>
-m /mnt/secondary <span class="se">\</span>
-u http://cloudstack.apt-get.eu/systemvm/4.6/systemvm64template-4.6.0-ovm.raw.bz2 <span class="se">\</span>
-h ovm3 <span class="se">\</span>
-s &lt;optional-management-server-secret-key&gt; <span class="se">\</span>
-F
</pre></div>
</div>
</li>
</ul>
</li>
<li><p class="first">If you are using a separate NFS server, perform this step. If you are
using the Management Server as the NFS server, you MUST NOT perform
this step.</p>
<p>When the script has finished, unmount secondary storage and remove
the created directory.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>umount /mnt/secondary
rmdir /mnt/secondary
</pre></div>
</div>
</li>
<li><p class="first">Repeat these steps for each secondary storage server.</p>
</li>
</ol>
</div>
<div class="section" id="installation-complete-next-steps">
<h4>Installation Complete! Next Steps<a class="headerlink" href="#installation-complete-next-steps" title="Permalink to this headline">¶</a></h4>
<p>Congratulations! You have now installed CloudStack Management Server and
the database it uses to persist system data.</p>
<p><img alt="installation-complete.png: Finished installs with single Management Server and multiple Management Servers" src="_images/installation-complete.png" /></p>
<p>What should you do next?</p>
<ul class="simple">
<li>Even without adding any cloud infrastructure, you can run the UI to
get a feel for what&#8217;s offered and how you will interact with
CloudStack on an ongoing basis. See Log In to the UI.</li>
<li>When you&#8217;re ready, add the cloud infrastructure and try running some
virtual machines on it, so you can watch how CloudStack manages the
infrastructure. See Provision Your Cloud Infrastructure.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="section" id="configuration">
<span id="id1"></span><h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-configuration"></span><div class="section" id="configuring-your-cloudstack-installation">
<span id="id1"></span><h3>Configuring your CloudStack Installation<a class="headerlink" href="#configuring-your-cloudstack-installation" title="Permalink to this headline">¶</a></h3>
<p>This section tells how to add regions, zones, pods, clusters, hosts,
storage, and networks to your cloud. If you are unfamiliar with these
entities, please begin by looking through <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html#cloud-infrastructure-concepts">*Cloud Infrastructure Concepts*</a>.</p>
<div class="section" id="overview-of-provisioning-steps">
<h4>Overview of Provisioning Steps<a class="headerlink" href="#overview-of-provisioning-steps" title="Permalink to this headline">¶</a></h4>
<p>After the Management Server is installed and running, you can add the
compute resources for it to manage. For an overview of how a CloudStack
cloud infrastructure is organized, see <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html#cloud-infrastructure-overview">“Cloud Infrastructure Overview”</a>.</p>
<p>To provision the cloud infrastructure, or to scale it up at any time,
follow these procedures:</p>
<ol class="arabic simple">
<li>Define regions (optional). See <a class="reference internal" href="#adding-regions"><span>Adding Regions (optional)</span></a>.</li>
<li>Add a zone to the region. See <a class="reference internal" href="#adding-a-zone"><span>Adding a Zone</span></a>.</li>
<li>Add more pods to the zone (optional). See <a class="reference internal" href="#adding-a-pod"><span>Adding a Pod</span></a>.</li>
<li>Add more clusters to the pod (optional). See <a class="reference internal" href="#adding-a-cluster"><span>Adding a Cluster</span></a>.</li>
<li>Add more hosts to the cluster (optional). See <a class="reference internal" href="#adding-a-host"><span>Adding a Host</span></a>.</li>
<li>Add primary storage to the cluster. See <a class="reference internal" href="#add-primary-storage"><span>Add Primary Storage</span></a>.</li>
<li>Add secondary storage to the zone. See <a class="reference internal" href="#add-secondary-storage"><span>Add Secondary Storage</span></a>.</li>
<li>Initialize and test the new cloud. See <a class="reference internal" href="#initialize-and-test"><span>Initialize and Test</span></a>.</li>
</ol>
<p>When you have finished these steps, you will have a deployment with the
following basic structure:</p>
<p><img alt="provisioning-overview.png: Conceptual overview of a basic deployment" src="_images/provisioning-overview.png" /></p>
</div>
<div class="section" id="adding-regions-optional">
<span id="adding-regions"></span><h4>Adding Regions (optional)<a class="headerlink" href="#adding-regions-optional" title="Permalink to this headline">¶</a></h4>
<p>Grouping your cloud resources into geographic regions is an optional
step when provisioning the cloud. For an overview of regions, see
<a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html#about-regions">“About Regions”</a>.</p>
<div class="section" id="the-first-region-the-default-region">
<h5>The First Region: The Default Region<a class="headerlink" href="#the-first-region-the-default-region" title="Permalink to this headline">¶</a></h5>
<p>If you do not take action to define regions, then all the zones in your
cloud will be automatically grouped into a single default region. This
region is assigned the region ID of 1. You can change the name or URL of
the default region by displaying the region in the CloudStack UI and
clicking the Edit button.</p>
</div>
<div class="section" id="adding-a-region">
<h5>Adding a Region<a class="headerlink" href="#adding-a-region" title="Permalink to this headline">¶</a></h5>
<p>Use these steps to add a second region in addition to the default
region.</p>
<ol class="arabic">
<li><p class="first">Each region has its own CloudStack instance. Therefore, the first
step of creating a new region is to install the Management Server
software, on one or more nodes, in the geographic area where you want
to set up the new region. Use the steps in the Installation guide.
When you come to the step where you set up the database, use the
additional command-line flag <code class="docutils literal"><span class="pre">-r</span> <span class="pre">&lt;region_id&gt;</span></code> to set a region ID
for the new region. The default region is automatically assigned a
region ID of 1, so your first additional region might be region 2.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># cloudstack-setup-databases cloud:&lt;dbpassword&gt;@localhost --deploy-as=root:&lt;password&gt; -e &lt;encryption_type&gt; -m &lt;management_server_key&gt; -k &lt;database_key&gt; -r &lt;region_id&gt;</span>
</pre></div>
</div>
</li>
<li><p class="first">By the end of the installation procedure, the Management Server
should have been started. Be sure that the Management Server
installation was successful and complete.</p>
</li>
<li><p class="first">Now add the new region to region 1 in CloudStack.</p>
<ol class="arabic simple">
<li>Log in to CloudStack in the first region as root administrator
(that is, log in to &lt;region.1.IP.address&gt;:8080/client).</li>
<li>In the left navigation bar, click Regions.</li>
<li>Click Add Region. In the dialog, fill in the following fields:<ul>
<li>ID. A unique identifying number. Use the same number you set in
the database during Management Server installation in the new
region; for example, 2.</li>
<li>Name. Give the new region a descriptive name.</li>
<li>Endpoint. The URL where you can log in to the Management Server
in the new region. This has the format
&lt;region.2.IP.address&gt;:8080/client.</li>
</ul>
</li>
</ol>
</li>
<li><p class="first">Now perform the same procedure in reverse. Log in to region 2, and
add region 1.</p>
</li>
<li><p class="first">Copy the account, user, and domain tables from the region 1 database
to the region 2 database.</p>
<p>In the following commands, it is assumed that you have set the root
password on the database, which is a CloudStack recommended best
practice. Substitute your own MySQL root password.</p>
<ol class="arabic">
<li><p class="first">First, run this command to copy the contents of the database:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysqldump -u root -p&lt;mysql_password&gt; -h &lt;region1_db_host&gt; cloud account user domain &gt; region1.sql</span>
</pre></div>
</div>
</li>
<li><p class="first">Then run this command to put the data onto the region 2 database:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysql -u root -p&lt;mysql_password&gt; -h &lt;region2_db_host&gt; cloud &lt; region1.sql</span>
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Remove project accounts. Run these commands on the region 2 database:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysql&gt; delete from account where type = 5;</span>
</pre></div>
</div>
</li>
<li><p class="first">Set the default zone as null:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysql&gt; update account set default_zone_id = null;</span>
</pre></div>
</div>
</li>
<li><p class="first">Restart the Management Servers in region 2.</p>
</li>
</ol>
</div>
<div class="section" id="adding-third-and-subsequent-regions">
<h5>Adding Third and Subsequent Regions<a class="headerlink" href="#adding-third-and-subsequent-regions" title="Permalink to this headline">¶</a></h5>
<p>To add the third region, and subsequent additional regions, the steps
are similar to those for adding the second region. However, you must
repeat certain steps additional times for each additional region:</p>
<ol class="arabic">
<li><p class="first">Install CloudStack in each additional region. Set the region ID for
each region during the database setup step.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack-setup-databases cloud:&lt;dbpassword&gt;@localhost --deploy-as<span class="o">=</span>root:&lt;password&gt; -e &lt;encryption_type&gt; -m &lt;management_server_key&gt; -k &lt;database_key&gt; -r &lt;region_id&gt;
</pre></div>
</div>
</li>
<li><p class="first">Once the Management Server is running, add your new region to all
existing regions by repeatedly using the Add Region button in the UI.
For example, if you were adding region 3:</p>
<ol class="arabic simple">
<li>Log in to CloudStack in the first region as root administrator
(that is, log in to &lt;region.1.IP.address&gt;:8080/client), and add a
region with ID 3, the name of region 3, and the endpoint
&lt;region.3.IP.address&gt;:8080/client.</li>
<li>Log in to CloudStack in the second region as root administrator
(that is, log in to &lt;region.2.IP.address&gt;:8080/client), and add a
region with ID 3, the name of region 3, and the endpoint
&lt;region.3.IP.address&gt;:8080/client.</li>
</ol>
</li>
<li><p class="first">Repeat the procedure in reverse to add all existing regions to the
new region. For example, for the third region, add the other two
existing regions:</p>
<ol class="arabic simple">
<li>Log in to CloudStack in the third region as root administrator
(that is, log in to &lt;region.3.IP.address&gt;:8080/client).</li>
<li>Add a region with ID 1, the name of region 1, and the endpoint
&lt;region.1.IP.address&gt;:8080/client.</li>
<li>Add a region with ID 2, the name of region 2, and the endpoint
&lt;region.2.IP.address&gt;:8080/client.</li>
</ol>
</li>
<li><p class="first">Copy the account, user, and domain tables from any existing region&#8217;s
database to the new region&#8217;s database.</p>
<p>In the following commands, it is assumed that you have set the root
password on the database, which is a CloudStack recommended best
practice. Substitute your own MySQL root password.</p>
<ol class="arabic">
<li><p class="first">First, run this command to copy the contents of the database:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysqldump -u root -p&lt;mysql_password&gt; -h &lt;region1_db_host&gt; cloud account user domain &gt; region1.sql</span>
</pre></div>
</div>
</li>
<li><p class="first">Then run this command to put the data onto the new region&#8217;s
database. For example, for region 3:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysql -u root -p&lt;mysql_password&gt; -h &lt;region3_db_host&gt; cloud &lt; region1.sql</span>
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Remove project accounts. Run these commands on the region 3 database:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mysql&gt; delete from account where <span class="nb">type</span> <span class="o">=</span> 5<span class="p">;</span>
</pre></div>
</div>
</li>
<li><p class="first">Set the default zone as null:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mysql&gt; update account <span class="nb">set</span> <span class="nv">default_zone_id</span> <span class="o">=</span> null<span class="p">;</span>
</pre></div>
</div>
</li>
<li><p class="first">Restart the Management Servers in the new region.</p>
</li>
</ol>
</div>
<div class="section" id="deleting-a-region">
<h5>Deleting a Region<a class="headerlink" href="#deleting-a-region" title="Permalink to this headline">¶</a></h5>
<p>Log in to each of the other regions, navigate to the one you want to
delete, and click Remove Region. For example, to remove the third region
in a 3-region cloud:</p>
<ol class="arabic simple">
<li>Log in to &lt;region.1.IP.address&gt;:8080/client.</li>
<li>In the left navigation bar, click Regions.</li>
<li>Click the name of the region you want to delete.</li>
<li>Click the Remove Region button.</li>
<li>Repeat these steps for &lt;region.2.IP.address&gt;:8080/client.</li>
</ol>
</div>
</div>
<div class="section" id="adding-a-zone">
<span id="id2"></span><h4>Adding a Zone<a class="headerlink" href="#adding-a-zone" title="Permalink to this headline">¶</a></h4>
<p>When you add a new zone, you will be prompted to configure the zone’s
physical network and add the first pod, cluster, host, primary storage,
and secondary storage.</p>
<ol class="arabic simple">
<li>Log in to the CloudStack UI as the root administrator. See <a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-administration/en/latest/ui.html#log-in-to-the-ui">“Log In to the UI”</a>.</li>
<li>In the left navigation, choose Infrastructure.</li>
<li>On Zones, click View More.</li>
<li>Click Add Zone. The zone creation wizard will appear.</li>
<li>Choose one of the following network types:<ul>
<li><strong>Basic.</strong> For AWS-style networking. Provides a single network
where each VM instance is assigned an IP directly from the
network. Guest isolation can be provided through layer-3 means
such as security groups (IP address source filtering).</li>
<li><strong>Advanced.</strong> For more sophisticated network topologies. This
network model provides the most flexibility in defining guest
networks and providing custom network offerings such as firewall,
VPN, or load balancer support.</li>
</ul>
</li>
<li>The rest of the steps differ depending on whether you chose Basic or
Advanced. Continue with the steps that apply to you:<ul>
<li><a class="reference external" href="#basic-zone-configuration">“Basic Zone Configuration”</a></li>
<li><a class="reference external" href="#advanced-zone-configuration">“Advanced Zone Configuration”</a></li>
</ul>
</li>
</ol>
<div class="section" id="id3">
<h5>Basic Zone Configuration<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<ol class="arabic">
<li><p class="first">After you select Basic in the Add Zone wizard and click Next, you
will be asked to enter the following details. Then click Next.</p>
<ul>
<li><p class="first"><strong>Name.</strong> A name for the zone.</p>
</li>
<li><p class="first"><strong>DNS 1 and 2.</strong> These are DNS servers for use by guest VMs in the
zone. These DNS servers will be accessed via the public network
you will add later. The public IP addresses for the zone must have
a route to the DNS server named here.</p>
</li>
<li><p class="first"><strong>Internal DNS 1 and Internal DNS 2.</strong> These are DNS servers for
use by system VMs in the zone (these are VMs used by CloudStack
itself, such as virtual routers, console proxies, and Secondary
Storage VMs.) These DNS servers will be accessed via the
management traffic network interface of the System VMs. The
private IP address you provide for the pods must have a route to
the internal DNS server named here.</p>
</li>
<li><p class="first"><strong>Hypervisor.</strong> (Introduced in version 3.0.1) Choose the
hypervisor for the first cluster in the zone. You can add clusters
with different hypervisors later, after you finish adding the
zone.</p>
</li>
<li><p class="first"><strong>Network Offering.</strong> Your choice here determines what network
services will be available on the network for guest VMs.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Network Offering</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>DefaultSharedNetworkOfferingWithSGService</td>
<td>If you want to enable security groups for guest traffic isolation, choose this. (See Using Security Groups to                                                              Control Traffic to VMs.)</td>
</tr>
<tr class="row-odd"><td>DefaultSharedNetworkOffering</td>
<td>If you do not need security groups, choose this.</td>
</tr>
<tr class="row-even"><td>DefaultSharedNetscalerEIPandELBNetworkOffering</td>
<td>If you have installed a Citrix NetScaler appliance as part of your zone network, and you will be using its Elastic                                                         IP and Elastic Load Balancing features, choose this. With the EIP and ELB features, a basic zone with security                                                             groups enabled can offer 1:1 static NAT and load balancing.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><strong>Network Domain.</strong> (Optional) If you want to assign a special
domain name to the guest VM network, specify the DNS suffix.</p>
</li>
<li><p class="first"><strong>Public.</strong> A public zone is available to all users. A zone that
is not public will be assigned to a particular domain. Only users
in that domain will be allowed to create guest VMs in this zone.</p>
</li>
</ul>
</li>
<li><p class="first">Choose which traffic types will be carried by the physical network.</p>
<p>The traffic types are management, public, guest, and storage traffic.
For more information about the types, roll over the icons to display
their tool tips, or see Basic Zone Network Traffic Types. This screen
starts out with some traffic types already assigned. To add more,
drag and drop traffic types onto the network. You can also change the
network name if desired.</p>
</li>
<li><p class="first">Assign a network traffic label to each traffic type on the physical
network. These labels must match the labels you have already defined
on the hypervisor host. To assign each label, click the Edit button
under the traffic type icon. A popup dialog appears where you can
type the label, then click OK.</p>
<p>These traffic labels will be defined only for the hypervisor selected
for the first cluster. For all other hypervisors, the labels can be
configured after the zone is created.</p>
</li>
<li><p class="first">Click Next.</p>
</li>
<li><p class="first">(NetScaler only) If you chose the network offering for NetScaler, you
have an additional screen to fill out. Provide the requested details
to set up the NetScaler, then click Next.</p>
<ul class="simple">
<li><strong>IP address.</strong> The NSIP (NetScaler IP) address of the NetScaler
device.</li>
<li><strong>Username/Password.</strong> The authentication credentials to access
the device. CloudStack uses these credentials to access the
device.</li>
<li><strong>Type.</strong> NetScaler device type that is being added. It could be
NetScaler VPX, NetScaler MPX, or NetScaler SDX. For a comparison
of the types, see About Using a NetScaler Load Balancer.</li>
<li><strong>Public interface.</strong> Interface of NetScaler that is configured to
be part of the public network.</li>
<li><strong>Private interface.</strong> Interface of NetScaler that is configured
to be part of the private network.</li>
<li><strong>Number of retries.</strong> Number of times to attempt a command on the
device before considering the operation failed. Default is 2.</li>
<li><strong>Capacity.</strong> Number of guest networks/accounts that will share
this NetScaler device.</li>
<li><strong>Dedicated.</strong> When marked as dedicated, this device will be
dedicated to a single account. When Dedicated is checked, the
value in the Capacity field has no significance – implicitly, its
value is 1.</li>
</ul>
</li>
<li><p class="first">(NetScaler only) Configure the IP range for public traffic. The IPs
in this range will be used for the static NAT capability which you
enabled by selecting the network offering for NetScaler with EIP and
ELB. Enter the following details, then click Add. If desired, you can
repeat this step to add more IP ranges. When done, click Next.</p>
<ul class="simple">
<li><strong>Gateway.</strong> The gateway in use for these IP addresses.</li>
<li><strong>Netmask.</strong> The netmask associated with this IP range.</li>
<li><strong>VLAN.</strong> The VLAN that will be used for public traffic.</li>
<li><strong>Start IP/End IP.</strong> A range of IP addresses that are assumed to
be accessible from the Internet and will be allocated for access
to guest VMs.</li>
</ul>
</li>
<li><p class="first">In a new zone, CloudStack adds the first pod for you. You can always
add more pods later. For an overview of what a pod is, see
<a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html?highlight=about%20pods#about-pods">“About Pods”</a>.</p>
<p>To configure the first pod, enter the following, then click Next:</p>
<ul class="simple">
<li><strong>Pod Name.</strong> A name for the pod.</li>
<li><strong>Reserved system gateway.</strong> The gateway for the hosts in that
pod.</li>
<li><strong>Reserved system netmask.</strong> The network prefix that defines the
pod&#8217;s subnet. Use CIDR notation.</li>
<li><strong>Start/End Reserved System IP.</strong> The IP range in the management
network that CloudStack uses to manage various system VMs, such as
Secondary Storage VMs, Console Proxy VMs, and DHCP. For more
information, see System Reserved IP Addresses.</li>
</ul>
</li>
<li><p class="first">Configure the network for guest traffic. Provide the following, then
click Next:</p>
<ul class="simple">
<li><strong>Guest gateway.</strong> The gateway that the guests should use.</li>
<li><strong>Guest netmask.</strong> The netmask in use on the subnet the guests
will use.</li>
<li><strong>Guest start IP/End IP.</strong> Enter the first and last IP addresses
that define a range that CloudStack can assign to guests.<ul>
<li>We strongly recommend the use of multiple NICs. If multiple
NICs are used, they may be in a different subnet.</li>
<li>If one NIC is used, these IPs should be in the same CIDR as the
pod CIDR.</li>
</ul>
</li>
</ul>
</li>
<li><p class="first">In a new pod, CloudStack adds the first cluster for you. You can
always add more clusters later. For an overview of what a cluster is,
see About Clusters.</p>
<p>To configure the first cluster, enter the following, then click Next:</p>
<ul class="simple">
<li><strong>Hypervisor.</strong> (Version 3.0.0 only; in 3.0.1, this field is read
only) Choose the type of hypervisor software that all hosts in
this cluster will run. If you choose VMware, additional fields
appear so you can give information about a vSphere cluster. For
vSphere servers, we recommend creating the cluster of hosts in
vCenter and then adding the entire cluster to CloudStack. See Add
Cluster: vSphere.</li>
<li><strong>Cluster name.</strong> Enter a name for the cluster. This can be text
of your choosing and is not used by CloudStack.</li>
</ul>
</li>
<li><p class="first">In a new cluster, CloudStack adds the first host for you. You can
always add more hosts later. For an overview of what a host is, see
About Hosts.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When you add a hypervisor host to CloudStack, the host must not have
any VMs already running.</p>
</div>
<p>Before you can configure the host, you need to install the hypervisor
software on the host. You will need to know which version of the
hypervisor software version is supported by CloudStack and what
additional configuration is required to ensure the host will work
with CloudStack. To find these installation details, see:</p>
<ul class="simple">
<li>Citrix XenServer Installation and Configuration</li>
<li>VMware vSphere Installation and Configuration</li>
<li>KVM vSphere Installation and Configuration</li>
</ul>
<p>To configure the first host, enter the following, then click Next:</p>
<ul class="simple">
<li><strong>Host Name.</strong> The DNS name or IP address of the host.</li>
<li><strong>Username.</strong> The username is root.</li>
<li><strong>Password.</strong> This is the password for the user named above (from
your XenServer or KVM install).</li>
<li><strong>Host Tags.</strong> (Optional) Any labels that you use to categorize
hosts for ease of maintenance. For example, you can set this to
the cloud&#8217;s HA tag (set in the ha.tag global configuration
parameter) if you want this host to be used only for VMs with the
&#8220;high availability&#8221; feature enabled. For more information, see
HA-Enabled Virtual Machines as well as HA for Hosts.</li>
</ul>
</li>
<li><p class="first">In a new cluster, CloudStack adds the first primary storage server
for you. You can always add more servers later. For an overview of
what primary storage is, see About Primary Storage.</p>
<p>To configure the first primary storage server, enter the following,
then click Next:</p>
<ul class="simple">
<li><strong>Name.</strong> The name of the storage device.</li>
<li><strong>Protocol.</strong> For XenServer, choose either NFS, iSCSI, or
PreSetup. For KVM, choose NFS, SharedMountPoint,CLVM, or RBD. For
vSphere choose either VMFS (iSCSI or FiberChannel) or NFS. The
remaining fields in the screen vary depending on what you choose
here.</li>
</ul>
</li>
</ol>
</div>
<div class="section" id="id4">
<h5>Advanced Zone Configuration<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h5>
<ol class="arabic">
<li><p class="first">After you select Advanced in the Add Zone wizard and click Next, you
will be asked to enter the following details. Then click Next.</p>
<ul class="simple">
<li><strong>Name.</strong> A name for the zone.</li>
<li><strong>DNS 1 and 2.</strong> These are DNS servers for use by guest VMs in the
zone. These DNS servers will be accessed via the public network
you will add later. The public IP addresses for the zone must have
a route to the DNS server named here.</li>
<li><strong>Internal DNS 1 and Internal DNS 2.</strong> These are DNS servers for
use by system VMs in the zone(these are VMs used by CloudStack
itself, such as virtual routers, console proxies,and Secondary
Storage VMs.) These DNS servers will be accessed via the
management traffic network interface of the System VMs. The
private IP address you provide for the pods must have a route to
the internal DNS server named here.</li>
<li><strong>Network Domain.</strong> (Optional) If you want to assign a special
domain name to the guest VM network, specify the DNS suffix.</li>
<li><strong>Guest CIDR.</strong> This is the CIDR that describes the IP addresses
in use in the guest virtual networks in this zone. For example,
10.1.1.0/24. As a matter of good practice you should set different
CIDRs for different zones. This will make it easier to set up VPNs
between networks in different zones.</li>
<li><strong>Hypervisor.</strong> (Introduced in version 3.0.1) Choose the
hypervisor for the first cluster in the zone. You can add clusters
with different hypervisors later, after you finish adding the
zone.</li>
<li><strong>Public.</strong> A public zone is available to all users. A zone that
is not public will be assigned to a particular domain. Only users
in that domain will be allowed to create guest VMs in this zone.</li>
</ul>
</li>
<li><p class="first">Choose which traffic types will be carried by the physical network.</p>
<p>The traffic types are management, public, guest, and storage traffic.
For more information about the types, roll over the icons to display
their tool tips, or see <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html?highlight=about%20pods#advanced-zone-network-traffic-types">“Advanced Zone Network Traffic Types”</a>.
This screenstarts out with one network already configured. If you have
multiple physical networks, you need to add more. Drag and drop traffic
types onto a greyed-out network and it will become active. You can move the
traffic icons from one network to another; for example, if the
default traffic types shown for Network 1 do not match your actual
setup, you can move them down. You can also change the network names
if desired.</p>
</li>
<li><p class="first">(Introduced in version 3.0.1) Assign a network traffic label to each
traffic type on each physical network. These labels must match the
labels you have already defined on the hypervisor host. To assign
each label, click the Edit button under the traffic type icon within
each physical network. A popup dialog appears where you can type the
label, then click OK.</p>
<p>These traffic labels will be defined only for the hypervisor selected
for the first cluster. For all other hypervisors, the labels can be
configured after the zone is created.</p>
<p>(VMware only) If you have enabled Nexus dvSwitch in the environment,
you must specify the corresponding Ethernet port profile names as
network traffic label for each traffic type on the physical network.
For more information on Nexus dvSwitch, see Configuring a vSphere
Cluster with Nexus 1000v Virtual Switch in the Installation Guide. If
you have enabled VMware dvSwitch in the environment, you must specify
the corresponding Switch name as network traffic label for each
traffic type on the physical network. For more information, see
Configuring a VMware Datacenter with VMware Distributed Virtual
Switch in the Installation Guide.</p>
</li>
<li><p class="first">Click Next.</p>
</li>
<li><p class="first">Configure the IP range for public Internet traffic. Enter the
following details, then click Add. If desired, you can repeat this
step to add more public Internet IP ranges. When done, click Next.</p>
<ul class="simple">
<li><strong>Gateway.</strong> The gateway in use for these IP addresses.</li>
<li><strong>Netmask.</strong> The netmask associated with this IP range.</li>
<li><strong>VLAN.</strong> The VLAN that will be used for public traffic.</li>
<li><strong>Start IP/End IP.</strong> A range of IP addresses that are assumed to
be accessible from the Internet and will be allocated for access
to guest networks.</li>
</ul>
</li>
<li><p class="first">In a new zone, CloudStack adds the first pod for you. You can always
add more pods later. For an overview of what a pod is, see <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html?highlight=about%20pods#about-pods">“About Pods”</a>.</p>
<p>To configure the first pod, enter the following, then click Next:</p>
<ul class="simple">
<li><strong>Pod Name.</strong> A name for the pod.</li>
<li><strong>Reserved system gateway.</strong> The gateway for the hosts in that
pod.</li>
<li><strong>Reserved system netmask.</strong> The network prefix that defines the
pod&#8217;s subnet. Use CIDR notation.</li>
<li><strong>Start/End Reserved System IP.</strong> The IP range in the management
network that CloudStack uses to manage various system VMs, such as
Secondary Storage VMs, Console Proxy VMs, and DHCP. For more
information, see <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html?highlight=about%20pods#id4">“System Reserved IP Addresses”</a>.</li>
</ul>
</li>
<li><p class="first">Specify a range of VLAN IDs to carry guest traffic for each physical
network (see VLAN Allocation Example ), then click Next.</p>
</li>
<li><p class="first">In a new pod, CloudStack adds the first cluster for you. You can
always add more clusters later. For an overview of what a cluster is,
see <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html?highlight=about%20pods#about-clusters">“About Clusters”</a>.</p>
<p>To configure the first cluster, enter the following, then click Next:</p>
<ul class="simple">
<li><strong>Hypervisor.</strong> (Version 3.0.0 only; in 3.0.1, this field is read
only) Choose the type of hypervisor software that all hosts in
this cluster will run. If you choose VMware, additional fields
appear so you can give information about a vSphere cluster. For
vSphere servers, we recommend creating the cluster of hosts in
vCenter and then adding the entire cluster to CloudStack. See Add
Cluster: vSphere .</li>
<li><strong>Cluster name.</strong> Enter a name for the cluster. This can be text
of your choosing and is not used by CloudStack.</li>
</ul>
</li>
<li><p class="first">In a new cluster, CloudStack adds the first host for you. You can
always add more hosts later. For an overview of what a host is, see
<a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html?highlight=about%20pods#about-hosts">“About Hosts”</a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When you deploy CloudStack, the hypervisor host must not have any VMs
already running.</p>
</div>
<p>Before you can configure the host, you need to install the hypervisor
software on the host. You will need to know which version of the
hypervisor software version is supported by CloudStack and what
additional configuration is required to ensure the host will work
with CloudStack. To find these installation details, see:</p>
<ul class="simple">
<li>Citrix XenServer Installation for CloudStack</li>
<li>VMware vSphere Installation and Configuration</li>
<li>KVM Installation and Configuration</li>
</ul>
<p>To configure the first host, enter the following, then click Next:</p>
<ul class="simple">
<li><strong>Host Name.</strong> The DNS name or IP address of the host.</li>
<li><strong>Username.</strong> Usually root.</li>
<li><strong>Password.</strong> This is the password for the user named above (from
your XenServer or KVM install).</li>
<li><strong>Host Tags.</strong> (Optional) Any labels that you use to categorize
hosts for ease of maintenance. For example, you can set to the
cloud&#8217;s HA tag (set in the ha.tag global configuration parameter)
if you want this host to be used only for VMs with the &#8220;high
availability&#8221; feature enabled. For more information, see
HA-Enabled Virtual Machines as well as HA for Hosts, both in the
Administration Guide.</li>
</ul>
</li>
<li><p class="first">In a new cluster, CloudStack adds the first primary storage server
for you. You can always add more servers later. For an overview of
what primary storage is, see <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html?highlight=about%20pods#about-primary-storage">“About Primary Storage”</a>.</p>
<p>To configure the first primary storage server, enter the following,
then click Next:</p>
<ul>
<li><p class="first"><strong>Name.</strong> The name of the storage device.</p>
</li>
<li><p class="first"><strong>Protocol.</strong> For XenServer, choose either NFS, iSCSI, or
PreSetup. For KVM, choose NFS, SharedMountPoint, CLVM, and RBD.
For vSphere choose either VMFS (iSCSI or FiberChannel) or NFS. The
remaining fields in the screen vary depending on what you choose
here.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="20%" />
<col width="80%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>NFS</td>
<td><ul class="first last simple">
<li><strong>Server.</strong> The IP address or DNS name of the storage device.</li>
<li><strong>Path.</strong> The exported path from the server.</li>
<li><strong>Tags (optional).</strong> The comma-separated list of tags for this
storage device. It should be an equivalent set or superset of
the tags on your disk offerings.</li>
</ul>
</td>
</tr>
<tr class="row-even"><td>iSCSI</td>
<td><ul class="first last simple">
<li><strong>Server.</strong> The IP address or DNS name of the storage device.</li>
<li><strong>Target IQN.</strong> The IQN of the target. For example,
iqn.1986-03.com.sun:02:01ec9bb549-1271378984.</li>
<li><strong>Lun.</strong> The LUN number. For example, 3.</li>
<li><strong>Tags (optional).</strong> The comma-separated list of tags for this
storage device. It should be an equivalent set or superset of
the tags on your disk offerings.</li>
</ul>
</td>
</tr>
<tr class="row-odd"><td>preSetup</td>
<td><ul class="first last simple">
<li><strong>Server.</strong> The IP address or DNS name of the storage device.</li>
<li><strong>SR Name-Label.</strong> Enter the name-label of the SR that has been
set up outside CloudStack.</li>
<li><strong>Tags (optional).</strong> The comma-separated list of tags for this
storage device. It should be an equivalent set or superset of
the tags on your disk offerings.</li>
</ul>
</td>
</tr>
<tr class="row-even"><td>SharedMountPoint</td>
<td><ul class="first last simple">
<li><strong>Path.</strong> The path on each host that is where this primary
storage is mounted. For example, &#8220;/mnt/primary&#8221;.</li>
<li><strong>Tags (optional).</strong> The comma-separated list of tags for this
storage device. It should be an equivalent set or superset of
the tags on your disk offerings.</li>
</ul>
</td>
</tr>
<tr class="row-odd"><td>VMFS</td>
<td><ul class="first last simple">
<li><strong>Server.</strong> The IP address or DNS name of the vCenter server.</li>
<li><strong>Path.</strong> A combination of the datacenter name and the
datastore name. The format is &#8220;/&#8221; datacenter name &#8220;/&#8221; datastore
name. For example, &#8220;/cloud.dc.VM/cluster1datastore&#8221;.</li>
<li><strong>Tags (optional).</strong> The comma-separated list of tags for this
storage device. It should be an equivalent set or superset of
the tags on your disk offerings.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The tag sets on primary storage across clusters in a Zone must be
identical. For example, if cluster A provides primary storage that
has tags T1 and T2, all other clusters in the Zone must also
provide primary storage that has tags T1 and T2.</p>
</li>
</ul>
</li>
<li><p class="first">In a new zone, CloudStack adds the first secondary storage server for
you. For an overview of what secondary storage is, see <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/concepts.html?highlight=about%20pods#about-secondary-storage">“About Secondary Storage”</a>.</p>
<p>Before you can fill out this screen, you need to prepare the
secondary storage by setting up NFS shares and installing the latest
CloudStack System VM template. See Adding Secondary Storage :</p>
<ul class="simple">
<li><strong>NFS Server.</strong> The IP address of the server or fully qualified
domain name of the server.</li>
<li><strong>Path.</strong> The exported path from the server.</li>
</ul>
</li>
<li><p class="first">Click Launch.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="adding-a-pod">
<span id="id6"></span><h4>Adding a Pod<a class="headerlink" href="#adding-a-pod" title="Permalink to this headline">¶</a></h4>
<p>When you created a new zone, CloudStack adds the first pod for you. You
can add more pods at any time using the procedure in this section.</p>
<ol class="arabic simple">
<li>Log in to the CloudStack UI. See <a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-administration/en/latest/ui.html#log-in-to-the-ui">“Log In to the UI”</a>.</li>
<li>In the left navigation, choose Infrastructure. In Zones, click View
More, then click the zone to which you want to add a pod.</li>
<li>Click the Compute and Storage tab. In the Pods node of the diagram,
click View All.</li>
<li>Click Add Pod.</li>
<li>Enter the following details in the dialog.<ul>
<li><strong>Name.</strong> The name of the pod.</li>
<li><strong>Gateway.</strong> The gateway for the hosts in that pod.</li>
<li><strong>Netmask.</strong> The network prefix that defines the pod&#8217;s subnet. Use
CIDR notation.</li>
<li><strong>Start/End Reserved System IP.</strong> The IP range in the management
network that CloudStack uses to manage various system VMs, such as
Secondary Storage VMs, Console Proxy VMs, and DHCP. For more
information, see System Reserved IP Addresses.</li>
</ul>
</li>
<li>Click OK.</li>
</ol>
</div>
<div class="section" id="adding-a-cluster">
<span id="id8"></span><h4>Adding a Cluster<a class="headerlink" href="#adding-a-cluster" title="Permalink to this headline">¶</a></h4>
<p>You need to tell CloudStack about the hosts that it will manage. Hosts
exist inside clusters, so before you begin adding hosts to the cloud,
you must add at least one cluster.</p>
<div class="section" id="add-cluster-kvm-or-xenserver">
<h5>Add Cluster: KVM or XenServer<a class="headerlink" href="#add-cluster-kvm-or-xenserver" title="Permalink to this headline">¶</a></h5>
<p>These steps assume you have already installed the hypervisor on the
hosts and logged in to the CloudStack UI.</p>
<ol class="arabic simple">
<li>In the left navigation, choose Infrastructure. In Zones, click View
More, then click the zone in which you want to add the cluster.</li>
<li>Click the Compute tab.</li>
<li>In the Clusters node of the diagram, click View All.</li>
<li>Click Add Cluster.</li>
<li>Choose the hypervisor type for this cluster.</li>
<li>Choose the pod in which you want to create the cluster.</li>
<li>Enter a name for the cluster. This can be text of your choosing and
is not used by CloudStack.</li>
<li>Click OK.</li>
</ol>
</div>
<div class="section" id="add-cluster-vsphere">
<h5>Add Cluster: vSphere<a class="headerlink" href="#add-cluster-vsphere" title="Permalink to this headline">¶</a></h5>
<p>Host management for vSphere is done through a combination of vCenter and
the CloudStack admin UI. CloudStack requires that all hosts be in a
CloudStack cluster, but the cluster may consist of a single host. As an
administrator you must decide if you would like to use clusters of one
host or of multiple hosts. Clusters of multiple hosts allow for features
like live migration. Clusters also require shared storage such as NFS or
iSCSI.</p>
<p>For vSphere servers, we recommend creating the cluster of hosts in
vCenter and then adding the entire cluster to CloudStack. Follow these
requirements:</p>
<ul class="simple">
<li>Do not put more than 8 hosts in a vSphere cluster</li>
<li>Make sure the hypervisor hosts do not have any VMs already running
before you add them to CloudStack.</li>
</ul>
<p>To add a vSphere cluster to CloudStack:</p>
<ol class="arabic">
<li><p class="first">Create the cluster of hosts in vCenter. Follow the vCenter
instructions to do this. You will create a cluster that looks
something like this in vCenter.</p>
<p><img alt="vsphereclient.png: vSphere client" src="_images/vsphere-client.png" /></p>
</li>
<li><p class="first">Log in to the UI.</p>
</li>
<li><p class="first">In the left navigation, choose Infrastructure. In Zones, click View
More, then click the zone in which you want to add the cluster.</p>
</li>
<li><p class="first">Click the Compute tab, and click View All on Pods. Choose the pod to
which you want to add the cluster.</p>
</li>
<li><p class="first">Click View Clusters.</p>
</li>
<li><p class="first">Click Add Cluster.</p>
</li>
<li><p class="first">In Hypervisor, choose VMware.</p>
</li>
<li><p class="first">Provide the following information in the dialog. The fields below
make reference to the values from vCenter.</p>
<p><img alt="addcluster.png: add a cluster" src="_images/add-cluster.png" /></p>
<ul>
<li><p class="first"><strong>Cluster Name</strong>: Enter the name of the cluster you created in
vCenter. For example, &#8220;cloud.cluster.2.2.1&#8221;</p>
</li>
<li><p class="first"><strong>vCenter Username</strong>: Enter the username that CloudStack should
use to connect to vCenter. This user must have all the
administrative privileges.</p>
</li>
<li><p class="first"><strong>CPU overcommit ratio</strong>: Enter the CPU overcommit ratio for the
cluster. The value you enter determines the CPU consumption of
each VM in the selected cluster. By increasing the
over-provisioning ratio, more resource capacity will be used. If
no value is specified, the value is defaulted to 1, which implies
no over-provisioning is done.</p>
</li>
<li><p class="first"><strong>RAM overcommit ratio</strong>: Enter the RAM overcommit ratio for the
cluster. The value you enter determines the memory consumption of
each VM in the selected cluster. By increasing the
over-provisioning ratio, more resource capacity will be used. If
no value is specified, the value is defaulted to 1, which implies
no over-provisioning is done.</p>
</li>
<li><p class="first"><strong>vCenter Host</strong>: Enter the hostname or IP address of the vCenter
server.</p>
</li>
<li><p class="first"><strong>vCenter Password</strong>: Enter the password for the user named above.</p>
</li>
<li><p class="first"><strong>vCenter Datacenter</strong>: Enter the vCenter datacenter that the
cluster is in. For example, &#8220;cloud.dc.VM&#8221;.</p>
</li>
<li><p class="first"><strong>Override Public Traffic</strong>: Enable this option to override the
zone-wide public traffic for the cluster you are creating.</p>
</li>
<li><p class="first"><strong>Public Traffic vSwitch Type</strong>: This option is displayed only if
you enable the Override Public Traffic option. Select a desirable
switch. If the vmware.use.dvswitch global parameter is true, the
default option will be VMware vNetwork Distributed Virtual Switch.</p>
<p>If you have enabled Nexus dvSwitch in the environment, the
following parameters for dvSwitch configuration are displayed:</p>
<ul class="simple">
<li>Nexus dvSwitch IP Address: The IP address of the Nexus VSM
appliance.</li>
<li>Nexus dvSwitch Username: The username required to access the
Nexus VSM appliance.</li>
<li>Nexus dvSwitch Password: The password associated with the
username specified above.</li>
</ul>
</li>
<li><p class="first"><strong>Override Guest Traffic</strong>: Enable this option to override the
zone-wide guest traffic for the cluster you are creating.</p>
</li>
<li><p class="first"><strong>Guest Traffic vSwitch Type</strong>: This option is displayed only if
you enable the Override Guest Traffic option. Select a desirable
switch.</p>
<p>If the vmware.use.dvswitch global parameter is true, the default
option will be VMware vNetwork Distributed Virtual Switch.</p>
<p>If you have enabled Nexus dvSwitch in the environment, the
following parameters for dvSwitch configuration are displayed:</p>
<ul class="simple">
<li>Nexus dvSwitch IP Address: The IP address of the Nexus VSM
appliance.</li>
<li>Nexus dvSwitch Username: The username required to access the
Nexus VSM appliance.</li>
<li>Nexus dvSwitch Password: The password associated with the
username specified above.</li>
</ul>
</li>
<li><p class="first">There might be a slight delay while the cluster is provisioned. It
will automatically display in the UI.</p>
</li>
</ul>
</li>
</ol>
</div>
</div>
<div class="section" id="adding-a-host">
<span id="id9"></span><h4>Adding a Host<a class="headerlink" href="#adding-a-host" title="Permalink to this headline">¶</a></h4>
<ol class="arabic">
<li><p class="first">Before adding a host to the CloudStack configuration, you must first
install your chosen hypervisor on the host. CloudStack can manage
hosts running VMs under a variety of hypervisors.</p>
<p>The CloudStack Installation Guide provides instructions on how to
install each supported hypervisor and configure it for use with
CloudStack. See the appropriate section in the Installation Guide for
information about which version of your chosen hypervisor is
supported, as well as crucial additional steps to configure the
hypervisor hosts for use with CloudStack.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Be sure you have performed the additional CloudStack-specific
configuration steps described in the hypervisor installation section for
your particular hypervisor.</p>
</div>
</li>
<li><p class="first">Now add the hypervisor host to CloudStack. The technique to use
varies depending on the hypervisor.</p>
<ul class="simple">
<li><a class="reference internal" href="#adding-a-host-xenserver-kvm"><span>Adding a Host (XenServer or KVM)</span></a></li>
<li><a class="reference internal" href="#adding-a-host-vsphere"><span>Adding a Host (vSphere)</span></a></li>
</ul>
</li>
</ol>
<div class="section" id="adding-a-host-xenserver-or-kvm">
<span id="adding-a-host-xenserver-kvm"></span><h5>Adding a Host (XenServer or KVM)<a class="headerlink" href="#adding-a-host-xenserver-or-kvm" title="Permalink to this headline">¶</a></h5>
<p>XenServer and KVM hosts can be added to a cluster at any time.</p>
<div class="section" id="requirements-for-xenserver-and-kvm-hosts">
<h6>Requirements for XenServer and KVM Hosts<a class="headerlink" href="#requirements-for-xenserver-and-kvm-hosts" title="Permalink to this headline">¶</a></h6>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Make sure the hypervisor host does not have any VMs already running before
you add it to CloudStack.</p>
</div>
<p>Configuration requirements:</p>
<ul class="simple">
<li>Each cluster must contain only hosts with the identical hypervisor.</li>
<li>For XenServer, do not put more than 8 hosts in a cluster.</li>
<li>For KVM, do not put more than 16 hosts in a cluster.</li>
</ul>
<p>For hardware requirements, see the installation section for your
hypervisor in the CloudStack Installation Guide.</p>
<div class="section" id="xenserver-host-additional-requirements">
<h7>XenServer Host Additional Requirements<a class="headerlink" href="#xenserver-host-additional-requirements" title="Permalink to this headline">¶</a></h7>
<p>If network bonding is in use, the administrator must cable the new host
identically to other hosts in the cluster.</p>
<p>For all additional hosts to be added to the cluster, run the following
command. This will cause the host to join the master in a XenServer
pool.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe pool-join master-address=[master IP] master-username=root master-password=[your password]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When copying and pasting a command, be sure the command has pasted as a
single line before executing. Some document viewers may introduce unwanted
line breaks in copied text.</p>
</div>
<p>With all hosts added to the XenServer pool, run the cloud-setup-bond
script. This script will complete the configuration and setup of the
bonds on the new hosts in the cluster.</p>
<ol class="arabic">
<li><p class="first">Copy the script from the Management Server in
/usr/share/cloudstack-common/scripts/vm/hypervisor/xenserver/cloud-setup-bonding.sh
to the master host and ensure it is executable.</p>
</li>
<li><p class="first">Run the script:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># ./cloud-setup-bonding.sh</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="kvm-host-additional-requirements">
<h7>KVM Host Additional Requirements<a class="headerlink" href="#kvm-host-additional-requirements" title="Permalink to this headline">¶</a></h7>
<ul>
<li><p class="first">If shared mountpoint storage is in use, the administrator should
ensure that the new host has all the same mountpoints (with storage
mounted) as the other hosts in the cluster.</p>
</li>
<li><p class="first">Make sure the new host has the same network configuration (guest,
private, and public network) as other hosts in the cluster.</p>
</li>
<li><p class="first">If you are using OpenVswitch bridges edit the file agent.properties
on the KVM host and set the parameter network.bridge.type to
openvswitch before adding the host to CloudStack</p>
</li>
<li><p class="first">If you&#8217;re using a non-root user to add a KVM host, please add the user to
sudoers file:</p>
<blockquote>
<div><p>cloudstack ALL=NOPASSWD: /usr/bin/cloudstack-setup-agent
defaults:cloudstack !requiretty</p>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="adding-a-xenserver-or-kvm-host">
<h6>Adding a XenServer or KVM Host<a class="headerlink" href="#adding-a-xenserver-or-kvm-host" title="Permalink to this headline">¶</a></h6>
<ol class="arabic">
<li><p class="first">If you have not already done so, install the hypervisor software on
the host. You will need to know which version of the hypervisor
software version is supported by CloudStack and what additional
configuration is required to ensure the host will work with
CloudStack. To find these installation details, see the appropriate
section for your hypervisor in the CloudStack Installation Guide.</p>
</li>
<li><p class="first">Log in to the CloudStack UI as administrator.</p>
</li>
<li><p class="first">In the left navigation, choose Infrastructure. In Zones, click View
More, then click the zone in which you want to add the host.</p>
</li>
<li><p class="first">Click the Compute tab. In the Clusters node, click View All.</p>
</li>
<li><p class="first">Click the cluster where you want to add the host.</p>
</li>
<li><p class="first">Click View Hosts.</p>
</li>
<li><p class="first">Click Add Host.</p>
</li>
<li><p class="first">Provide the following information.</p>
<ul class="simple">
<li>Host Name. The DNS name or IP address of the host.</li>
<li>Username. Usually root.</li>
<li>Password. This is the password for the user from your XenServer or
KVM install).</li>
<li>Host Tags (Optional). Any labels that you use to categorize hosts
for ease of maintenance. For example, you can set to the cloud&#8217;s
HA tag (set in the ha.tag global configuration parameter) if you
want this host to be used only for VMs with the &#8220;high
availability&#8221; feature enabled. For more information, see
HA-Enabled Virtual Machines as well as HA for Hosts.</li>
</ul>
<p>There may be a slight delay while the host is provisioned. It should
automatically display in the UI.</p>
</li>
<li><p class="first">Repeat for additional hosts.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="adding-a-host-vsphere">
<span id="id10"></span><h5>Adding a Host (vSphere)<a class="headerlink" href="#adding-a-host-vsphere" title="Permalink to this headline">¶</a></h5>
<p>For vSphere servers, we recommend creating the cluster of hosts in
vCenter and then adding the entire cluster to CloudStack. See Add
Cluster: vSphere.</p>
</div>
</div>
<div class="section" id="add-primary-storage">
<span id="id11"></span><h4>Add Primary Storage<a class="headerlink" href="#add-primary-storage" title="Permalink to this headline">¶</a></h4>
<div class="section" id="system-requirements-for-primary-storage">
<h5>System Requirements for Primary Storage<a class="headerlink" href="#system-requirements-for-primary-storage" title="Permalink to this headline">¶</a></h5>
<p>Hardware requirements:</p>
<ul class="simple">
<li>Any standards-compliant iSCSI, SMB, or NFS server that is supported
by the underlying hypervisor.</li>
<li>The storage server should be a machine with a large number of disks.
The disks should ideally be managed by a hardware RAID controller.</li>
<li>Minimum required capacity depends on your needs.</li>
</ul>
<p>When setting up primary storage, follow these restrictions:</p>
<ul class="simple">
<li>Primary storage cannot be added until a host has been added to the
cluster.</li>
<li>If you do not provision shared primary storage, you must set the
global configuration parameter system.vm.local.storage.required to
true, or else you will not be able to start VMs.</li>
</ul>
</div>
<div class="section" id="adding-primary-storage">
<h5>Adding Primary Storage<a class="headerlink" href="#adding-primary-storage" title="Permalink to this headline">¶</a></h5>
<p>When you create a new zone, the first primary storage is added as part
of that procedure. You can add primary storage servers at any time, such
as when adding a new cluster or adding more servers to an existing
cluster.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">When using preallocated storage for primary storage, be sure there is
nothing on the storage (ex. you have an empty SAN volume or an empty NFS
share). Adding the storage to CloudStack will destroy any existing data.</p>
</div>
<ol class="arabic">
<li><p class="first">Log in to the CloudStack UI (see <a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-administration/en/latest/ui.html#log-in-to-the-ui">“Log In to the UI”</a>).</p>
</li>
<li><p class="first">In the left navigation, choose Infrastructure. In Zones, click View
More, then click the zone in which you want to add the primary
storage.</p>
</li>
<li><p class="first">Click the Compute tab.</p>
</li>
<li><p class="first">In the Primary Storage node of the diagram, click View All.</p>
</li>
<li><p class="first">Click Add Primary Storage.</p>
</li>
<li><p class="first">Provide the following information in the dialog. The information
required varies depending on your choice in Protocol.</p>
<ul class="simple">
<li><strong>Scope.</strong> Indicate whether the storage is available to all hosts
in the zone or only to hosts in a single cluster.</li>
<li><strong>Pod.</strong> (Visible only if you choose Cluster in the Scope field.)
The pod for the storage device.</li>
<li><strong>Cluster.</strong> (Visible only if you choose Cluster in the Scope
field.) The cluster for the storage device.</li>
<li><strong>Name.</strong> The name of the storage device.</li>
<li><strong>Protocol.</strong> For XenServer, choose either NFS, iSCSI, or
PreSetup. For KVM, choose NFS or SharedMountPoint. For vSphere
choose either VMFS (iSCSI or FiberChannel) or NFS. For Hyper-V,
choose SMB.</li>
<li><strong>Server (for NFS, iSCSI, or PreSetup).</strong> The IP address or DNS
name of the storage device.</li>
<li><strong>Server (for VMFS).</strong> The IP address or DNS name of the vCenter
server.</li>
<li><strong>Path (for NFS).</strong> In NFS this is the exported path from the
server.</li>
<li><strong>Path (for VMFS).</strong> In vSphere this is a combination of the
datacenter name and the datastore name. The format is &#8220;/&#8221;
datacenter name &#8220;/&#8221; datastore name. For example,
&#8220;/cloud.dc.VM/cluster1datastore&#8221;.</li>
<li><strong>Path (for SharedMountPoint).</strong> With KVM this is the path on each
host that is where this primary storage is mounted. For example,
&#8220;/mnt/primary&#8221;.</li>
<li><strong>SMB Username</strong> (for SMB/CIFS): Applicable only if you select
SMB/CIFS provider. The username of the account which has the
necessary permissions to the SMB shares. The user must be part of
the Hyper-V administrator group.</li>
<li><strong>SMB Password</strong> (for SMB/CIFS): Applicable only if you select
SMB/CIFS provider. The password associated with the account.</li>
<li><strong>SMB Domain</strong>(for SMB/CIFS): Applicable only if you select
SMB/CIFS provider. The Active Directory domain that the SMB share
is a part of.</li>
<li><strong>SR Name-Label (for PreSetup).</strong> Enter the name-label of the SR
that has been set up outside CloudStack.</li>
<li><strong>Target IQN (for iSCSI).</strong> In iSCSI this is the IQN of the
target. For example, iqn.1986-03.com.sun:02:01ec9bb549-1271378984.</li>
<li><strong>Lun # (for iSCSI).</strong> In iSCSI this is the LUN number. For
example, 3.</li>
<li><strong>Tags (optional).</strong> The comma-separated list of tags for this
storage device. It should be an equivalent set or superset of the
tags on your disk offerings..</li>
</ul>
<p>The tag sets on primary storage across clusters in a Zone must be
identical. For example, if cluster A provides primary storage that
has tags T1 and T2, all other clusters in the Zone must also provide
primary storage that has tags T1 and T2.</p>
</li>
<li><p class="first">Click OK.</p>
</li>
</ol>
</div>
<div class="section" id="configuring-a-storage-plug-in">
<h5>Configuring a Storage Plug-in<a class="headerlink" href="#configuring-a-storage-plug-in" title="Permalink to this headline">¶</a></h5>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Primary storage that is based on a custom plug-in (ex. SolidFire) must be
added through the CloudStack API (described later in this section). There
is no support at this time through the CloudStack UI to add this type of
primary storage (although most of its features are available through the
CloudStack UI).</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The SolidFire storage plug-in for CloudStack is part of the standard
CloudStack install. There is no additional work required to add this
component.</p>
</div>
<p>Adding primary storage that is based on the SolidFire plug-in enables
CloudStack to provide hard quality-of-service (QoS) guarantees.</p>
<p>When used with Compute or Disk Offerings, an administrator is able to
build an environment in which a root or data disk that a user creates
leads to the dynamic creation of a SolidFire volume, which has guaranteed
performance. Such a SolidFire volume is associated with one (and only
ever one) CloudStack volume, so performance of the CloudStack volume
does not vary depending on how heavily other tenants are using the
system.</p>
<p>The createStoragePool API has been augmented to support plugable storage
providers. The following is a list of parameters to use when adding
storage to CloudStack that is based on the SolidFire plug-in:</p>
<ul class="simple">
<li>command=createStoragePool</li>
<li>scope=zone</li>
<li>zoneId=[your zone id]</li>
<li>name=[name for primary storage]</li>
<li>hypervisor=Any</li>
<li>provider=SolidFire</li>
<li>capacityIops=[whole number of IOPS from the SAN to give to
CloudStack]</li>
<li>capacityBytes=[whole number of bytes from the SAN to give to
CloudStack]</li>
</ul>
<p>The url parameter is somewhat unique in that its value can contain
additional key/value pairs.</p>
<p>url=[key/value pairs detailed below (values are URL encoded; for
example, &#8216;=&#8217; is represented as &#8216;%3D&#8217;)]</p>
<ul class="simple">
<li>MVIP%3D[Management Virtual IP Address] (can be suffixed with :[port
number])</li>
<li>SVIP%3D[Storage Virtual IP Address] (can be suffixed with :[port
number])</li>
<li>clusterAdminUsername%3D[cluster admin&#8217;s username]</li>
<li>clusterAdminPassword%3D[cluster admin&#8217;s password]</li>
<li>clusterDefaultMinIops%3D[Min IOPS (whole number) to set for a volume;
used if Min IOPS is not specified by administrator or user]</li>
<li>clusterDefaultMaxIops%3D[Max IOPS (whole number) to set for a volume;
used if Max IOPS is not specified by administrator or user]</li>
<li>clusterDefaultBurstIopsPercentOfMaxIops%3D[Burst IOPS is determined
by (Min IOPS * clusterDefaultBurstIopsPercentOfMaxIops parameter)
(can be a decimal value)]</li>
</ul>
</div>
</div>
<div class="section" id="add-secondary-storage">
<span id="id13"></span><h4>Add Secondary Storage<a class="headerlink" href="#add-secondary-storage" title="Permalink to this headline">¶</a></h4>
<div class="section" id="system-requirements-for-secondary-storage">
<h5>System Requirements for Secondary Storage<a class="headerlink" href="#system-requirements-for-secondary-storage" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>NFS storage appliance or Linux NFS server</li>
<li>SMB/CIFS (Hyper-V)</li>
<li>(Optional) OpenStack Object Storage (Swift) (see
<a class="reference external" href="http://swift.openstack.org">http://swift.openstack.org</a>)</li>
<li>100GB minimum capacity</li>
<li>A secondary storage device must be located in the same zone as the
guest VMs it serves.</li>
<li>Each Secondary Storage server must be available to all hosts in the
zone.</li>
</ul>
</div>
<div class="section" id="adding-secondary-storage">
<h5>Adding Secondary Storage<a class="headerlink" href="#adding-secondary-storage" title="Permalink to this headline">¶</a></h5>
<p>When you create a new zone, the first secondary storage is added as part
of that procedure. You can add secondary storage servers at any time to
add more servers to an existing zone.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Ensure that nothing is stored on the server. Adding the server to
CloudStack will destroy any existing data.</p>
</div>
<ol class="arabic">
<li><p class="first">To prepare for the zone-based Secondary Staging Store, you should
have created and mounted an NFS share during Management Server
installation. See <a class="reference internal" href="index.html#prepare-nfs-shares"><span>Prepare NFS Shares</span></a>.</p>
<p>If you are using an Hyper-V host, ensure that you have created a SMB
share.</p>
</li>
<li><p class="first">Make sure you prepared the system VM template during Management
Server installation. See <a class="reference external" href="installation.html#prepare-the-system-vm-template">“Prepare the System VM Template”</a>.</p>
</li>
<li><p class="first">Log in to the CloudStack UI as root administrator.</p>
</li>
<li><p class="first">In the left navigation bar, click Infrastructure.</p>
</li>
<li><p class="first">In Secondary Storage, click View All.</p>
</li>
<li><p class="first">Click Add Secondary Storage.</p>
</li>
<li><p class="first">Fill in the following fields:</p>
<ul>
<li><p class="first">Name. Give the storage a descriptive name.</p>
</li>
<li><p class="first">Provider. Choose S3, Swift, NFS, or CIFS then fill in the related
fields which appear. The fields will vary depending on the storage
provider; for more information, consult the provider&#8217;s
documentation (such as the S3 or Swift website). NFS can be used
for zone-based storage, and the others for region-wide storage.
For Hyper-V, select SMB/CIFS.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Heterogeneous Secondary Storage is not supported in Regions. You can
use only a single NFS, S3, or Swift account per region.</p>
</div>
</li>
<li><p class="first">Create NFS Secondary Staging Store. This box must always be
checked.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Even if the UI allows you to uncheck this box, do not do so. This
checkbox and the three fields below it must be filled in. Even when
Swift or S3 is used as the secondary storage provider, an NFS staging
storage in each zone is still required.</p>
</div>
</li>
<li><p class="first">Zone. The zone where the NFS Secondary Staging Store is to be
located.</p>
</li>
<li><p class="first"><strong>SMB Username</strong>: Applicable only if you select SMB/CIFS provider.
The username of the account which has the necessary permissions to
the SMB shares. The user must be part of the Hyper-V administrator
group.</p>
</li>
<li><p class="first"><strong>SMB Password</strong>: Applicable only if you select SMB/CIFS provider.
The password associated with the account.</p>
</li>
<li><p class="first"><strong>SMB Domain</strong>: Applicable only if you select SMB/CIFS provider.
The Active Directory domain that the SMB share is a part of.</p>
</li>
<li><p class="first">NFS server. The name of the zone&#8217;s Secondary Staging Store.</p>
</li>
<li><p class="first">Path. The path to the zone&#8217;s Secondary Staging Store.</p>
</li>
</ul>
</li>
</ol>
</div>
<div class="section" id="adding-an-nfs-secondary-staging-store-for-each-zone">
<h5>Adding an NFS Secondary Staging Store for Each Zone<a class="headerlink" href="#adding-an-nfs-secondary-staging-store-for-each-zone" title="Permalink to this headline">¶</a></h5>
<p>Every zone must have at least one NFS store provisioned; multiple NFS
servers are allowed per zone. To provision an NFS Staging Store for a
zone:</p>
<ol class="arabic simple">
<li>Log in to the CloudStack UI as root administrator.</li>
<li>In the left navigation bar, click Infrastructure.</li>
<li>In Secondary Storage, click View All.</li>
<li>In Select View, choose Secondary Staging Store.</li>
<li>Click the Add NFS Secondary Staging Store button.</li>
<li>Fill out the dialog box fields, then click OK:<ul>
<li>Zone. The zone where the NFS Secondary Staging Store is to be
located.</li>
<li>NFS server. The name of the zone&#8217;s Secondary Staging Store.</li>
<li>Path. The path to the zone&#8217;s Secondary Staging Store.</li>
</ul>
</li>
</ol>
</div>
</div>
<div class="section" id="initialize-and-test">
<span id="id14"></span><h4>Initialize and Test<a class="headerlink" href="#initialize-and-test" title="Permalink to this headline">¶</a></h4>
<p>After everything is configured, CloudStack will perform its
initialization. This can take 30 minutes or more, depending on the speed
of your network. When the initialization has completed successfully, the
administrator&#8217;s Dashboard should be displayed in the CloudStack UI.</p>
<ol class="arabic">
<li><p class="first">Verify that the system is ready. In the left navigation bar, select
Templates. Click on the CentOS 5.5 (64bit) no Gui (KVM) template.
Check to be sure that the status is &#8220;Download Complete.&#8221; Do not
proceed to the next step until this status is displayed.</p>
</li>
<li><p class="first">Go to the Instances tab, and filter by My Instances.</p>
</li>
<li><p class="first">Click Add Instance and follow the steps in the wizard.</p>
<ol class="arabic simple">
<li>Choose the zone you just added.</li>
<li>In the template selection, choose the template to use in the VM.
If this is a fresh installation, likely only the provided CentOS
template is available.</li>
<li>Select a service offering. Be sure that the hardware you have
allows starting the selected service offering.</li>
<li>In data disk offering, if desired, add another data disk. This is
a second volume that will be available to but not mounted in the
guest. For example, in Linux on XenServer you will see /dev/xvdb
in the guest after rebooting the VM. A reboot is not required if
you have a PV-enabled OS kernel in use.</li>
<li>In default network, choose the primary network for the guest. In a
trial installation, you would have only one option here.</li>
<li>Optionally give your VM a name and a group. Use any descriptive
text you would like.</li>
<li>Click Launch VM. Your VM will be created and started. It might
take some time to download the template and complete the VM
startup. You can watch the VMâ€™s progress in the Instances
screen.</li>
</ol>
</li>
<li><p class="first">To use the VM, click the View Console button. <img alt="ConsoleButton.png: button to launch a console" src="_images/console-icon.png" /></p>
<p>For more information about using VMs, including instructions for how
to allow incoming network traffic to the VM, start, stop, and delete
VMs, and move a VM from one host to another, see Working With Virtual
Machines in the Administratorâ€™s Guide.</p>
</li>
</ol>
<p>Congratulations! You have successfully completed a CloudStack
Installation.</p>
<p>If you decide to grow your deployment, you can add more hosts, primary
storage, zones, pods, and clusters.</p>
</div>
<div class="section" id="configuration-parameters">
<h4>Configuration Parameters<a class="headerlink" href="#configuration-parameters" title="Permalink to this headline">¶</a></h4>
<div class="section" id="about-configuration-parameters">
<h5>About Configuration Parameters<a class="headerlink" href="#about-configuration-parameters" title="Permalink to this headline">¶</a></h5>
<p>CloudStack provides a variety of settings you can use to set limits,
configure features, and enable or disable features in the cloud. Once
your Management Server is running, you might need to set some of these
configuration parameters, depending on what optional features you are
setting up. You can set default values at the global level, which will
be in effect throughout the cloud unless you override them at a lower
level. You can make local settings, which will override the global
configuration parameter values, at the level of an account, zone,
cluster, or primary storage.</p>
<p>The documentation for each CloudStack feature should direct you to the
names of the applicable parameters. The following table shows a few of
the more useful parameters.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="4%" />
<col width="96%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>management.network.cidr</td>
<td>A CIDR that describes the network that the management CIDRs reside on. This                                        variable must be set for deployments that use vSphere. It is recommended to be                                     set for other deployments as well. Example: 192.168.3.0/24.</td>
</tr>
<tr class="row-odd"><td>xen.setup.multipath</td>
<td>For XenServer nodes, this is a true/false variable that instructs CloudStack to                                    enable iSCSI multipath on the XenServer Hosts when they are added. This                                            defaults to false. Set it to true if you would like CloudStack to enable                                           multipath.If this is true for a NFS-based deployment multipath will still be                                       enabled on the XenServer host. However, this does not impact NFS operation and                                     is harmless.</td>
</tr>
<tr class="row-even"><td>secstorage.allowed.internal.sites</td>
<td>This is used to protect your internal network from rogue attempts to download                                      arbitrary files using the template download feature. This is a comma-separated                                     list of CIDRs. If a requested URL matches any of these CIDRs the Secondary                                         Storage VM will use the private network interface to fetch the URL. Other URLs                                     will go through the public interface. We suggest you set this to 1 or 2                                            hardened internal machines where you keep your templates. For example, set it                                      to 192.168.1.66/32.</td>
</tr>
<tr class="row-odd"><td>use.local.storage</td>
<td>Determines whether CloudStack will use storage that is local to the Host for                                       data disks, templates, and snapshots. By default CloudStack will not use this                                      storage. You should change this to true if you want to use local storage and                                       you understand the reliability and feature drawbacks to choosing local storage.</td>
</tr>
<tr class="row-even"><td>host</td>
<td>This is the IP address of the Management Server. If you are using multiple                                         Management Servers you should enter a load balanced IP address that is                                             reachable via the private network.</td>
</tr>
<tr class="row-odd"><td>default.page.size</td>
<td>Maximum number of items per page that can be returned by a CloudStack API                                          command. The limit applies at the cloud level and can vary from cloud to cloud.                                    You can override this with a lower value on a particular API call by using the                                     page and pagesize API command parameters. For more information, see the                                            Developer&#8217;s Guide. Default: 500.</td>
</tr>
<tr class="row-even"><td>ha.tag</td>
<td>The label you want to use throughout the cloud to designate certain hosts as                                       dedicated HA hosts. These hosts will be used only for HA-enabled VMs that are                                      restarting due to the failure of another host. For example, you could set this                                     to ha_host. Specify the ha.tag value asa host tag when you add a new host to                                      the cloud.</td>
</tr>
<tr class="row-odd"><td>vmware.vcenter.session.timeout</td>
<td>Determines the vCenter session timeout value by using this parameter. The                                          default value is 20 minutes. Increase the timeout value to avoid timeout errors                                    in VMware deployments because certain VMware operations take more than 20                                          minutes.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="setting-global-configuration-parameters">
<h5>Setting Global Configuration Parameters<a class="headerlink" href="#setting-global-configuration-parameters" title="Permalink to this headline">¶</a></h5>
<p>Use the following steps to set global configuration parameters. These
values will be the defaults in effect throughout your CloudStack
deployment.</p>
<ol class="arabic simple">
<li>Log in to the UI as administrator.</li>
<li>In the left navigation bar, click Global Settings.</li>
<li>In Select View, choose one of the following:<ul>
<li>Global Settings. This displays a list of the parameters with brief
descriptions and current values.</li>
<li>Hypervisor Capabilities. This displays a list of hypervisor
versions with the maximum number of guests supported for each.</li>
</ul>
</li>
<li>Use the search box to narrow down the list to those you are
interested in.</li>
<li>In the Actions column, click the Edit icon to modify a value. If you
are viewing Hypervisor Capabilities, you must click the name of the
hypervisor first to display the editing screen.</li>
</ol>
</div>
<div class="section" id="setting-local-configuration-parameters">
<h5>Setting Local Configuration Parameters<a class="headerlink" href="#setting-local-configuration-parameters" title="Permalink to this headline">¶</a></h5>
<p>Use the following steps to set local configuration parameters for an
account, zone, cluster, or primary storage. These values will override
the global configuration settings.</p>
<ol class="arabic simple">
<li>Log in to the UI as administrator.</li>
<li>In the left navigation bar, click Infrastructure or Accounts,
depending on where you want to set a value.</li>
<li>Find the name of the particular resource that you want to work with.
For example, if you are in Infrastructure, click View All on the
Zones, Clusters, or Primary Storage area.</li>
<li>Click the name of the resource where you want to set a limit.</li>
<li>Click the Settings tab.</li>
<li>Use the search box to narrow down the list to those you are
interested in.</li>
<li>In the Actions column, click the Edit icon to modify a value.</li>
</ol>
</div>
</div>
<div class="section" id="granular-global-configuration-parameters">
<h4>Granular Global Configuration Parameters<a class="headerlink" href="#granular-global-configuration-parameters" title="Permalink to this headline">¶</a></h4>
<p>The following global configuration parameters have been made more
granular. The parameters are listed under three different scopes:
account, cluster, and zone.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="2%" />
<col width="15%" />
<col width="82%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field</th>
<th class="head">Field</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>account</td>
<td>remote.access.vpn.client.iprange</td>
<td>The range of IPs to be allocated to remotely access the VPN clients. The first IP in the range is                                                                                                          used by the VPN server.</td>
</tr>
<tr class="row-odd"><td>account</td>
<td>allow.public.user.templates</td>
<td>If false, users will not be able to create public templates.</td>
</tr>
<tr class="row-even"><td>account</td>
<td>use.system.public.ips</td>
<td>If true and if an account has one or more dedicated public IP ranges, IPs are                                                                                                                              acquired from the system pool after all the IPs dedicated to the account have been consumed.</td>
</tr>
<tr class="row-odd"><td>account</td>
<td>use.system.guest.vlans</td>
<td>If true and if an account has one or more dedicated guest VLAN ranges, VLANs are allocated from the                                                                                                        system pool after all the VLANs dedicated to the account have been consumed.</td>
</tr>
<tr class="row-even"><td>cluster</td>
<td>cluster.storage.allocated.capacity.notificationthreshold</td>
<td>The percentage, as a value between 0 and 1, of allocated storage utilization                                                                                                                               above which alerts are sent that the storage is below the threshold.</td>
</tr>
<tr class="row-odd"><td>cluster</td>
<td>cluster.storage.capacity.notificationthreshold</td>
<td>The percentage, as a value between 0 and 1, of storage utilization above which alerts are sent that the available storage is below                                                                         the threshold.</td>
</tr>
<tr class="row-even"><td>cluster</td>
<td>cluster.cpu.allocated.capacity.notificationthreshold</td>
<td>The percentage, as a value between 0 and 1, of cpu utilization above which alerts are sent that the available CPU is below the                                                                             threshold.</td>
</tr>
<tr class="row-odd"><td>cluster</td>
<td>cluster.memory.allocated.capacity.notificationthreshold</td>
<td>The percentage, as a value between 0 and 1, of memory utilization above which alerts are sent that the available memory is below the                                                                       threshold.</td>
</tr>
<tr class="row-even"><td>cluster</td>
<td>cluster.cpu.allocated.capacity.disablethreshold</td>
<td>The percentage, as a value between 0 and 1, of CPU utilization above which allocators will disable that cluster from further usage.                                                                        Keep the corresponding notification threshold lower than this value to be notified beforehand.</td>
</tr>
<tr class="row-odd"><td>cluster</td>
<td>cluster.memory.allocated.capacity.disablethreshold</td>
<td>The percentage, as a value between 0 and 1, of memory utilization above which allocators will disable that cluster from further                                                                            usage. Keep the corresponding notification threshold lower than this value to be notified beforehand.</td>
</tr>
<tr class="row-even"><td>cluster</td>
<td>cpu.overprovisioning.factor</td>
<td>Used for CPU over-provisioning calculation; the available CPU will be
the mathematical product of actualCpuCapacity and cpu.overprovisioning.factor.</td>
</tr>
<tr class="row-odd"><td>cluster</td>
<td>mem.overprovisioning.factor</td>
<td>Used for memory over-provisioning calculation.</td>
</tr>
<tr class="row-even"><td>cluster</td>
<td>vmware.reserve.cpu</td>
<td>Specify whether or not to reserve CPU when not over-provisioning; In case of CPU over-provisioning, CPU is always reserved.</td>
</tr>
<tr class="row-odd"><td>cluster</td>
<td>vmware.reserve.mem</td>
<td>Specify whether or not to reserve memory when not over-provisioning; In case of memory over-provisioning memory is always reserved.</td>
</tr>
<tr class="row-even"><td>zone</td>
<td>pool.storage.allocated.capacity.disablethreshold</td>
<td>The percentage, as a value between 0 and 1, of allocated storage utilization above which allocators will disable that pool because the
available allocated storage is below the threshold.</td>
</tr>
<tr class="row-odd"><td>zone</td>
<td>pool.storage.capacity.disablethreshold</td>
<td>The percentage, as a value between 0 and 1, of storage utilization above which allocators will disable the pool because the available                                                                      storage capacity is below the threshold.</td>
</tr>
<tr class="row-even"><td>zone</td>
<td>storage.overprovisioning.factor</td>
<td>Used for storage over-provisioning calculation; available storage will be the mathematical product of actualStorageSize and                                                                                storage.overprovisioning.factor.</td>
</tr>
<tr class="row-odd"><td>zone</td>
<td>network.throttling.rate</td>
<td>Default data transfer rate in megabits per second allowed in a network.</td>
</tr>
<tr class="row-even"><td>zone</td>
<td>guest.domain.suffix</td>
<td>Default domain name for VMs inside a virtual networks with a router.</td>
</tr>
<tr class="row-odd"><td>zone</td>
<td>router.template.xen</td>
<td>Name of the default router template on Xenserver.</td>
</tr>
<tr class="row-even"><td>zone</td>
<td>router.template.kvm</td>
<td>Name of the default router template on KVM.</td>
</tr>
<tr class="row-odd"><td>zone</td>
<td>router.template.vmware</td>
<td>Name of the default router template on VMware.</td>
</tr>
<tr class="row-even"><td>zone</td>
<td>enable.dynamic.scale.vm</td>
<td>Enable or diable dynamically scaling of a VM.</td>
</tr>
<tr class="row-odd"><td>zone</td>
<td>use.external.dns</td>
<td>Bypass internal DNS, and use the external DNS1 and DNS2</td>
</tr>
<tr class="row-even"><td>zone</td>
<td>blacklisted.routes</td>
<td>Routes that are blacklisted cannot be used for creating static routes for a VPC Private Gateway.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div class="section" id="hypervisor-setup">
<span id="hypervisors"></span><h2>Hypervisor Setup<a class="headerlink" href="#hypervisor-setup" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-hypervisor/hyperv"></span><div class="section" id="host-hyper-v-installation">
<h3>Host Hyper-V Installation<a class="headerlink" href="#host-hyper-v-installation" title="Permalink to this headline">¶</a></h3>
<p>If you want to use Hyper-V hypervisor to run guest virtual machines,
install Hyper-V on the hosts in your cloud. The instructions in this
section doesn&#8217;t duplicate Hyper-V Installation documentation. It
provides the CloudStack-specific steps that are needed to prepare a
Hyper-V host to work with CloudStack.</p>
<div class="section" id="system-requirements-for-hyper-v-hypervisor-hosts">
<h4>System Requirements for Hyper-V Hypervisor Hosts<a class="headerlink" href="#system-requirements-for-hyper-v-hypervisor-hosts" title="Permalink to this headline">¶</a></h4>
<div class="section" id="supported-operating-systems-for-hyper-v-hosts">
<h5>Supported Operating Systems for Hyper-V Hosts<a class="headerlink" href="#supported-operating-systems-for-hyper-v-hosts" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Windows Server 2012 R2 Standard</li>
<li>Windows Server 2012 R2 Datacenter</li>
<li>Hyper-V 2012 R2</li>
</ul>
</div>
<div class="section" id="minimum-system-requirements-for-hyper-v-hosts">
<h5>Minimum System Requirements for Hyper-V Hosts<a class="headerlink" href="#minimum-system-requirements-for-hyper-v-hosts" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>1.4 GHz 64-bit processor with hardware-assisted virtualization.</li>
<li>800 MB of RAM</li>
<li>32 GB of disk space</li>
<li>Gigabit (10/100/1000baseT) Ethernet adapter</li>
</ul>
</div>
<div class="section" id="supported-storage">
<h5>Supported Storage<a class="headerlink" href="#supported-storage" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Primary Storage: Server Message Block (SMB) Version 3, Local</li>
<li>Secondary Storage: SMB</li>
</ul>
</div>
</div>
<div class="section" id="preparation-checklist-for-hyper-v">
<h4>Preparation Checklist for Hyper-V<a class="headerlink" href="#preparation-checklist-for-hyper-v" title="Permalink to this headline">¶</a></h4>
<p>For a smoother installation, gather the following information before you
start:</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="16%" />
<col width="13%" />
<col width="71%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Hyper-V
Requiremen
ts</th>
<th class="head">Value</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Server
Roles</td>
<td>Hyper-V</td>
<td>After the Windows Server 2012 R2 installation,
ensure that Hyper-V is selected from Server Roles.
For more information, see <a class="reference external" href="http://technet.microsoft.com/en-us/library/jj134187.aspx#BKMK_Step2">Installing
Hyper-V</a>.</td>
</tr>
<tr class="row-odd"><td>Share
Location</td>
<td>New
folders
in the
/Share
director
y</td>
<td><p class="first">Ensure that folders are created for Primary and
Secondary storage. The SMB share and the hosts
should be part of the same domain.</p>
<p class="last">If you are using Windows SMB share, the location of
the file share for the Hyper-V deployment will be
the new folder created in the \Shares on the
selected volume. You can create sub-folders for both
PRODUCT Primary and Secondary storage within the
share location. When you select the profile for the
file shares, ensure that you select SMB Share
-Applications. This creates the file shares with
settings appropriate for Hyper-V.</p>
</td>
</tr>
<tr class="row-even"><td>Domain and
Hosts</td>
<td>&nbsp;</td>
<td>Hosts should be part of the same Active Directory
domain.</td>
</tr>
<tr class="row-odd"><td>Hyper-V
Users</td>
<td>Full
control</td>
<td>Full control on the SMB file share.</td>
</tr>
<tr class="row-even"><td>Virtual
Switch</td>
<td>&nbsp;</td>
<td><p class="first">If you are using Hyper-V 2012 R2, manually create an
external virtual switch before adding the host to
PRODUCT. If the Hyper-V host is added to the Hyper-V
manager, select the host, then click Virtual Switch
Manager, then New Virtual Switch. In the External
Network, select the desired NIC adapter and click
Apply.</p>
<p class="last">If you are using Windows 2012 R2, virtual switch is
created automatically.</p>
</td>
</tr>
<tr class="row-odd"><td>Virtual
Switch
Name</td>
<td>&nbsp;</td>
<td>Take a note of the name of the virtual switch. You
need to specify that when configuring PRODUCT
physical network labels.</td>
</tr>
<tr class="row-even"><td>Hyper-V
Domain
Users</td>
<td>&nbsp;</td>
<td><ul class="first last simple">
<li>Add the Hyper-V domain users to the Hyper-V
Administrators group.</li>
<li>A domain user should have full control on the SMB
share that is exported for primary and secondary
storage.</li>
<li>This domain user should be part of the Hyper-V
Administrators and Local Administrators group on
the Hyper-V hosts that are to be managed by
PRODUCT.</li>
<li>The Hyper-V Agent service runs with the
credentials of this domain user account.</li>
<li>Specify the credential of the domain user while
adding a host to PRODUCT so that it can manage
it.</li>
<li>Specify the credential of the domain user while
adding a shared SMB primary or secondary storage.</li>
</ul>
</td>
</tr>
<tr class="row-odd"><td>Migration</td>
<td>Migratio
n</td>
<td>Enable Migration.</td>
</tr>
<tr class="row-even"><td>Migration</td>
<td>Delegati
on</td>
<td>If you want to use Live Migration, enable
Delegation. Enable the following services of other
hosts participating in Live Migration: CIFS and
Microsoft Virtual System Migration Service.</td>
</tr>
<tr class="row-odd"><td>Migration</td>
<td>Kerberos</td>
<td>Enable Kerberos for Live Migration.</td>
</tr>
<tr class="row-even"><td>Network
Access
Permission
for
Dial-in</td>
<td>Allow
access</td>
<td>Allow access for Dial-in connections.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="hyper-v-installation-steps">
<h4>Hyper-V Installation Steps<a class="headerlink" href="#hyper-v-installation-steps" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li>Download the operating system from <a class="reference external" href="http://technet.microsoft.com/en-us/windowsserver/hh534429">Windows Server 2012 R2</a>.</li>
<li>Install it on the host as given in <a class="reference external" href="http://technet.microsoft.com/library/hh831620">Install and Deploy Windows Server 2012
R2</a>.</li>
<li>Post installation, ensure that you enable Hyper-V role in the server.</li>
<li>If no Active Directory domain exists in your deployment, create one
and add users to the domain.</li>
<li>In the Active Directory domain, ensure that all the Hyper-v hosts are
added so that all the hosts are part of the domain.</li>
<li>Add the domain user to the following groups on the Hyper-V host:
Hyper-V Administrators and Local Administrators.</li>
</ol>
</div>
<div class="section" id="installing-the-cloudstack-agent-on-a-hyper-v-host">
<h4>Installing the CloudStack Agent on a Hyper-V Host<a class="headerlink" href="#installing-the-cloudstack-agent-on-a-hyper-v-host" title="Permalink to this headline">¶</a></h4>
<p>The Hyper-V Agent helps CloudStack perform operations on the Hyper-V
hosts. This Agent communicates with the Management Server and controls
all the instances on the host. Each Hyper-V host must have the Hyper-V
Agent installed on it for successful interaction between the host and
CloudStack. The Hyper-V Agent runs as a Windows service. Install the
Agent on each host using the following steps.</p>
<p>CloudStack Management Server communicates with Hyper-V Agent by using
HTTPS. For secure communication between the Management Server and the
host, install a self-signed certificate on port 8250.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The Agent installer automatically perform this operation. You have not
selected this option during the Agent installation, it can also be done
manually as given in step 1.</p>
</div>
<ol class="arabic">
<li><p class="first">Create and add a self-signed SSL certificate on port 8250:</p>
<ol class="arabic">
<li><p class="first">Create A self-signed SSL certificate:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># New-SelfSignedCertificate -DnsName apachecloudstack -CertStoreLocation Cert:\LocalMachine\My</span>
</pre></div>
</div>
<p>This command creates the self-signed certificate and add that to
the certificate store <code class="docutils literal"><span class="pre">LocalMachine\My</span></code>.</p>
</li>
<li><p class="first">Add the created certificate to port 8250 for https communication:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>netsh http add sslcert <span class="nv">ipport</span><span class="o">=</span>0.0.0.0:8250 <span class="nv">certhash</span><span class="o">=</span>&lt;thumbprint&gt; <span class="nv">appid</span><span class="o">=</span><span class="s2">&quot;{727beb1c-6e7c-49b2-8fbd-f03dbe481b08}&quot;</span>
</pre></div>
</div>
<p>Thumbprint is the thumbprint of the certificate you created.</p>
</li>
</ol>
</li>
<li><p class="first">Build the CloudStack Agent for Hyper-V as given in <a class="reference external" href="https://cwiki.apache.org/confluence/display/CLOUDSTACK/Creating+Hyperv+Agent+Installer">Building CloudStack
Hyper-V Agent</a>.</p>
</li>
<li><p class="first">As an administrator, run the installer.</p>
</li>
<li><p class="first">Provide the Hyper-V admin credentials when prompted.</p>
<p>When the agent installation is finished, the agent runs as a service
on the host machine.</p>
</li>
</ol>
</div>
<div class="section" id="physical-network-configuration-for-hyper-v">
<h4>Physical Network Configuration for Hyper-V<a class="headerlink" href="#physical-network-configuration-for-hyper-v" title="Permalink to this headline">¶</a></h4>
<p>You should have a plan for how the hosts will be cabled and which
physical NICs will carry what types of traffic. By default, CloudStack
will use the device that is used for the default route.</p>
<p>If you are using Hyper-V 2012 R2, manually create an external virtual
switch before adding the host to CloudStack. If the Hyper-V host is
added to the Hyper-V manager, select the host, then click Virtual Switch
Manager, then New Virtual Switch. In the External Network, select the
desired NIC adapter and click Apply.</p>
<p>If you are using Windows 2012 R2, virtual switch is created
automatically.</p>
</div>
<div class="section" id="storage-preparation-for-hyper-v-optional">
<h4>Storage Preparation for Hyper-V (Optional)<a class="headerlink" href="#storage-preparation-for-hyper-v-optional" title="Permalink to this headline">¶</a></h4>
<p>CloudStack allows administrators to set up shared Primary Storage and
Secondary Storage that uses SMB.</p>
<ol class="arabic">
<li><p class="first">Create a SMB storage and expose it over SMB Version 3.</p>
<p>For more information, see <a class="reference external" href="http://technet.microsoft.com/en-us/library/jj134187.aspx">Deploying Hyper-V over SMB</a>.</p>
<p>You can also create and export SMB share using Windows. After the
Windows Server 2012 R2 installation, select File and Storage Services
from Server Roles to create an SMB file share. For more information,
see <a class="reference external" href="http://technet.microsoft.com/en-us/library/jj134187.aspx#BKMK_Step3">Creating an SMB File Share Using Server Manager</a>.</p>
</li>
<li><p class="first">Add the SMB share to the Active Directory domain.</p>
<p>The SMB share and the hosts managed by CloudStack need to be in the
same domain. However, the storage should be accessible from the
Management Server with the domain user privileges.</p>
</li>
<li><p class="first">While adding storage to CloudStack, ensure that the correct domain,
and credentials are supplied. This user should be able to access the
storage from the Management Server.</p>
</li>
</ol>
</div>
</div>
<span id="document-hypervisor/kvm"></span><div class="section" id="host-kvm-installation">
<h3>Host KVM Installation<a class="headerlink" href="#host-kvm-installation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="system-requirements-for-kvm-hypervisor-hosts">
<h4>System Requirements for KVM Hypervisor Hosts<a class="headerlink" href="#system-requirements-for-kvm-hypervisor-hosts" title="Permalink to this headline">¶</a></h4>
<p>KVM is included with a variety of Linux-based operating systems.
Although you are not required to run these distributions, the following
are recommended:</p>
<ul class="simple">
<li>CentOS / RHEL: 7.X</li>
<li>Ubuntu: 14.04</li>
</ul>
<p>The main requirement for KVM hypervisors is the libvirt and Qemu
version. No matter what Linux distribution you are using, make sure the
following requirements are met:</p>
<ul class="simple">
<li>libvirt: 1.2.0 or higher</li>
<li>Qemu/KVM: 2.0 or higher</li>
</ul>
<p>The default bridge in CloudStack is the Linux native bridge
implementation (bridge module). CloudStack includes an option to work
with OpenVswitch, the requirements are listed below</p>
<ul class="simple">
<li>libvirt: 1.2.0 or higher</li>
<li>openvswitch: 1.7.1 or higher</li>
</ul>
<p>In addition, the following hardware requirements apply:</p>
<ul class="simple">
<li>Within a single cluster, the hosts must be of the same distribution
version.</li>
<li>All hosts within a cluster must be homogenous. The CPUs must be of
the same type, count, and feature flags.</li>
<li>Must support HVM (Intel-VT or AMD-V enabled)</li>
<li>64-bit x86 CPU (more cores results in better performance)</li>
<li>4 GB of memory</li>
<li>At least 1 NIC</li>
<li>When you deploy CloudStack, the hypervisor host must not have any VMs
already running. These will be destroy by CloudStack.</li>
</ul>
</div>
<div class="section" id="kvm-installation-overview">
<h4>KVM Installation Overview<a class="headerlink" href="#kvm-installation-overview" title="Permalink to this headline">¶</a></h4>
<p>If you want to use the Linux Kernel Virtual Machine (KVM) hypervisor to
run guest virtual machines, install KVM on the host(s) in your cloud.
The material in this section doesn&#8217;t duplicate KVM installation docs. It
provides the CloudStack-specific steps that are needed to prepare a KVM
host to work with CloudStack.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Before continuing, make sure that you have applied the latest updates to
your host.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">It is NOT recommended to run services on this host not controlled by
CloudStack.</p>
</div>
<p>The procedure for installing a KVM Hypervisor Host is:</p>
<ol class="arabic simple">
<li>Prepare the Operating System</li>
<li>Install and configure libvirt</li>
<li>Configure Security Policies (AppArmor and SELinux)</li>
<li>Install and configure the Agent</li>
</ol>
</div>
<div class="section" id="prepare-the-operating-system">
<h4>Prepare the Operating System<a class="headerlink" href="#prepare-the-operating-system" title="Permalink to this headline">¶</a></h4>
<p>The OS of the Host must be prepared to host the CloudStack Agent and run
KVM instances.</p>
<ol class="arabic">
<li><p class="first">Log in to your OS as root.</p>
</li>
<li><p class="first">Check for a fully qualified hostname.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ hostname --fqdn
</pre></div>
</div>
<p>This should return a fully qualified hostname such as
&#8220;kvm1.lab.example.org&#8221;. If it does not, edit /etc/hosts so that it
does.</p>
</li>
<li><p class="first">Make sure that the machine can reach the Internet.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ping www.cloudstack.org
</pre></div>
</div>
</li>
<li><p class="first">Turn on NTP for time synchronization.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">NTP is required to synchronize the clocks of the servers in your
cloud. Unsynchronized clocks can cause unexpected problems.</p>
</div>
<ol class="arabic">
<li><p class="first">Install NTP</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ yum install ntp
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ apt-get install openntpd
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Repeat all of these steps on every hypervisor host.</p>
</li>
</ol>
</div>
<div class="section" id="install-and-configure-the-agent">
<h4>Install and configure the Agent<a class="headerlink" href="#install-and-configure-the-agent" title="Permalink to this headline">¶</a></h4>
<p>To manage KVM instances on the host CloudStack uses a Agent. This Agent
communicates with the Management server and controls all the instances
on the host.</p>
<p>First we start by installing the agent:</p>
<p>In RHEL or CentOS:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ yum install cloudstack-agent
</pre></div>
</div>
<p>In Ubuntu:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ apt-get install cloudstack-agent
</pre></div>
</div>
<p>The host is now ready to be added to a cluster. This is covered in a
later section, see <a class="reference internal" href="index.html#adding-a-host"><span>Adding a Host</span></a>. It is
recommended that you continue to read the documentation before adding
the host!</p>
<p>If you&#8217;re using a non-root user to add the KVM host, please add the user to
sudoers file:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cloudstack <span class="nv">ALL</span><span class="o">=</span>NOPASSWD: /usr/bin/cloudstack-setup-agent
defaults:cloudstack !requiretty
</pre></div>
</div>
<div class="section" id="configure-cpu-model-for-kvm-guest-optional">
<h5>Configure CPU model for KVM guest (Optional)<a class="headerlink" href="#configure-cpu-model-for-kvm-guest-optional" title="Permalink to this headline">¶</a></h5>
<p>In additional,the CloudStack Agent allows host administrator to control
the guest CPU model which is exposed to KVM instances. By default, the
CPU model of KVM instance is likely QEMU Virtual CPU version x.x.x with
least CPU features exposed. There are a couple of reasons to specify the
CPU model:</p>
<ul class="simple">
<li>To maximise performance of instances by exposing new host CPU
features to the KVM instances;</li>
<li>To ensure a consistent default CPU across all machines,removing
reliance of variable QEMU defaults;</li>
</ul>
<p>For the most part it will be sufficient for the host administrator to
specify the guest CPU config in the per-host configuration file
(/etc/cloudstack/agent/agent.properties). This will be achieved by
introducing following configuration parameters:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>guest.cpu.mode<span class="o">=</span>custom<span class="p">|</span>host-model<span class="p">|</span>host-passthrough
guest.cpu.model<span class="o">=</span>from /usr/share/libvirt/cpu_map.xml<span class="o">(</span>only valid when guest.cpu.mode<span class="o">=</span>custom<span class="o">)</span>
guest.cpu.features<span class="o">=</span>vmx ept aes smx mmx ht <span class="o">(</span>space separated list of cpu flags to apply<span class="o">)</span>
</pre></div>
</div>
<p>There are three choices to fulfill the cpu model changes:</p>
<ol class="arabic simple">
<li><strong>custom:</strong> you can explicitly specify one of the supported named
model in /usr/share/libvirt/cpu_map.xml</li>
<li><strong>host-model:</strong> libvirt will identify the CPU model in
/usr/share/libvirt/cpu_map.xml which most closely matches the host,
and then request additional CPU flags to complete the match. This
should give close to maximum functionality/performance, which
maintaining good reliability/compatibility if the guest is migrated
to another host with slightly different host CPUs.</li>
<li><strong>host-passthrough:</strong> libvirt will tell KVM to passthrough the host
CPU with no modifications. The difference to host-model, instead of
just matching feature flags, every last detail of the host CPU is
matched. This gives absolutely best performance, and can be important
to some apps which check low level CPU details, but it comes at a
cost with respect to migration: the guest can only be migrated to an
exactly matching host CPU.</li>
</ol>
<p>Here are some examples:</p>
<ul>
<li><p class="first">custom</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>guest.cpu.mode<span class="o">=</span>custom
guest.cpu.model<span class="o">=</span>SandyBridge
</pre></div>
</div>
</li>
<li><p class="first">host-model</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>guest.cpu.mode<span class="o">=</span>host-model
</pre></div>
</div>
</li>
<li><p class="first">host-passthrough</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>guest.cpu.mode<span class="o">=</span>host-passthrough
guest.cpu.features<span class="o">=</span>vmx
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">host-passthrough may lead to migration failure,if you have this problem,
you should use host-model or custom. guest.cpu.features will force cpu features
as a required policy so make sure to put only those features that are provided
by the host CPU.</p>
</div>
</div>
</div>
<div class="section" id="install-and-configure-libvirt">
<h4>Install and Configure libvirt<a class="headerlink" href="#install-and-configure-libvirt" title="Permalink to this headline">¶</a></h4>
<p>CloudStack uses libvirt for managing virtual machines. Therefore it is
vital that libvirt is configured correctly. Libvirt is a dependency of
cloudstack-agent and should already be installed.</p>
<ol class="arabic">
<li><p class="first">In order to have live migration working libvirt has to listen for
unsecured TCP connections. We also need to turn off libvirts attempt
to use Multicast DNS advertising. Both of these settings are in
<code class="docutils literal"><span class="pre">/etc/libvirt/libvirtd.conf</span></code></p>
<p>Set the following parameters:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">listen_tls</span> <span class="o">=</span> 0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">listen_tcp</span> <span class="o">=</span> 1
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">tcp_port</span> <span class="o">=</span> <span class="s2">&quot;16509&quot;</span>
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">auth_tcp</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">mdns_adv</span> <span class="o">=</span> 0
</pre></div>
</div>
</li>
<li><p class="first">Turning on &#8220;listen_tcp&#8221; in libvirtd.conf is not enough, we have to
change the parameters as well:</p>
<p>On RHEL or CentOS modify <code class="docutils literal"><span class="pre">/etc/sysconfig/libvirtd</span></code>:</p>
<p>Uncomment the following line:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1">#LIBVIRTD_ARGS=&quot;--listen&quot;</span>
</pre></div>
</div>
<p>On Ubuntu: modify <code class="docutils literal"><span class="pre">/etc/default/libvirt-bin</span></code></p>
<p>Add &#8220;-l&#8221; to the following line</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">libvirtd_opts</span><span class="o">=</span><span class="s2">&quot;-d&quot;</span>
</pre></div>
</div>
<p>so it looks like:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">libvirtd_opts</span><span class="o">=</span><span class="s2">&quot;-d -l&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">Restart libvirt</p>
<p>In RHEL or CentOS:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ service libvirtd restart
</pre></div>
</div>
<p>In Ubuntu:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ service libvirt-bin restart
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="configure-the-security-policies">
<h4>Configure the Security Policies<a class="headerlink" href="#configure-the-security-policies" title="Permalink to this headline">¶</a></h4>
<p>CloudStack does various things which can be blocked by security
mechanisms like AppArmor and SELinux. These have to be disabled to
ensure the Agent has all the required permissions.</p>
<ol class="arabic">
<li><p class="first">Configure SELinux (RHEL and CentOS)</p>
<ol class="arabic">
<li><p class="first">Check to see whether SELinux is installed on your machine. If not,
you can skip this section.</p>
<p>In RHEL or CentOS, SELinux is installed and enabled by default.
You can verify this with:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ rpm -qa <span class="p">|</span> grep selinux
</pre></div>
</div>
</li>
<li><p class="first">Set the SELINUX variable in <code class="docutils literal"><span class="pre">/etc/selinux/config</span></code> to
&#8220;permissive&#8221;. This ensures that the permissive setting will be
maintained after a system reboot.</p>
<p>In RHEL or CentOS:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/selinux/config
</pre></div>
</div>
<p>Change the following line</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">SELINUX</span><span class="o">=</span>enforcing
</pre></div>
</div>
<p>to this</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">SELINUX</span><span class="o">=</span>permissive
</pre></div>
</div>
</li>
<li><p class="first">Then set SELinux to permissive starting immediately, without
requiring a system reboot.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ setenforce permissive
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Configure Apparmor (Ubuntu)</p>
<ol class="arabic">
<li><p class="first">Check to see whether AppArmor is installed on your machine. If
not, you can skip this section.</p>
<p>In Ubuntu AppArmor is installed and enabled by default. You can
verify this with:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ dpkg --list <span class="s1">&#39;apparmor&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first">Disable the AppArmor profiles for libvirt</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ln -s /etc/apparmor.d/usr.sbin.libvirtd /etc/apparmor.d/disable/
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ln -s /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper /etc/apparmor.d/disable/
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ apparmor_parser -R /etc/apparmor.d/usr.sbin.libvirtd
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ apparmor_parser -R /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</div>
<div class="section" id="configure-the-network-bridges">
<h4>Configure the network bridges<a class="headerlink" href="#configure-the-network-bridges" title="Permalink to this headline">¶</a></h4>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This is a very important section, please make sure you read this thoroughly.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This section details how to configure bridges using the native
implementation in Linux. Please refer to the next section if you intend to
use OpenVswitch</p>
</div>
<p>In order to forward traffic to your instances you will need at least two
bridges: <em>public</em> and <em>private</em>.</p>
<p>By default these bridges are called <em>cloudbr0</em> and <em>cloudbr1</em>, but you
do have to make sure they are available on each hypervisor.</p>
<p>The most important factor is that you keep the configuration consistent
on all your hypervisors.</p>
<div class="section" id="network-example">
<h5>Network example<a class="headerlink" href="#network-example" title="Permalink to this headline">¶</a></h5>
<p>There are many ways to configure your network. In the Basic networking
mode you should have two (V)LAN&#8217;s, one for your private network and one
for the public network.</p>
<p>We assume that the hypervisor has one NIC (eth0) with three tagged
VLAN&#8217;s:</p>
<ol class="arabic simple">
<li>VLAN 100 for management of the hypervisor</li>
<li>VLAN 200 for public network of the instances (cloudbr0)</li>
<li>VLAN 300 for private network of the instances (cloudbr1)</li>
</ol>
<p>On VLAN 100 we give the Hypervisor the IP-Address 192.168.42.11/24 with
the gateway 192.168.42.1</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The Hypervisor and Management server don&#8217;t have to be in the same subnet!</p>
</div>
</div>
<div class="section" id="configuring-the-network-bridges">
<h5>Configuring the network bridges<a class="headerlink" href="#configuring-the-network-bridges" title="Permalink to this headline">¶</a></h5>
<p>It depends on the distribution you are using how to configure these,
below you&#8217;ll find examples for RHEL/CentOS and Ubuntu.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The goal is to have two bridges called &#8216;cloudbr0&#8217; and &#8216;cloudbr1&#8217; after this
section. This should be used as a guideline only. The exact configuration
will depend on your network layout.</p>
</div>
<div class="section" id="configure-in-rhel-or-centos">
<h6>Configure in RHEL or CentOS<a class="headerlink" href="#configure-in-rhel-or-centos" title="Permalink to this headline">¶</a></h6>
<p>The required packages were installed when libvirt was installed, we can
proceed to configuring the network.</p>
<p>First we configure eth0</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0
</pre></div>
</div>
<p>Make sure it looks similar to:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
</pre></div>
</div>
<p>We now have to configure the three VLAN interfaces:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0.100
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0.100
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
<span class="nv">VLAN</span><span class="o">=</span>yes
<span class="nv">IPADDR</span><span class="o">=</span>192.168.42.11
<span class="nv">GATEWAY</span><span class="o">=</span>192.168.42.1
<span class="nv">NETMASK</span><span class="o">=</span>255.255.255.0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0.200
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0.200
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
<span class="nv">VLAN</span><span class="o">=</span>yes
<span class="nv">BRIDGE</span><span class="o">=</span>cloudbr0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0.300
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0.300
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
<span class="nv">VLAN</span><span class="o">=</span>yes
<span class="nv">BRIDGE</span><span class="o">=</span>cloudbr1
</pre></div>
</div>
<p>Now we have the VLAN interfaces configured we can add the bridges on top
of them.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-cloudbr0
</pre></div>
</div>
<p>Now we just configure it is a plain bridge without an IP-Address</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>cloudbr0
<span class="nv">TYPE</span><span class="o">=</span>Bridge
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">IPV6INIT</span><span class="o">=</span>no
<span class="nv">IPV6_AUTOCONF</span><span class="o">=</span>no
<span class="nv">DELAY</span><span class="o">=</span>5
<span class="nv">STP</span><span class="o">=</span>yes
</pre></div>
</div>
<p>We do the same for cloudbr1</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-cloudbr1
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>cloudbr1
<span class="nv">TYPE</span><span class="o">=</span>Bridge
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">IPV6INIT</span><span class="o">=</span>no
<span class="nv">IPV6_AUTOCONF</span><span class="o">=</span>no
<span class="nv">DELAY</span><span class="o">=</span>5
<span class="nv">STP</span><span class="o">=</span>yes
</pre></div>
</div>
<p>With this configuration you should be able to restart the network,
although a reboot is recommended to see if everything works properly.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Make sure you have an alternative way like IPMI or ILO to reach the machine
in case you made a configuration error and the network stops functioning!</p>
</div>
</div>
<div class="section" id="configure-in-ubuntu">
<h6>Configure in Ubuntu<a class="headerlink" href="#configure-in-ubuntu" title="Permalink to this headline">¶</a></h6>
<p>All the required packages were installed when you installed libvirt, so
we only have to configure the network.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/network/interfaces
</pre></div>
</div>
<p>Modify the interfaces file to look like this:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>auto lo
iface lo inet loopback

<span class="c1"># The primary network interface</span>
auto eth0.100
iface eth0.100 inet static
    address 192.168.42.11
    netmask 255.255.255.240
    gateway 192.168.42.1
    dns-nameservers 8.8.8.8 8.8.4.4
    dns-domain lab.example.org

<span class="c1"># Public network</span>
auto cloudbr0
iface cloudbr0 inet manual
    bridge_ports eth0.200
    bridge_fd 5
    bridge_stp off
    bridge_maxwait 1

<span class="c1"># Private network</span>
auto cloudbr1
iface cloudbr1 inet manual
    bridge_ports eth0.300
    bridge_fd 5
    bridge_stp off
    bridge_maxwait 1
</pre></div>
</div>
<p>With this configuration you should be able to restart the network,
although a reboot is recommended to see if everything works properly.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Make sure you have an alternative way like IPMI or ILO to reach the machine
in case you made a configuration error and the network stops functioning!</p>
</div>
</div>
</div>
</div>
<div class="section" id="configure-the-network-using-openvswitch">
<h4>Configure the network using OpenVswitch<a class="headerlink" href="#configure-the-network-using-openvswitch" title="Permalink to this headline">¶</a></h4>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This is a very important section, please make sure you read this thoroughly.</p>
</div>
<p>In order to forward traffic to your instances you will need at least two
bridges: <em>public</em> and <em>private</em>.</p>
<p>By default these bridges are called <em>cloudbr0</em> and <em>cloudbr1</em>, but you
do have to make sure they are available on each hypervisor.</p>
<p>The most important factor is that you keep the configuration consistent
on all your hypervisors.</p>
<div class="section" id="preparing">
<h5>Preparing<a class="headerlink" href="#preparing" title="Permalink to this headline">¶</a></h5>
<p>To make sure that the native bridge module will not interfere with
openvswitch the bridge module should be added to the blacklist. See the
modprobe documentation for your distribution on where to find the
blacklist. Make sure the module is not loaded either by rebooting or
executing rmmod bridge before executing next steps.</p>
<p>The network configurations below depend on the ifup-ovs and ifdown-ovs
scripts which are part of the openvswitch installation. They should be
installed in /etc/sysconfig/network-scripts/</p>
</div>
<div class="section" id="id1">
<h5>Network example<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>There are many ways to configure your network. In the Basic networking
mode you should have two (V)LAN&#8217;s, one for your private network and one
for the public network.</p>
<p>We assume that the hypervisor has one NIC (eth0) with three tagged
VLAN&#8217;s:</p>
<ol class="arabic simple">
<li>VLAN 100 for management of the hypervisor</li>
<li>VLAN 200 for public network of the instances (cloudbr0)</li>
<li>VLAN 300 for private network of the instances (cloudbr1)</li>
</ol>
<p>On VLAN 100 we give the Hypervisor the IP-Address 192.168.42.11/24 with
the gateway 192.168.42.1</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The Hypervisor and Management server don&#8217;t have to be in the same subnet!</p>
</div>
</div>
<div class="section" id="id2">
<h5>Configuring the network bridges<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<p>It depends on the distribution you are using how to configure these,
below you&#8217;ll find examples for RHEL/CentOS.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The goal is to have three bridges called &#8216;mgmt0&#8217;, &#8216;cloudbr0&#8217; and &#8216;cloudbr1&#8217;
after this section. This should be used as a guideline only. The exact
configuration will depend on your network layout.</p>
</div>
<div class="section" id="configure-openvswitch">
<h6>Configure OpenVswitch<a class="headerlink" href="#configure-openvswitch" title="Permalink to this headline">¶</a></h6>
<p>The network interfaces using OpenVswitch are created using the ovs-vsctl
command. This command will configure the interfaces and persist them to
the OpenVswitch database.</p>
<p>First we create a main bridge connected to the eth0 interface. Next we
create three fake bridges, each connected to a specific vlan tag.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># ovs-vsctl add-br cloudbr</span>
<span class="c1"># ovs-vsctl add-port cloudbr eth0</span>
<span class="c1"># ovs-vsctl set port cloudbr trunks=100,200,300</span>
<span class="c1"># ovs-vsctl add-br mgmt0 cloudbr 100</span>
<span class="c1"># ovs-vsctl add-br cloudbr0 cloudbr 200</span>
<span class="c1"># ovs-vsctl add-br cloudbr1 cloudbr 300</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h6>Configure in RHEL or CentOS<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h6>
<p>The required packages were installed when openvswitch and libvirt were
installed, we can proceed to configuring the network.</p>
<p>First we configure eth0</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0
</pre></div>
</div>
<p>Make sure it looks similar to:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
</pre></div>
</div>
<p>We have to configure the base bridge with the trunk.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-cloudbr
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>cloudbr
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">DEVICETYPE</span><span class="o">=</span>ovs
<span class="nv">TYPE</span><span class="o">=</span>OVSBridge
</pre></div>
</div>
<p>We now have to configure the three VLAN bridges:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-mgmt0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>mgmt0
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>static
<span class="nv">DEVICETYPE</span><span class="o">=</span>ovs
<span class="nv">TYPE</span><span class="o">=</span>OVSBridge
<span class="nv">IPADDR</span><span class="o">=</span>192.168.42.11
<span class="nv">GATEWAY</span><span class="o">=</span>192.168.42.1
<span class="nv">NETMASK</span><span class="o">=</span>255.255.255.0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-cloudbr0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>cloudbr0
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">DEVICETYPE</span><span class="o">=</span>ovs
<span class="nv">TYPE</span><span class="o">=</span>OVSBridge
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-cloudbr1
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>cloudbr1
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>OVSBridge
<span class="nv">DEVICETYPE</span><span class="o">=</span>ovs
</pre></div>
</div>
<p>With this configuration you should be able to restart the network,
although a reboot is recommended to see if everything works properly.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Make sure you have an alternative way like IPMI or ILO to reach the machine
in case you made a configuration error and the network stops functioning!</p>
</div>
</div>
</div>
</div>
<div class="section" id="configuring-the-firewall">
<h4>Configuring the firewall<a class="headerlink" href="#configuring-the-firewall" title="Permalink to this headline">¶</a></h4>
<p>The hypervisor needs to be able to communicate with other hypervisors
and the management server needs to be able to reach the hypervisor.</p>
<p>In order to do so we have to open the following TCP ports (if you are
using a firewall):</p>
<ol class="arabic simple">
<li>22 (SSH)</li>
<li>1798</li>
<li>16509 (libvirt)</li>
<li>5900 - 6100 (VNC consoles)</li>
<li>49152 - 49216 (libvirt live migration)</li>
</ol>
<p>It depends on the firewall you are using how to open these ports. Below
you&#8217;ll find examples how to open these ports in RHEL/CentOS and Ubuntu.</p>
<div class="section" id="open-ports-in-rhel-centos">
<h5>Open ports in RHEL/CentOS<a class="headerlink" href="#open-ports-in-rhel-centos" title="Permalink to this headline">¶</a></h5>
<p>RHEL and CentOS use iptables for firewalling the system, you can open
extra ports by executing the following iptable commands:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport <span class="m">22</span> -j ACCEPT
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport <span class="m">1798</span> -j ACCEPT
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport <span class="m">16509</span> -j ACCEPT
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport 5900:6100 -j ACCEPT
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport 49152:49216 -j ACCEPT
</pre></div>
</div>
<p>These iptable settings are not persistent accross reboots, we have to
save them first.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables-save &gt; /etc/sysconfig/iptables
</pre></div>
</div>
</div>
<div class="section" id="open-ports-in-ubuntu">
<h5>Open ports in Ubuntu<a class="headerlink" href="#open-ports-in-ubuntu" title="Permalink to this headline">¶</a></h5>
<p>The default firewall under Ubuntu is UFW (Uncomplicated FireWall), which
is a Python wrapper around iptables.</p>
<p>To open the required ports, execute the following commands:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 22
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 1798
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 16509
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 5900:6100
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 49152:49216
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">By default UFW is not enabled on Ubuntu. Executing these commands with the
firewall disabled does not enable the firewall.</p>
</div>
</div>
</div>
<div class="section" id="add-the-host-to-cloudstack">
<h4>Add the host to CloudStack<a class="headerlink" href="#add-the-host-to-cloudstack" title="Permalink to this headline">¶</a></h4>
<p>The host is now ready to be added to a cluster. This is covered in a
later section, see <a class="reference internal" href="index.html#adding-a-host"><span>Adding a Host</span></a>. It is
recommended that you continue to read the documentation before adding
the host!</p>
</div>
</div>
<span id="document-hypervisor/lxc"></span><div class="section" id="host-lxc-installation">
<h3>Host LXC Installation<a class="headerlink" href="#host-lxc-installation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="system-requirements-for-lxc-hosts">
<h4>System Requirements for LXC Hosts<a class="headerlink" href="#system-requirements-for-lxc-hosts" title="Permalink to this headline">¶</a></h4>
<p>LXC requires the Linux kernel cgroups functionality which is available
starting 2.6.24. Although you are not required to run these
distributions, the following are recommended:</p>
<ul class="simple">
<li>CentOS / RHEL: 6.3</li>
<li>Ubuntu: 12.04(.1)</li>
</ul>
<p>The main requirement for LXC hypervisors is the libvirt and Qemu
version. No matter what Linux distribution you are using, make sure the
following requirements are met:</p>
<ul class="simple">
<li>libvirt: 1.0.0 or higher</li>
<li>Qemu/KVM: 1.0 or higher</li>
</ul>
<p>The default bridge in CloudStack is the Linux native bridge
implementation (bridge module). CloudStack includes an option to work
with OpenVswitch, the requirements are listed below</p>
<ul class="simple">
<li>libvirt: 1.0.0 or higher</li>
<li>openvswitch: 1.7.1 or higher</li>
</ul>
<p>In addition, the following hardware requirements apply:</p>
<ul class="simple">
<li>Within a single cluster, the hosts must be of the same distribution
version.</li>
<li>All hosts within a cluster must be homogenous. The CPUs must be of
the same type, count, and feature flags.</li>
<li>Must support HVM (Intel-VT or AMD-V enabled)</li>
<li>64-bit x86 CPU (more cores results in better performance)</li>
<li>4 GB of memory</li>
<li>At least 1 NIC</li>
<li>When you deploy CloudStack, the hypervisor host must not have any VMs
already running</li>
</ul>
</div>
<div class="section" id="lxc-installation-overview">
<h4>LXC Installation Overview<a class="headerlink" href="#lxc-installation-overview" title="Permalink to this headline">¶</a></h4>
<p>LXC does not have any native system VMs, instead KVM will be used to run
system VMs. This means that your host will need to support both LXC and
KVM, thus most of the installation and configuration will be identical
to the KVM installation. The material in this section doesn&#8217;t duplicate
KVM installation docs. It provides the CloudStack-specific steps that
are needed to prepare a KVM host to work with CloudStack.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Before continuing, make sure that you have applied the latest updates to
your host.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">It is NOT recommended to run services on this host not controlled by
CloudStack.</p>
</div>
<p>The procedure for installing an LXC Host is:</p>
<ol class="arabic simple">
<li>Prepare the Operating System</li>
<li>Install and configure libvirt</li>
<li>Configure Security Policies (AppArmor and SELinux)</li>
<li>Install and configure the Agent</li>
</ol>
</div>
<div class="section" id="prepare-the-operating-system">
<h4>Prepare the Operating System<a class="headerlink" href="#prepare-the-operating-system" title="Permalink to this headline">¶</a></h4>
<p>The OS of the Host must be prepared to host the CloudStack Agent and run
KVM instances.</p>
<ol class="arabic">
<li><p class="first">Log in to your OS as root.</p>
</li>
<li><p class="first">Check for a fully qualified hostname.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ hostname --fqdn
</pre></div>
</div>
<p>This should return a fully qualified hostname such as
&#8220;kvm1.lab.example.org&#8221;. If it does not, edit /etc/hosts so that it
does.</p>
</li>
<li><p class="first">Make sure that the machine can reach the Internet.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ping www.cloudstack.org
</pre></div>
</div>
</li>
<li><p class="first">Turn on NTP for time synchronization.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">NTP is required to synchronize the clocks of the servers in your
cloud. Unsynchronized clocks can cause unexpected problems.</p>
</div>
<ol class="arabic">
<li><p class="first">Install NTP</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ yum install ntp
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ apt-get install openntpd
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Repeat all of these steps on every hypervisor host.</p>
</li>
</ol>
</div>
<div class="section" id="install-and-configure-the-agent">
<h4>Install and configure the Agent<a class="headerlink" href="#install-and-configure-the-agent" title="Permalink to this headline">¶</a></h4>
<p>To manage LXC instances on the host CloudStack uses a Agent. This Agent
communicates with the Management server and controls all the instances
on the host.</p>
<p>First we start by installing the agent:</p>
<p>In RHEL or CentOS:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ yum install cloudstack-agent
</pre></div>
</div>
<p>In Ubuntu:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ apt-get install cloudstack-agent
</pre></div>
</div>
<p>Next step is to update the Agent configuration setttings. The settings
are in <code class="docutils literal"><span class="pre">/etc/cloudstack/agent/agent.properties</span></code></p>
<ol class="arabic">
<li><p class="first">Set the Agent to run in LXC mode:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>hypervisor.type<span class="o">=</span>lxc
</pre></div>
</div>
</li>
<li><p class="first">Optional: If you would like to use direct networking (instead of the
default bridge networking), configure these lines:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>libvirt.vif.driver<span class="o">=</span>com.cloud.hypervisor.kvm.resource.DirectVifDriver
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>network.direct.source.mode<span class="o">=</span>private
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>network.direct.device<span class="o">=</span>eth0
</pre></div>
</div>
</li>
</ol>
<p>The host is now ready to be added to a cluster. This is covered in a
later section, see <a class="reference internal" href="index.html#adding-a-host"><span>Adding a Host</span></a>. It is
recommended that you continue to read the documentation before adding
the host!</p>
</div>
<div class="section" id="install-and-configure-libvirt">
<h4>Install and Configure libvirt<a class="headerlink" href="#install-and-configure-libvirt" title="Permalink to this headline">¶</a></h4>
<p>CloudStack uses libvirt for managing virtual machines. Therefore it is
vital that libvirt is configured correctly. Libvirt is a dependency of
cloudstack-agent and should already be installed.</p>
<ol class="arabic">
<li><p class="first">In order to have live migration working libvirt has to listen for
unsecured TCP connections. We also need to turn off libvirts attempt
to use Multicast DNS advertising. Both of these settings are in
<code class="docutils literal"><span class="pre">/etc/libvirt/libvirtd.conf</span></code></p>
<p>Set the following parameters:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">listen_tls</span> <span class="o">=</span> 0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">listen_tcp</span> <span class="o">=</span> 1
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">tcp_port</span> <span class="o">=</span> <span class="s2">&quot;16509&quot;</span>
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">auth_tcp</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">mdns_adv</span> <span class="o">=</span> 0
</pre></div>
</div>
</li>
<li><p class="first">Turning on &#8220;listen_tcp&#8221; in libvirtd.conf is not enough, we have to
change the parameters as well:</p>
<p>On RHEL or CentOS modify <code class="docutils literal"><span class="pre">/etc/sysconfig/libvirtd</span></code>:</p>
<p>Uncomment the following line:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1">#LIBVIRTD_ARGS=&quot;--listen&quot;</span>
</pre></div>
</div>
<p>On Ubuntu: modify <code class="docutils literal"><span class="pre">/etc/default/libvirt-bin</span></code></p>
<p>Add &#8220;-l&#8221; to the following line</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">libvirtd_opts</span><span class="o">=</span><span class="s2">&quot;-d&quot;</span>
</pre></div>
</div>
<p>so it looks like:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">libvirtd_opts</span><span class="o">=</span><span class="s2">&quot;-d -l&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">In order to have the VNC Console work we have to make sure it will
bind on 0.0.0.0. We do this by editing <code class="docutils literal"><span class="pre">/etc/libvirt/qemu.conf</span></code></p>
<p>Make sure this parameter is set:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">vnc_listen</span> <span class="o">=</span> <span class="s2">&quot;0.0.0.0&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">Restart libvirt</p>
<p>In RHEL or CentOS:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ service libvirtd restart
</pre></div>
</div>
<p>In Ubuntu:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ service libvirt-bin restart
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="configure-the-security-policies">
<h4>Configure the Security Policies<a class="headerlink" href="#configure-the-security-policies" title="Permalink to this headline">¶</a></h4>
<p>CloudStack does various things which can be blocked by security
mechanisms like AppArmor and SELinux. These have to be disabled to
ensure the Agent has all the required permissions.</p>
<ol class="arabic">
<li><p class="first">Configure SELinux (RHEL and CentOS)</p>
<ol class="arabic">
<li><p class="first">Check to see whether SELinux is installed on your machine. If not,
you can skip this section.</p>
<p>In RHEL or CentOS, SELinux is installed and enabled by default.
You can verify this with:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ rpm -qa <span class="p">|</span> grep selinux
</pre></div>
</div>
</li>
<li><p class="first">Set the SELINUX variable in <code class="docutils literal"><span class="pre">/etc/selinux/config</span></code> to
&#8220;permissive&#8221;. This ensures that the permissive setting will be
maintained after a system reboot.</p>
<p>In RHEL or CentOS:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/selinux/config
</pre></div>
</div>
<p>Change the following line</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">SELINUX</span><span class="o">=</span>enforcing
</pre></div>
</div>
<p>to this</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">SELINUX</span><span class="o">=</span>permissive
</pre></div>
</div>
</li>
<li><p class="first">Then set SELinux to permissive starting immediately, without
requiring a system reboot.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ setenforce permissive
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Configure Apparmor (Ubuntu)</p>
<ol class="arabic">
<li><p class="first">Check to see whether AppArmor is installed on your machine. If
not, you can skip this section.</p>
<p>In Ubuntu AppArmor is installed and enabled by default. You can
verify this with:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ dpkg --list <span class="s1">&#39;apparmor&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first">Disable the AppArmor profiles for libvirt</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ln -s /etc/apparmor.d/usr.sbin.libvirtd /etc/apparmor.d/disable/
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ln -s /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper /etc/apparmor.d/disable/
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ apparmor_parser -R /etc/apparmor.d/usr.sbin.libvirtd
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ apparmor_parser -R /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</div>
<div class="section" id="configure-the-network-bridges">
<h4>Configure the network bridges<a class="headerlink" href="#configure-the-network-bridges" title="Permalink to this headline">¶</a></h4>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This is a very important section, please make sure you read this thoroughly.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This section details how to configure bridges using the native
implementation in Linux. Please refer to the next section if you intend to
use OpenVswitch</p>
</div>
<p>In order to forward traffic to your instances you will need at least two
bridges: <em>public</em> and <em>private</em>.</p>
<p>By default these bridges are called <em>cloudbr0</em> and <em>cloudbr1</em>, but you
do have to make sure they are available on each hypervisor.</p>
<p>The most important factor is that you keep the configuration consistent
on all your hypervisors.</p>
<div class="section" id="network-example">
<h5>Network example<a class="headerlink" href="#network-example" title="Permalink to this headline">¶</a></h5>
<p>There are many ways to configure your network. In the Basic networking
mode you should have two (V)LAN&#8217;s, one for your private network and one
for the public network.</p>
<p>We assume that the hypervisor has one NIC (eth0) with three tagged
VLAN&#8217;s:</p>
<ol class="arabic simple">
<li>VLAN 100 for management of the hypervisor</li>
<li>VLAN 200 for public network of the instances (cloudbr0)</li>
<li>VLAN 300 for private network of the instances (cloudbr1)</li>
</ol>
<p>On VLAN 100 we give the Hypervisor the IP-Address 192.168.42.11/24 with
the gateway 192.168.42.1</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The Hypervisor and Management server don&#8217;t have to be in the same subnet!</p>
</div>
</div>
<div class="section" id="configuring-the-network-bridges">
<h5>Configuring the network bridges<a class="headerlink" href="#configuring-the-network-bridges" title="Permalink to this headline">¶</a></h5>
<p>It depends on the distribution you are using how to configure these,
below you&#8217;ll find examples for RHEL/CentOS and Ubuntu.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The goal is to have two bridges called &#8216;cloudbr0&#8217; and &#8216;cloudbr1&#8217; after this
section. This should be used as a guideline only. The exact configuration
will depend on your network layout.</p>
</div>
<div class="section" id="configure-in-rhel-or-centos">
<h6>Configure in RHEL or CentOS<a class="headerlink" href="#configure-in-rhel-or-centos" title="Permalink to this headline">¶</a></h6>
<p>The required packages were installed when libvirt was installed, we can
proceed to configuring the network.</p>
<p>First we configure eth0</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0
</pre></div>
</div>
<p>Make sure it looks similar to:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
</pre></div>
</div>
<p>We now have to configure the three VLAN interfaces:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0.100
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0.100
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
<span class="nv">VLAN</span><span class="o">=</span>yes
<span class="nv">IPADDR</span><span class="o">=</span>192.168.42.11
<span class="nv">GATEWAY</span><span class="o">=</span>192.168.42.1
<span class="nv">NETMASK</span><span class="o">=</span>255.255.255.0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0.200
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0.200
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
<span class="nv">VLAN</span><span class="o">=</span>yes
<span class="nv">BRIDGE</span><span class="o">=</span>cloudbr0
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-eth0.300
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>eth0.300
<span class="nv">HWADDR</span><span class="o">=</span>00:04:xx:xx:xx:xx
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">HOTPLUG</span><span class="o">=</span>no
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">TYPE</span><span class="o">=</span>Ethernet
<span class="nv">VLAN</span><span class="o">=</span>yes
<span class="nv">BRIDGE</span><span class="o">=</span>cloudbr1
</pre></div>
</div>
<p>Now we have the VLAN interfaces configured we can add the bridges on top
of them.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-cloudbr0
</pre></div>
</div>
<p>Now we just configure it is a plain bridge without an IP-Address</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>cloudbr0
<span class="nv">TYPE</span><span class="o">=</span>Bridge
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">IPV6INIT</span><span class="o">=</span>no
<span class="nv">IPV6_AUTOCONF</span><span class="o">=</span>no
<span class="nv">DELAY</span><span class="o">=</span>5
<span class="nv">STP</span><span class="o">=</span>yes
</pre></div>
</div>
<p>We do the same for cloudbr1</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/sysconfig/network-scripts/ifcfg-cloudbr1
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">DEVICE</span><span class="o">=</span>cloudbr1
<span class="nv">TYPE</span><span class="o">=</span>Bridge
<span class="nv">ONBOOT</span><span class="o">=</span>yes
<span class="nv">BOOTPROTO</span><span class="o">=</span>none
<span class="nv">IPV6INIT</span><span class="o">=</span>no
<span class="nv">IPV6_AUTOCONF</span><span class="o">=</span>no
<span class="nv">DELAY</span><span class="o">=</span>5
<span class="nv">STP</span><span class="o">=</span>yes
</pre></div>
</div>
<p>With this configuration you should be able to restart the network,
although a reboot is recommended to see if everything works properly.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Make sure you have an alternative way like IPMI or ILO to reach the machine
in case you made a configuration error and the network stops functioning!</p>
</div>
</div>
<div class="section" id="configure-in-ubuntu">
<h6>Configure in Ubuntu<a class="headerlink" href="#configure-in-ubuntu" title="Permalink to this headline">¶</a></h6>
<p>All the required packages were installed when you installed libvirt, so
we only have to configure the network.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ vi /etc/network/interfaces
</pre></div>
</div>
<p>Modify the interfaces file to look like this:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>auto lo
iface lo inet loopback

<span class="c1"># The primary network interface</span>
auto eth0.100
iface eth0.100 inet static
    address 192.168.42.11
    netmask 255.255.255.240
    gateway 192.168.42.1
    dns-nameservers 8.8.8.8 8.8.4.4
    dns-domain lab.example.org

<span class="c1"># Public network</span>
auto cloudbr0
iface cloudbr0 inet manual
    bridge_ports eth0.200
    bridge_fd 5
    bridge_stp off
    bridge_maxwait 1

<span class="c1"># Private network</span>
auto cloudbr1
iface cloudbr1 inet manual
    bridge_ports eth0.300
    bridge_fd 5
    bridge_stp off
    bridge_maxwait 1
</pre></div>
</div>
<p>With this configuration you should be able to restart the network,
although a reboot is recommended to see if everything works properly.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Make sure you have an alternative way like IPMI or ILO to reach the machine
in case you made a configuration error and the network stops functioning!</p>
</div>
</div>
</div>
</div>
<div class="section" id="configuring-the-firewall">
<h4>Configuring the firewall<a class="headerlink" href="#configuring-the-firewall" title="Permalink to this headline">¶</a></h4>
<p>The hypervisor needs to be able to communicate with other hypervisors
and the management server needs to be able to reach the hypervisor.</p>
<p>In order to do so we have to open the following TCP ports (if you are
using a firewall):</p>
<ol class="arabic simple">
<li>22 (SSH)</li>
<li>1798</li>
<li>16509 (libvirt)</li>
<li>5900 - 6100 (VNC consoles)</li>
<li>49152 - 49216 (libvirt live migration)</li>
</ol>
<p>It depends on the firewall you are using how to open these ports. Below
you&#8217;ll find examples how to open these ports in RHEL/CentOS and Ubuntu.</p>
<div class="section" id="open-ports-in-rhel-centos">
<h5>Open ports in RHEL/CentOS<a class="headerlink" href="#open-ports-in-rhel-centos" title="Permalink to this headline">¶</a></h5>
<p>RHEL and CentOS use iptables for firewalling the system, you can open
extra ports by executing the following iptable commands:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport <span class="m">22</span> -j ACCEPT
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport <span class="m">1798</span> -j ACCEPT
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport <span class="m">16509</span> -j ACCEPT
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport 5900:6100 -j ACCEPT
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables -I INPUT -p tcp -m tcp --dport 49152:49216 -j ACCEPT
</pre></div>
</div>
<p>These iptable settings are not persistent accross reboots, we have to
save them first.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ iptables-save &gt; /etc/sysconfig/iptables
</pre></div>
</div>
</div>
<div class="section" id="open-ports-in-ubuntu">
<h5>Open ports in Ubuntu<a class="headerlink" href="#open-ports-in-ubuntu" title="Permalink to this headline">¶</a></h5>
<p>The default firewall under Ubuntu is UFW (Uncomplicated FireWall), which
is a Python wrapper around iptables.</p>
<p>To open the required ports, execute the following commands:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 22
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 1798
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 16509
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 5900:6100
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ ufw allow proto tcp from any to any port 49152:49216
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">By default UFW is not enabled on Ubuntu. Executing these commands with the
firewall disabled does not enable the firewall.</p>
</div>
</div>
</div>
<div class="section" id="add-the-host-to-cloudstack">
<h4>Add the host to CloudStack<a class="headerlink" href="#add-the-host-to-cloudstack" title="Permalink to this headline">¶</a></h4>
<p>The host is now ready to be added to a cluster. This is covered in a
later section, see <a class="reference internal" href="index.html#adding-a-host"><span>Adding a Host</span></a>. It is
recommended that you continue to read the documentation before adding
the host!</p>
</div>
</div>
<span id="document-hypervisor/vsphere"></span><div class="section" id="host-vmware-vsphere-installation">
<h3>Host VMware vSphere Installation<a class="headerlink" href="#host-vmware-vsphere-installation" title="Permalink to this headline">¶</a></h3>
<p>If you want to use the VMware vSphere hypervisor to run guest virtual
machines, install vSphere on the host(s) in your cloud.</p>
<div class="section" id="system-requirements-for-vsphere-hosts">
<h4>System Requirements for vSphere Hosts<a class="headerlink" href="#system-requirements-for-vsphere-hosts" title="Permalink to this headline">¶</a></h4>
<div class="section" id="software-requirements">
<h5>Software requirements:<a class="headerlink" href="#software-requirements" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first">vSphere and vCenter, versions 4.1, 5.0, 5.1 or 5.5.</p>
<p>vSphere Standard is recommended. Note however that customers need to
consider the CPU constraints in place with vSphere licensing. See
<a class="reference external" href="http://www.vmware.com/files/pdf/vsphere_pricing.pdf">http://www.vmware.com/files/pdf/vsphere_pricing.pdf</a>
and discuss with your VMware sales representative.</p>
<p>vCenter Server Standard is recommended.</p>
</li>
<li><p class="first">Be sure all the hotfixes provided by the hypervisor vendor are
applied. Track the release of hypervisor patches through your
hypervisor vendor&#8217;s support channel, and apply patches as soon as
possible after they are released. CloudStack will not track or notify
you of required hypervisor patches. It is essential that your hosts
are completely up to date with the provided hypervisor patches. The
hypervisor vendor is likely to refuse to support any system that is
not up to date with patches.</p>
</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Apply All Necessary Hotfixes. The lack of up-do-date hotfixes can lead to
data corruption and lost VMs.</p>
</div>
</div>
<div class="section" id="hardware-requirements">
<h5>Hardware requirements:<a class="headerlink" href="#hardware-requirements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>The host must be certified as compatible with vSphere. See the VMware
Hardware Compatibility Guide at
<a class="reference external" href="http://www.vmware.com/resources/compatibility/search.php">http://www.vmware.com/resources/compatibility/search.php</a>.</li>
<li>All hosts must be 64-bit and must support HVM (Intel-VT or AMD-V
enabled).</li>
<li>All hosts within a cluster must be homogenous. That means the CPUs
must be of the same type, count, and feature flags.</li>
<li>64-bit x86 CPU (more cores results in better performance)</li>
<li>Hardware virtualization support required</li>
<li>4 GB of memory</li>
<li>36 GB of local disk</li>
<li>At least 1 NIC</li>
<li>Statically allocated IP Address</li>
</ul>
</div>
<div class="section" id="vcenter-server-requirements">
<h5>vCenter Server requirements:<a class="headerlink" href="#vcenter-server-requirements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Processor - 2 CPUs 2.0GHz or higher Intel or AMD x86 processors.
Processor requirements may be higher if the database runs on the same
machine.</li>
<li>Memory - 3GB RAM. RAM requirements may be higher if your database
runs on the same machine.</li>
<li>Disk storage - 2GB. Disk requirements may be higher if your database
runs on the same machine.</li>
<li>Microsoft SQL Server 2005 Express disk requirements. The bundled
database requires up to 2GB free disk space to decompress the
installation archive.</li>
<li>Networking - 1Gbit or 10Gbit.</li>
</ul>
<p>For more information, see <a class="reference external" href="http://pubs.vmware.com/vsp40/wwhelp/wwhimpl/js/html/wwhelp.htm#href=install/c_vc_hw.html">&#8220;vCenter Server and the vSphere Client Hardware
Requirements&#8221;</a>.</p>
</div>
<div class="section" id="other-requirements">
<h5>Other requirements:<a class="headerlink" href="#other-requirements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>VMware vCenter Standard Edition 4.1, 5.0, 5.1 or 5.5 must be installed and
available to manage the vSphere hosts.</li>
<li>vCenter must be configured to use the standard port 443 so that it
can communicate with the CloudStack Management Server.</li>
<li>You must re-install VMware ESXi if you are going to re-use a host
from a previous install.</li>
<li>CloudStack requires VMware vSphere 4.1, 5.0, 5.1 or 5.5. VMware vSphere 4.0 is
not supported.</li>
<li>All hosts must be 64-bit and must support HVM (Intel-VT or AMD-V
enabled). All hosts within a cluster must be homogeneous. That means
the CPUs must be of the same type, count, and feature flags.</li>
<li>The CloudStack management network must not be configured as a
separate virtual network. The CloudStack management network is the
same as the vCenter management network, and will inherit its
configuration. See <a class="reference internal" href="#configure-vcenter-management-network"><span>Configure vCenter Management Network</span></a>.</li>
<li>CloudStack requires ESXi and vCenter. ESX is not supported.</li>
<li>Ideally all resources used for CloudStack must be used for CloudStack only.
CloudStack should not share instance of ESXi or storage with other
management consoles. Do not share the same storage volumes that will
be used by CloudStack with a different set of ESXi servers that are
not managed by CloudStack.</li>
<li>Put all target ESXi hypervisors in dedicated clusters in a separate Datacenter
in vCenter.</li>
<li>Ideally clusters that will be managed by CloudStack should not contain
any other VMs. Do not run the management server or vCenter on
the cluster that is designated for CloudStack use. Create a separate
cluster for use of CloudStack and make sure that they are no VMs in
this cluster.</li>
<li>All of the required VLANs must be trunked into all network switches that
are connected to the ESXi hypervisor hosts. These would include the
VLANs for Management, Storage, vMotion, and guest VLANs. The guest
VLAN (used in Advanced Networking; see Network Setup) is a contiguous
range of VLANs that will be managed by CloudStack.</li>
</ul>
</div>
</div>
<div class="section" id="preparation-checklist-for-vmware">
<h4>Preparation Checklist for VMware<a class="headerlink" href="#preparation-checklist-for-vmware" title="Permalink to this headline">¶</a></h4>
<p>For a smoother installation, gather the following information before you
start:</p>
<ul class="simple">
<li>Information listed in <a class="reference internal" href="#vcenter-checklist"><span>vCenter Checklist</span></a></li>
<li>Information listed in <a class="reference internal" href="#networking-checklist-for-vmware"><span>Networking Checklist for VMware</span></a></li>
</ul>
<div class="section" id="vcenter-checklist">
<span id="id1"></span><h5>vCenter Checklist<a class="headerlink" href="#vcenter-checklist" title="Permalink to this headline">¶</a></h5>
<p>You will need the following information about vCenter.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="39%" />
<col width="61%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">vCenter Requirement</th>
<th class="head">Notes</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>vCenter User</td>
<td>This user must have admin privileges.</td>
</tr>
<tr class="row-odd"><td>vCenter User Password</td>
<td>Password for the above user.</td>
</tr>
<tr class="row-even"><td>vCenter Datacenter Name</td>
<td>Name of the datacenter.</td>
</tr>
<tr class="row-odd"><td>vCenter Cluster Name</td>
<td>Name of the cluster.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="networking-checklist-for-vmware">
<span id="id2"></span><h5>Networking Checklist for VMware<a class="headerlink" href="#networking-checklist-for-vmware" title="Permalink to this headline">¶</a></h5>
<p>You will need the following information about your VLANs.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="24%" />
<col width="76%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">VLAN Information</th>
<th class="head">Notes</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ESXi VLAN</td>
<td>VLAN on which all your ESXi hypervisors reside.</td>
</tr>
<tr class="row-odd"><td>ESXI VLAN IP Address</td>
<td>IP Address Range in the ESXi VLAN. One address per Virtual Router is used from this range.</td>
</tr>
<tr class="row-even"><td>ESXi VLAN IP Gateway</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>ESXi VLAN Netmask</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>Management Server VLAN</td>
<td>VLAN on which the CloudStack Management server is installed.</td>
</tr>
<tr class="row-odd"><td>Public VLAN</td>
<td>VLAN for the Public Network.</td>
</tr>
<tr class="row-even"><td>Public VLAN Gateway</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>Public VLAN Netmask</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>Public VLAN IP Address Range</td>
<td>Range of Public IP Addresses available for CloudStack use. These
addresses will be used for virtual router on CloudStack to route private
traffic to external networks.</td>
</tr>
<tr class="row-odd"><td>VLAN Range for Customer use</td>
<td>A contiguous range of non-routable VLANs. One VLAN will be assigned for
each customer.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="vsphere-installation-steps">
<h4>vSphere Installation Steps<a class="headerlink" href="#vsphere-installation-steps" title="Permalink to this headline">¶</a></h4>
<ol class="arabic">
<li><p class="first">If you haven&#8217;t already, you&#8217;ll need to download and purchase vSphere
from the VMware Website
(<a class="reference external" href="https://www.vmware.com/tryvmware/index.php?p=vmware-vsphere&amp;lp=1">https://www.vmware.com/tryvmware/index.php?p=vmware-vsphere&amp;lp=1</a>)
and install it by following the VMware vSphere Installation Guide.</p>
</li>
<li><p class="first">Following installation, perform the following configuration, which
are described in the next few sections:</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="84%" />
<col width="16%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Required</th>
<th class="head">Optional</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ESXi host setup</td>
<td>NIC bonding</td>
</tr>
<tr class="row-odd"><td>Configure host physical networking,virtual switch, vCenter Management Network, and extended port range</td>
<td>Multipath storage</td>
</tr>
<tr class="row-even"><td>Prepare storage for iSCSI</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>Configure clusters in vCenter and add hosts to them, or add hosts without clusters to vCenter</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
</li>
</ol>
</div>
<div class="section" id="esxi-host-setup">
<h4>ESXi Host setup<a class="headerlink" href="#esxi-host-setup" title="Permalink to this headline">¶</a></h4>
<p>All ESXi hosts should have CPU hardware virtualization support enabled in
the BIOS. Please note hardware virtualization support is not enabled by
default on most servers.</p>
</div>
<div class="section" id="physical-host-networking">
<h4>Physical Host Networking<a class="headerlink" href="#physical-host-networking" title="Permalink to this headline">¶</a></h4>
<p>You should have a plan for cabling the vSphere hosts. Proper network
configuration is required before adding a vSphere host to CloudStack. To
configure an ESXi host, you can use vClient to add it as standalone host
to vCenter first. Once you see the host appearing in the vCenter
inventory tree, click the host node in the inventory tree, and navigate
to the Configuration tab.</p>
<p><img alt="vspherephysicalnetwork.png: vSphere client" src="_images/vmware-physical-network.png" /></p>
<p>In the host configuration tab, click the &#8220;Hardware/Networking&#8221; link to
bring up the networking configuration page as above.</p>
<div class="section" id="configure-virtual-switch">
<h5>Configure Virtual Switch<a class="headerlink" href="#configure-virtual-switch" title="Permalink to this headline">¶</a></h5>
<p>During the initial installation of an ESXi host a default virtual switch
vSwitch0 is created. You may need to create additional vSwiches depending
on your required architecture. CloudStack requires all ESXi hosts in the cloud
to use consistently named virtual switches. If
you change the default virtual switch name, you will need to configure
one or more CloudStack configuration variables as well.</p>
<div class="section" id="separating-traffic">
<h6>Separating Traffic<a class="headerlink" href="#separating-traffic" title="Permalink to this headline">¶</a></h6>
<p>CloudStack allows you to configure three separate networks per ESXi host.
CloudStack identifies these networks by the name of the vSwitch
they are connected to. The networks for configuration are public (for
traffic to/from the public internet), guest (for guest-guest traffic),
and private (for management and usually storage traffic). You can use
the default virtual switch for all three, or create one or two other
vSwitches for those traffic types.</p>
<p>If you want to separate traffic in this way you should first create and
configure vSwitches in vCenter according to the vCenter instructions.
Take note of the vSwitch names you have used for each traffic type. You
will configure CloudStack to use these vSwitches.</p>
</div>
<div class="section" id="increasing-ports">
<h6>Increasing Ports<a class="headerlink" href="#increasing-ports" title="Permalink to this headline">¶</a></h6>
<p>By default a virtual switch on ESXi hosts is created with 56 ports. We
recommend setting it to 4088, the maximum number of ports allowed. To do
that, click the &#8220;Properties...&#8221; link for virtual switch (note this is
not the Properties link for Networking).</p>
<p><img alt="vsphereincreaseports.png: vSphere client" src="_images/vmware-increase-ports.png" /></p>
<p>In vSwitch properties dialog, select the vSwitch and click Edit. You
should see the following dialog:</p>
<p><img alt="vspherevswitchproperties.png: vSphere client" src="_images/vmware-vswitch-properties.png" /></p>
<p>In this dialog, you can change the number of switch ports. After you&#8217;ve
done that, ESXi hosts are required to reboot in order for the setting to
take effect.</p>
</div>
</div>
<div class="section" id="configure-vcenter-management-network">
<span id="id3"></span><h5>Configure vCenter Management Network<a class="headerlink" href="#configure-vcenter-management-network" title="Permalink to this headline">¶</a></h5>
<p>In the vSwitch properties dialog box, you may see a vCenter management
network. This same network will also be used as the CloudStack
management network. CloudStack requires the vCenter management network
to be configured properly. Select the management network item in the
dialog, then click Edit.</p>
<p><img alt="vspheremgtnetwork.png: vSphere client" src="_images/vmware-mgt-network-properties.png" /></p>
<p>Make sure the following values are set:</p>
<ul class="simple">
<li>VLAN ID set to the desired ID</li>
<li>vMotion enabled.</li>
<li>Management traffic enabled.</li>
</ul>
<p>If the ESXi hosts have multiple VMKernel ports, and ESXi is not using
the default value &#8220;Management Network&#8221; as the management network name,
you must follow these guidelines to configure the management network
port group so that CloudStack can find it:</p>
<ul class="simple">
<li>Use one label for the management network port across all ESXi hosts.</li>
<li>In the CloudStack UI, go to Configuration - Global Settings and set
vmware.management.portgroup to the management network label from the
ESXi hosts.</li>
</ul>
</div>
<div class="section" id="extend-port-range-for-cloudstack-console-proxy">
<h5>Extend Port Range for CloudStack Console Proxy<a class="headerlink" href="#extend-port-range-for-cloudstack-console-proxy" title="Permalink to this headline">¶</a></h5>
<p>(Applies only to VMware vSphere version 4.x)</p>
<p>You need to extend the range of firewall ports that the console proxy
works with on the hosts. This is to enable the console proxy to work
with VMware-based VMs. The default additional port range is 59000-60000.
To extend the port range, log in to the VMware ESX service console on
each host and run the following commands:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>esxcfg-firewall -o 59000-60000,tcp,in,vncextras
esxcfg-firewall -o 59000-60000,tcp,out,vncextras
</pre></div>
</div>
</div>
<div class="section" id="configure-nic-bonding-for-vsphere">
<h5>Configure NIC Bonding for vSphere<a class="headerlink" href="#configure-nic-bonding-for-vsphere" title="Permalink to this headline">¶</a></h5>
<p>NIC bonding on vSphere hosts may be done according to the vSphere
installation guide.</p>
</div>
</div>
<div class="section" id="configuring-a-vsphere-cluster-with-nexus-1000v-virtual-switch">
<h4>Configuring a vSphere Cluster with Nexus 1000v Virtual Switch<a class="headerlink" href="#configuring-a-vsphere-cluster-with-nexus-1000v-virtual-switch" title="Permalink to this headline">¶</a></h4>
<p>CloudStack supports Cisco Nexus 1000v dvSwitch (Distributed Virtual
Switch) for virtual network configuration in a VMware vSphere
environment. This section helps you configure a vSphere cluster with
Nexus 1000v virtual switch in a VMware vCenter environment. For
information on creating a vSphere cluster, see
<a class="reference external" href="#vmware-vsphere-installation-and-configuration">&#8220;VMware vSphere Installation and Configuration&#8221;</a></p>
<div class="section" id="about-cisco-nexus-1000v-distributed-virtual-switch">
<h5>About Cisco Nexus 1000v Distributed Virtual Switch<a class="headerlink" href="#about-cisco-nexus-1000v-distributed-virtual-switch" title="Permalink to this headline">¶</a></h5>
<p>The Cisco Nexus 1000V virtual switch is a software-based virtual machine
access switch for VMware vSphere environments. It can span multiple
hosts running VMware ESXi 4.0 and later. A Nexus virtual switch consists
of two components: the Virtual Supervisor Module (VSM) and the Virtual
Ethernet Module (VEM). The VSM is a virtual appliance that acts as the
switch&#8217;s supervisor. It controls multiple VEMs as a single network
device. The VSM is installed independent of the VEM and is deployed in
redundancy mode as pairs or as a standalone appliance. The VEM is
installed on each VMware ESXi server to provide packet-forwarding
capability. It provides each virtual machine with dedicated switch
ports. This VSM-VEM architecture is analogous to a physical Cisco
switch&#8217;s supervisor (standalone or configured in high-availability mode)
and multiple linecards architecture.</p>
<p>Nexus 1000v switch uses vEthernet port profiles to simplify network
provisioning for virtual machines. There are two types of port profiles:
Ethernet port profile and vEthernet port profile. The Ethernet port
profile is applied to the physical uplink ports-the NIC ports of the
physical NIC adapter on an ESXi server. The vEthernet port profile is
associated with the virtual NIC (vNIC) that is plumbed on a guest VM on
the ESXi server. The port profiles help the network administrators
define network policies which can be reused for new virtual machines.
The Ethernet port profiles are created on the VSM and are represented as
port groups on the vCenter server.</p>
</div>
<div class="section" id="prerequisites-and-guidelines">
<h5>Prerequisites and Guidelines<a class="headerlink" href="#prerequisites-and-guidelines" title="Permalink to this headline">¶</a></h5>
<p>This section discusses prerequisites and guidelines for using Nexus
virtual switch in CloudStack. Before configuring Nexus virtual switch,
ensure that your system meets the following requirements:</p>
<ul class="simple">
<li>A cluster of servers (ESXi 4.1 or later) is configured in the
vCenter.</li>
<li>Each cluster managed by CloudStack is the only cluster in its vCenter
datacenter.</li>
<li>A Cisco Nexus 1000v virtual switch is installed to serve the
datacenter that contains the vCenter cluster. This ensures that
CloudStack doesn&#8217;t have to deal with dynamic migration of virtual
adapters or networks across other existing virtual switches. See
<a class="reference external" href="http://www.cisco.com/en/US/docs/switches/datacenter/nexus1000/sw/4_2_1_s_v_1_5_1/install_upgrade/vsm_vem/guide/n1000v_installupgrade.html">Cisco Nexus 1000V Installation and Upgrade Guide</a>
for guidelines on how to install the Nexus 1000v VSM and VEM modules.</li>
<li>The Nexus 1000v VSM is not deployed on a vSphere host that is managed
by CloudStack.</li>
<li>When the maximum number of VEM modules per VSM instance is reached,
an additional VSM instance is created before introducing any more
ESXi hosts. The limit is 64 VEM modules for each VSM instance.</li>
<li>CloudStack expects that the Management Network of the ESXi host is
configured on the standard vSwitch and searches for it in the
standard vSwitch. Therefore, ensure that you do not migrate the
management network to Nexus 1000v virtual switch during
configuration.</li>
<li>All information given in <a class="reference internal" href="#nexus-vswift-preconf"><span>Nexus 1000v Virtual Switch Preconfiguration</span></a></li>
</ul>
</div>
<div class="section" id="nexus-1000v-virtual-switch-preconfiguration">
<span id="nexus-vswift-preconf"></span><h5>Nexus 1000v Virtual Switch Preconfiguration<a class="headerlink" href="#nexus-1000v-virtual-switch-preconfiguration" title="Permalink to this headline">¶</a></h5>
<div class="section" id="preparation-checklist">
<h6>Preparation Checklist<a class="headerlink" href="#preparation-checklist" title="Permalink to this headline">¶</a></h6>
<p>For a smoother configuration of Nexus 1000v switch, gather the following
information before you start:</p>
<ul class="simple">
<li>vCenter credentials</li>
<li>Nexus 1000v VSM IP address</li>
<li>Nexus 1000v VSM Credentials</li>
<li>Ethernet port profile names</li>
</ul>
</div>
<div class="section" id="vcenter-credentials-checklist">
<h6>vCenter Credentials Checklist<a class="headerlink" href="#vcenter-credentials-checklist" title="Permalink to this headline">¶</a></h6>
<p>You will need the following information about vCenter:</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="25%" />
<col width="8%" />
<col width="67%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Nexus vSwitch Requirements</th>
<th class="head">Value</th>
<th class="head">Notes</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>vCenter IP</td>
<td>&nbsp;</td>
<td>The IP address of the vCenter.</td>
</tr>
<tr class="row-odd"><td>Secure HTTP Port Number</td>
<td>443</td>
<td>Port 443 is configured by default; however, you can change the port if needed.</td>
</tr>
<tr class="row-even"><td>vCenter User ID</td>
<td>&nbsp;</td>
<td>The vCenter user with administrator-level privileges. The vCenter User ID is
required when you configure the virtual switch in CloudStack.</td>
</tr>
<tr class="row-odd"><td>vCenter Password</td>
<td>&nbsp;</td>
<td>The password for the vCenter user specified above. The password for this
vCenter user is required when you configure the switch in CloudStack.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="network-configuration-checklist">
<h6>Network Configuration Checklist<a class="headerlink" href="#network-configuration-checklist" title="Permalink to this headline">¶</a></h6>
<p>The following information specified in the Nexus Configure Networking
screen is displayed in the Details tab of the Nexus dvSwitch in the
CloudStack UI:</p>
<p><strong>Control Port Group VLAN ID</strong>
The VLAN ID of the Control Port Group. The control VLAN is used for
communication between the VSM and the VEMs.</p>
<p><strong>Management Port Group VLAN ID</strong>
The VLAN ID of the Management Port Group. The management VLAN corresponds to
the mgmt0 interface that is used to establish and maintain the connection
between the VSM and VMware vCenter Server.</p>
<p><strong>Packet Port Group VLAN ID</strong>
The VLAN ID of the Packet Port Group. The packet VLAN forwards relevant data
packets from the VEMs to the VSM.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The VLANs used for control, packet, and management port groups can be the
same.</p>
</div>
<p>For more information, see <a class="reference external" href="http://www.cisco.com/en/US/docs/switches/datacenter/nexus1000/sw/4_2_1_s_v_1_4_b/getting_started/configuration/guide/n1000v_gsg.pdf">Cisco Nexus 1000V Getting Started Guide</a>.</p>
</div>
<div class="section" id="vsm-configuration-checklist">
<h6>VSM Configuration Checklist<a class="headerlink" href="#vsm-configuration-checklist" title="Permalink to this headline">¶</a></h6>
<p>You will need the following VSM configuration parameters:</p>
<p><strong>Admin Name and Password</strong>
The admin name and password to connect to the VSM appliance. You must specify
these credentials while configuring Nexus virtual switch.</p>
<p><strong>Management IP Address</strong>
This is the IP address of the VSM appliance. This is the IP address you
specify in the virtual switch IP Address field while configuting Nexus virtual
switch.</p>
<p><strong>SSL</strong>
Should be set to Enable.Always enable SSL. SSH is usually enabled by default
during the VSM installation. However, check whether the SSH connection to the
VSM is working, without which CloudStack failes to connect to the VSM.</p>
</div>
<div class="section" id="creating-a-port-profile">
<h6>Creating a Port Profile<a class="headerlink" href="#creating-a-port-profile" title="Permalink to this headline">¶</a></h6>
<ul class="simple">
<li>Whether you create a Basic or Advanced zone configuration, ensure
that you always create an Ethernet port profile on the VSM after you
install it and before you create the zone.<ul>
<li>The Ethernet port profile created to represent the physical
network or networks used by an Advanced zone configuration trunk
all the VLANs including guest VLANs, the VLANs that serve the
native VLAN, and the packet/control/data/management VLANs of the
VSM.</li>
<li>The Ethernet port profile created for a Basic zone configuration
does not trunk the guest VLANs because the guest VMs do not get
their own VLANs provisioned on their network interfaces in a Basic
zone.</li>
</ul>
</li>
<li>An Ethernet port profile configured on the Nexus 1000v virtual switch
should not use in its set of system VLANs, or any of the VLANs
configured or intended to be configured for use towards VMs or VM
resources in the CloudStack environment.</li>
<li>You do not have to create any vEthernet port profiles – CloudStack
does that during VM deployment.</li>
<li>Ensure that you create required port profiles to be used by
CloudStack for different traffic types of CloudStack, such as
Management traffic, Guest traffic, Storage traffic, and Public
traffic. The physical networks configured during zone creation should
have a one-to-one relation with the Ethernet port profiles.</li>
</ul>
<p><img alt="vmwarenexusportprofile.png: vSphere client" src="_images/vmware-nexus-port-profile.png" /></p>
<p>For information on creating a port profile, see <a class="reference external" href="http://www.cisco.com/en/US/docs/switches/datacenter/nexus1000/sw/4_2_1_s_v_1_4_a/port_profile/configuration/guide/n1000v_port_profile.html">Cisco Nexus 1000V Port
Profile Configuration Guide</a>.</p>
</div>
<div class="section" id="assigning-physical-nic-adapters">
<h6>Assigning Physical NIC Adapters<a class="headerlink" href="#assigning-physical-nic-adapters" title="Permalink to this headline">¶</a></h6>
<p>Assign ESXi host&#8217;s physical NIC adapters, which correspond to each
physical network, to the port profiles. In each ESXi host that is part
of the vCenter cluster, observe the physical networks assigned to each
port profile and note down the names of the port profile for future use.
This mapping information helps you when configuring physical networks
during the zone configuration on CloudStack. These Ethernet port profile
names are later specified as VMware Traffic Labels for different traffic
types when configuring physical networks during the zone configuration.
For more information on configuring physical networks, see
<a class="reference external" href="#configuring-a-vsphere-cluster-with-nexus-1000v-virtual-switch">&#8220;Configuring a vSphere Cluster with Nexus 1000v Virtual Switch&#8221;</a>.</p>
</div>
<div class="section" id="adding-vlan-ranges">
<h6>Adding VLAN Ranges<a class="headerlink" href="#adding-vlan-ranges" title="Permalink to this headline">¶</a></h6>
<p>Determine the public VLAN, System VLAN, and Guest VLANs to be used by
the CloudStack. Ensure that you add them to the port profile database.
Corresponding to each physical network, add the VLAN range to port
profiles. In the VSM command prompt, run the switchport trunk allowed
vlan&lt;range&gt; command to add the VLAN ranges to the port profile.</p>
<p>For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>switchport trunk allowed vlan 1,140-147,196-203
</pre></div>
</div>
<p>In this example, the allowed VLANs added are 1, 140-147, and 196-203</p>
<p>You must also add all the public and private VLANs or VLAN ranges to the
switch. This range is the VLAN range you specify in your zone.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Before you run the vlan command, ensure that the configuration mode is
enabled in Nexus 1000v virtual switch.</p>
</div>
<p>For example:</p>
<p>If you want the VLAN 200 to be used on the switch, run the following
command:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vlan 200
</pre></div>
</div>
<p>If you want the VLAN range 1350-1750 to be used on the switch, run the
following command:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vlan 1350-1750
</pre></div>
</div>
<p>Refer to Cisco Nexus 1000V Command Reference of specific product
version.</p>
</div>
</div>
<div class="section" id="enabling-nexus-virtual-switch-in-cloudstack">
<h5>Enabling Nexus Virtual Switch in CloudStack<a class="headerlink" href="#enabling-nexus-virtual-switch-in-cloudstack" title="Permalink to this headline">¶</a></h5>
<p>To make a CloudStack deployment Nexus enabled, you must set the
vmware.use.nexus.vswitch parameter true by using the Global Settings
page in the CloudStack UI. Unless this parameter is set to &#8220;true&#8221; and
restart the management server, you cannot see any UI options specific to
Nexus virtual switch, and CloudStack ignores the Nexus virtual switch
specific parameters specified in the AddTrafficTypeCmd,
UpdateTrafficTypeCmd, and AddClusterCmd API calls.</p>
<p>Unless the CloudStack global parameter &#8220;vmware.use.nexus.vswitch&#8221; is set
to &#8220;true&#8221;, CloudStack by default uses VMware standard vSwitch for
virtual network infrastructure. In this release, CloudStack doesn’t
support configuring virtual networks in a deployment with a mix of
standard vSwitch and Nexus 1000v virtual switch. The deployment can have
either standard vSwitch or Nexus 1000v virtual switch.</p>
</div>
<div class="section" id="configuring-nexus-1000v-virtual-switch-in-cloudstack">
<h5>Configuring Nexus 1000v Virtual Switch in CloudStack<a class="headerlink" href="#configuring-nexus-1000v-virtual-switch-in-cloudstack" title="Permalink to this headline">¶</a></h5>
<p>You can configure Nexus dvSwitch by adding the necessary resources while
the zone is being created.</p>
<p><img alt="vmwarenexusaddcluster.png: vmware nexus add cluster" src="_images/vmware-nexus-add-cluster.png" /></p>
<p>After the zone is created, if you want to create an additional cluster
along with Nexus 1000v virtual switch in the existing zone, use the Add
Cluster option. For information on creating a cluster, see
<a class="reference external" href="configuration.html#add-cluster-vsphere">&#8220;Add Cluster: vSphere&#8221;</a>.</p>
<p>In both these cases, you must specify the following parameters to
configure Nexus virtual switch:</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Parameters</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Cluster Name</td>
<td>Enter the name of the cluster you created in vCenter. For example,&#8221;cloud.cluster&#8221;.</td>
</tr>
<tr class="row-odd"><td>vCenter Host</td>
<td>Enter the host name or the IP address of the vCenter host where you have deployed the Nexus virtual switch.</td>
</tr>
<tr class="row-even"><td>vCenter User name</td>
<td>Enter the username that CloudStack should use to connect to vCenter. This user must have all administrative privileges.</td>
</tr>
<tr class="row-odd"><td>vCenter Password</td>
<td>Enter the password for the user named above.</td>
</tr>
<tr class="row-even"><td>vCenter Datacenter</td>
<td>Enter the vCenter datacenter that the cluster is in. For example, &#8220;cloud.dc.VM&#8221;.</td>
</tr>
<tr class="row-odd"><td>Nexus dvSwitch IP Address</td>
<td>The IP address of the VSM component of the Nexus 1000v virtual switch.</td>
</tr>
<tr class="row-even"><td>Nexus dvSwitch Username</td>
<td>The admin name to connect to the VSM appliance.</td>
</tr>
<tr class="row-odd"><td>Nexus dvSwitch Password</td>
<td>The corresponding password for the admin user specified above.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="removing-nexus-virtual-switch">
<h5>Removing Nexus Virtual Switch<a class="headerlink" href="#removing-nexus-virtual-switch" title="Permalink to this headline">¶</a></h5>
<ol class="arabic">
<li><p class="first">In the vCenter datacenter that is served by the Nexus virtual switch,
ensure that you delete all the hosts in the corresponding cluster.</p>
</li>
<li><p class="first">Log in with Admin permissions to the CloudStack administrator UI.</p>
</li>
<li><p class="first">In the left navigation bar, select Infrastructure.</p>
</li>
<li><p class="first">In the Infrastructure page, click View all under Clusters.</p>
</li>
<li><p class="first">Select the cluster where you want to remove the virtual switch.</p>
</li>
<li><p class="first">In the dvSwitch tab, click the name of the virtual switch.</p>
</li>
<li><p class="first">In the Details page, click Delete Nexus dvSwitch icon.
<img alt="DeleteButton.png: button to delete dvSwitch" src="_images/delete-button.png" /></p>
<p>Click Yes in the confirmation dialog box.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="configuring-a-vmware-datacenter-with-vmware-distributed-virtual-switch">
<h4>Configuring a VMware Datacenter with VMware Distributed Virtual Switch<a class="headerlink" href="#configuring-a-vmware-datacenter-with-vmware-distributed-virtual-switch" title="Permalink to this headline">¶</a></h4>
<p>CloudStack supports VMware vNetwork Distributed Switch (VDS) for virtual
network configuration in a VMware vSphere environment. This section
helps you configure VMware VDS in a CloudStack deployment. Each vCenter
server instance can support up to 128 VDS instances and each VDS
instance can manage up to 500 VMware hosts.</p>
<div class="section" id="about-vmware-distributed-virtual-switch">
<h5>About VMware Distributed Virtual Switch<a class="headerlink" href="#about-vmware-distributed-virtual-switch" title="Permalink to this headline">¶</a></h5>
<p>VMware VDS is an aggregation of host-level virtual switches on a VMware
vCenter server. VDS abstracts the configuration of individual virtual
switches that span across a large number of hosts, and enables
centralized provisioning, administration, and monitoring for your entire
datacenter from a centralized interface. In effect, a VDS acts as a
single virtual switch at the datacenter level and manages networking for
a number of hosts in a datacenter from a centralized VMware vCenter
server. Each VDS maintains network runtime state for VMs as they move
across multiple hosts, enabling inline monitoring and centralized
firewall services. A VDS can be deployed with or without Virtual
Standard Switch and a Nexus 1000V virtual switch.</p>
</div>
<div class="section" id="id5">
<h5>Prerequisites and Guidelines<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>VMware VDS is supported only on Public and Guest traffic in
CloudStack.</li>
<li>VMware VDS does not support multiple VDS per traffic type. If a user
has many VDS switches, only one can be used for Guest traffic and
another one for Public traffic.</li>
<li>Additional switches of any type can be added for each cluster in the
same zone. While adding the clusters with different switch type,
traffic labels is overridden at the cluster level.</li>
<li>Management and Storage network does not support VDS. Therefore, use
Standard Switch for these networks.</li>
<li>When you remove a guest network, the corresponding dvportgroup will
not be removed on the vCenter. You must manually delete them on the
vCenter.</li>
</ul>
</div>
<div class="section" id="id6">
<h5>Preparation Checklist<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h5>
<p>For a smoother configuration of VMware VDS, note down the VDS name you
have added in the datacenter before you start:</p>
<p><img alt="vds-name.png: Name of the dvSwitch as specified in the vCenter." src="_images/vds-name.png" /></p>
<p>Use this VDS name in the following:</p>
<ul>
<li><p class="first">The switch name in the Edit traffic label dialog while configuring a
public and guest traffic during zone creation.</p>
<p>During a zone creation, ensure that you select VMware vNetwork
Distributed Virtual Switch when you configure guest and public
traffic type.</p>
<p><img alt="traffic-type.png" src="_images/traffic-type.png" /></p>
</li>
<li><p class="first">The Public Traffic vSwitch Type field when you add a VMware
VDS-enabled cluster.</p>
</li>
<li><p class="first">The switch name in the traffic label while updating the switch type
in a zone.</p>
</li>
</ul>
<p>Traffic label format in the last case is
[[&#8220;Name of vSwitch/dvSwitch/EthernetPortProfile&#8221;][,&#8221;VLAN ID&#8221;[,&#8221;vSwitch Type&#8221;]]]</p>
<p>The possible values for traffic labels are:</p>
<ul class="simple">
<li>empty string</li>
<li>dvSwitch0</li>
<li>dvSwitch0,200</li>
<li>dvSwitch1,300,vmwaredvs</li>
<li>myEthernetPortProfile,,nexusdvs</li>
<li>dvSwitch0,,vmwaredvs</li>
</ul>
<p>The three fields to fill in are:</p>
<ul>
<li><p class="first">Name of the virtual / distributed virtual switch at vCenter.</p>
<p>The default value depends on the type of virtual switch:</p>
<p><strong>vSwitch0</strong>: If type of virtual switch is VMware vNetwork Standard virtual
switch</p>
<p><strong>dvSwitch0</strong>: If type of virtual switch is VMware vNetwork Distributed
virtual switch</p>
<p><strong>epp0</strong>: If type of virtual switch is Cisco Nexus 1000v Distributed
virtual switch</p>
</li>
<li><p class="first">VLAN ID to be used for this traffic wherever applicable.</p>
<p>This field would be used for only public traffic as of now. In case of
guest traffic this field would be ignored and could be left empty for guest
traffic. By default empty string would be assumed which translates to
untagged VLAN for that specific traffic type.</p>
</li>
<li><p class="first">Type of virtual switch. Specified as string.</p>
<p>Possible valid values are vmwaredvs, vmwaresvs, nexusdvs.</p>
<p><strong>vmwaresvs</strong>: Represents VMware vNetwork Standard virtual switch</p>
<p><strong>vmwaredvs</strong>: Represents VMware vNetwork distributed virtual switch</p>
<p><strong>nexusdvs</strong>: Represents Cisco Nexus 1000v distributed virtual switch.</p>
<p>If nothing specified (left empty), zone-level default virtual switchwould
be defaulted, based on the value of global parameter you specify.</p>
<p>Following are the global configuration parameters:</p>
<p><strong>vmware.use.dvswitch</strong>: Set to true to enable any kind (VMware DVS and
Cisco Nexus 1000v) of distributed virtual switch in a CloudStack
deployment. If set to false, the virtual switch that can be used in that
CloudStack deployment is Standard virtual switch.</p>
<p><strong>vmware.use.nexus.vswitch</strong>: This parameter is ignored if
vmware.use.dvswitch is set to false. Set to true to enable Cisco Nexus
1000v distributed virtual switch in a CloudStack deployment.</p>
</li>
</ul>
</div>
<div class="section" id="enabling-virtual-distributed-switch-in-cloudstack">
<h5>Enabling Virtual Distributed Switch in CloudStack<a class="headerlink" href="#enabling-virtual-distributed-switch-in-cloudstack" title="Permalink to this headline">¶</a></h5>
<p>To make a CloudStack deployment VDS enabled, set the vmware.use.dvswitch
parameter to true by using the Global Settings page in the CloudStack UI
and restart the Management Server. Unless you enable the
vmware.use.dvswitch parameter, you cannot see any UI options specific to
VDS, and CloudStack ignores the VDS-specific parameters that you
specify. Additionally, CloudStack uses VDS for virtual network
infrastructure if the value of vmware.use.dvswitch parameter is true and
the value of vmware.use.nexus.dvswitch parameter is false. Another
global parameter that defines VDS configuration is
vmware.ports.per.dvportgroup. This is the default number of ports per
VMware dvPortGroup in a VMware environment. Default value is 256. This
number directly associated with the number of guest network you can
create.</p>
<p>CloudStack supports orchestration of virtual networks in a deployment
with a mix of Virtual Distributed Switch, Standard Virtual Switch and
Nexus 1000v Virtual Switch.</p>
</div>
<div class="section" id="configuring-distributed-virtual-switch-in-cloudstack">
<h5>Configuring Distributed Virtual Switch in CloudStack<a class="headerlink" href="#configuring-distributed-virtual-switch-in-cloudstack" title="Permalink to this headline">¶</a></h5>
<p>You can configure VDS by adding the necessary resources while a zone is
created.</p>
<p>Alternatively, at the cluster level, you can create an additional
cluster with VDS enabled in the existing zone. Use the Add Cluster
option. For information as given in <a class="reference external" href="configuration.html#add-cluster-vsphere">“Add Cluster: vSphere”</a>.</p>
<p>In both these cases, you must specify the following parameters to
configure VDS:</p>
<p><img alt="dvSwitchConfig.png: Configuring dvSwitch" src="_images/dvswitchconfig.png" /></p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Parameters Description</th>
<th class="head">&nbsp;</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Cluster Name</td>
<td>Enter the name of the cluster you created in vCenter. For example, &#8220;cloudcluster&#8221;.</td>
</tr>
<tr class="row-odd"><td>vCenter Host</td>
<td>Enter the name or the IP address of the vCenter host where you have deployed the VMware VDS.</td>
</tr>
<tr class="row-even"><td>vCenter User name</td>
<td>Enter the username that CloudStack should use to connect to vCenter. This user must have all administrative privileges.</td>
</tr>
<tr class="row-odd"><td>vCenter Password</td>
<td>Enter the password for the user named above.</td>
</tr>
<tr class="row-even"><td>vCenter Datacenter</td>
<td>Enter the vCenter datacenter that the cluster is in. For example, &#8220;clouddcVM&#8221;.</td>
</tr>
<tr class="row-odd"><td>Override Public Traffic</td>
<td>Enable this option to override the zone-wide public traffic for the cluster you are creating.</td>
</tr>
<tr class="row-even"><td>Public Traffic vSwitch Type</td>
<td>This option is displayed only if you enable the Override Public Traffic option. Select VMware vNetwork Distributed Virtual Switch. If the vmware.use.dvswitch global parameter is true, the default option will be VMware vNetwork Distributed Virtual Switch.</td>
</tr>
<tr class="row-odd"><td>Public Traffic vSwitch Name</td>
<td>Name of virtual switch to be used for the public traffic.</td>
</tr>
<tr class="row-even"><td>Override Guest Traffic</td>
<td>Enable the option to override the zone-wide guest traffic for the cluster you are creating.</td>
</tr>
<tr class="row-odd"><td>Guest Traffic vSwitch Type</td>
<td>This option is displayed only if you enable the Override Guest Traffic option. Select VMware vNetwork Distributed Virtual Switch. If the vmware.use.dvswitch global parameter is true, the default option will be VMware vNetwork Distributed Virtual Switch.</td>
</tr>
<tr class="row-even"><td>Guest Traffic vSwitch Name</td>
<td>Name of virtual switch to be used for guest traffic.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="storage-preparation-for-vsphere-iscsi-only">
<h4>Storage Preparation for vSphere (iSCSI only)<a class="headerlink" href="#storage-preparation-for-vsphere-iscsi-only" title="Permalink to this headline">¶</a></h4>
<p>Use of iSCSI requires preparatory work in vCenter. You must add an iSCSI
target and create an iSCSI datastore.</p>
<p>If you are using NFS, skip this section.</p>
<div class="section" id="enable-iscsi-initiator-for-esxi-hosts">
<h5>Enable iSCSI initiator for ESXi hosts<a class="headerlink" href="#enable-iscsi-initiator-for-esxi-hosts" title="Permalink to this headline">¶</a></h5>
<ol class="arabic">
<li><p class="first">In vCenter, go to hosts and Clusters/Configuration, and click Storage
Adapters link. You will see:</p>
<p><img alt="vmwareiscsiinitiator.png: iscsi initiator" src="_images/vmware-iscsi-initiator.png" /></p>
</li>
<li><p class="first">Select iSCSI software adapter and click Properties.</p>
<p><img alt="vmwareiscsiinitiatorproperties.png: iscsi initiator properties" src="_images/vmware-iscsi-initiator-properties.png" /></p>
</li>
<li><p class="first">Click the Configure... button.</p>
<p><img alt="vmwareiscsigeneral.png: iscsi general" src="_images/vmware-iscsi-general.png" /></p>
</li>
<li><p class="first">Check Enabled to enable the initiator.</p>
</li>
<li><p class="first">Click OK to save.</p>
</li>
</ol>
</div>
<div class="section" id="add-iscsi-target">
<h5>Add iSCSI target<a class="headerlink" href="#add-iscsi-target" title="Permalink to this headline">¶</a></h5>
<p>Under the properties dialog, add the iSCSI target info:</p>
<p><img alt="vmwareiscsitargetadd.png: iscsi target add" src="_images/vmware-iscsi-target-add.png" /></p>
<p>Repeat these steps for all ESXi hosts in the cluster.</p>
</div>
<div class="section" id="create-an-iscsi-datastore">
<h5>Create an iSCSI datastore<a class="headerlink" href="#create-an-iscsi-datastore" title="Permalink to this headline">¶</a></h5>
<p>You should now create a VMFS datastore. Follow these steps to do so:</p>
<ol class="arabic simple">
<li>Select Home/Inventory/Datastores.</li>
<li>Right click on the datacenter node.</li>
<li>Choose Add Datastore... command.</li>
<li>Follow the wizard to create a iSCSI datastore.</li>
</ol>
<p>This procedure should be done on one host in the cluster. It is not
necessary to do this on all hosts.</p>
<p><img alt="vmwareiscsidatastore.png: iscsi datastore" src="_images/vmware-iscsi-datastore.png" /></p>
</div>
<div class="section" id="multipathing-for-vsphere-optional">
<h5>Multipathing for vSphere (Optional)<a class="headerlink" href="#multipathing-for-vsphere-optional" title="Permalink to this headline">¶</a></h5>
<p>Storage multipathing on vSphere nodes may be done according to the
vSphere installation guide.</p>
</div>
</div>
<div class="section" id="add-hosts-or-configure-clusters-vsphere">
<h4>Add Hosts or Configure Clusters (vSphere)<a class="headerlink" href="#add-hosts-or-configure-clusters-vsphere" title="Permalink to this headline">¶</a></h4>
<p>Use vCenter to create a vCenter cluster and add your desired hosts to
the cluster. You will later add the entire cluster to CloudStack. (see
<a class="reference external" href="configuration.html#add-cluster-vsphere">“Add Cluster: vSphere”</a>).</p>
</div>
<div class="section" id="applying-hotfixes-to-a-vmware-vsphere-host">
<h4>Applying Hotfixes to a VMware vSphere Host<a class="headerlink" href="#applying-hotfixes-to-a-vmware-vsphere-host" title="Permalink to this headline">¶</a></h4>
<ol class="arabic">
<li><p class="first">Disconnect the VMware vSphere cluster from CloudStack. It should
remain disconnected long enough to apply the hotfix on the host.</p>
<ol class="arabic">
<li><p class="first">Log in to the CloudStack UI as root.</p>
<p>See <a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-administration/en/latest/ui.html#log-in-to-the-ui">“Log In to the UI”</a>.</p>
</li>
<li><p class="first">Navigate to the VMware cluster, click Actions, and select
Unmanage.</p>
</li>
<li><p class="first">Watch the cluster status until it shows Unmanaged.</p>
</li>
</ol>
</li>
<li><p class="first">Perform the following on each of the ESXi hosts in the cluster:</p>
<ol class="arabic simple">
<li>Move each of the ESXi hosts in the cluster to maintenance mode.</li>
<li>Ensure that all the VMs are migrated to other hosts in that
cluster.</li>
<li>If there is only one host in that cluster, shutdown all the VMs
and move the host into maintenance mode.</li>
<li>Apply the patch on the ESXi host.</li>
<li>Restart the host if prompted.</li>
<li>Cancel the maintenance mode on the host.</li>
</ol>
</li>
<li><p class="first">Reconnect the cluster to CloudStack:</p>
<ol class="arabic">
<li><p class="first">Log in to the CloudStack UI as root.</p>
</li>
<li><p class="first">Navigate to the VMware cluster, click Actions, and select Manage.</p>
</li>
<li><p class="first">Watch the status to see that all the hosts come up. It might take
several minutes for the hosts to come up.</p>
<p>Alternatively, verify the host state is properly synchronized and
updated in the CloudStack database.</p>
</li>
</ol>
</li>
</ol>
</div>
</div>
<span id="document-hypervisor/xenserver"></span><div class="section" id="host-citrix-xenserver-installation">
<h3>Host Citrix XenServer Installation<a class="headerlink" href="#host-citrix-xenserver-installation" title="Permalink to this headline">¶</a></h3>
<p>If you want to use the Citrix XenServer hypervisor to run guest virtual
machines, install XenServer 6.0 or XenServer 6.0.2 on the host(s) in
your cloud. For an initial installation, follow the steps below. If you
have previously installed XenServer and want to upgrade to another
version, see <a class="reference internal" href="#upgrading-xenserver-version"><span>Upgrading XenServer Versions</span></a>.</p>
<div class="section" id="system-requirements-for-xenserver-hosts">
<h4>System Requirements for XenServer Hosts<a class="headerlink" href="#system-requirements-for-xenserver-hosts" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">The host must be certified as compatible with one of the following.
See the Citrix Hardware Compatibility Guide:
<a class="reference external" href="http://hcl.xensource.com">http://hcl.xensource.com</a></p>
<blockquote>
<div><ul class="simple">
<li>XenServer 5.6 SP2</li>
<li>XenServer 6.0</li>
<li>XenServer 6.0.2</li>
<li>XenServer 6.1.0</li>
<li>XenServer 6.2.0</li>
<li>XenServer 6.5.0</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">You must re-install Citrix XenServer if you are going to re-use a
host from a previous install.</p>
</li>
<li><p class="first">Must support HVM (Intel-VT or AMD-V enabled)</p>
</li>
<li><p class="first">Be sure all the hotfixes provided by the hypervisor vendor are
applied. Track the release of hypervisor patches through your
hypervisor vendor’s support channel, and apply patches as soon as
possible after they are released. CloudStack will not track or notify
you of required hypervisor patches. It is essential that your hosts
are completely up to date with the provided hypervisor patches. The
hypervisor vendor is likely to refuse to support any system that is
not up to date with patches.</p>
</li>
<li><p class="first">All hosts within a cluster must be homogeneous. The CPUs must be of
the same type, count, and feature flags.</p>
</li>
<li><p class="first">Must support HVM (Intel-VT or AMD-V enabled in BIOS)</p>
</li>
<li><p class="first">64-bit x86 CPU (more cores results in better performance)</p>
</li>
<li><p class="first">Hardware virtualization support required</p>
</li>
<li><p class="first">4 GB of memory</p>
</li>
<li><p class="first">36 GB of local disk</p>
</li>
<li><p class="first">At least 1 NIC</p>
</li>
<li><p class="first">Statically allocated IP Address</p>
</li>
<li><p class="first">When you deploy CloudStack, the hypervisor host must not have any VMs
already running</p>
</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">The lack of up-to-date hotfixes can lead to data corruption and lost VMs.</p>
</div>
</div>
<div class="section" id="xenserver-installation-steps">
<h4>XenServer Installation Steps<a class="headerlink" href="#xenserver-installation-steps" title="Permalink to this headline">¶</a></h4>
<ol class="arabic">
<li><p class="first">From <a class="reference external" href="https://www.citrix.com/English/ss/downloads/">https://www.citrix.com/English/ss/downloads/</a>,
download the appropriate version of XenServer for your CloudStack
version (see <a class="reference external" href="#system-requirements-for-xenserver-hosts">&#8220;System Requirements for XenServer Hosts&#8221;</a>). Install it using
the Citrix XenServer Installation Guide.</p>
<p>Older Versions of XenServer:</p>
<p>Note that you can download the most recent release of XenServer
without having a Citrix account. If you wish to download older
versions, you will need to create an account and look through the
download archives.</p>
</li>
</ol>
</div>
<div class="section" id="configure-xenserver-dom0-memory">
<h4>Configure XenServer dom0 Memory<a class="headerlink" href="#configure-xenserver-dom0-memory" title="Permalink to this headline">¶</a></h4>
<p>Configure the XenServer dom0 settings to allocate more memory to dom0.
This can enable XenServer to handle larger numbers of virtual machines.
We recommend 2940 MB of RAM for XenServer dom0. For instructions on how
to do this, see <a class="reference external" href="http://support.citrix.com/article/CTX126531">http://support.citrix.com/article/CTX126531</a>. The article refers to
XenServer 5.6, but the same information applies to XenServer 6.0.</p>
</div>
<div class="section" id="username-and-password">
<h4>Username and Password<a class="headerlink" href="#username-and-password" title="Permalink to this headline">¶</a></h4>
<p>All XenServers in a cluster must have the same username and password as
configured in CloudStack.</p>
</div>
<div class="section" id="time-synchronization">
<h4>Time Synchronization<a class="headerlink" href="#time-synchronization" title="Permalink to this headline">¶</a></h4>
<p>The host must be set to use NTP. All hosts in a pod must have the same
time.</p>
<ol class="arabic">
<li><p class="first">Install NTP.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum install ntp</span>
</pre></div>
</div>
</li>
<li><p class="first">Edit the NTP configuration file to point to your NTP server.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># vi /etc/ntp.conf</span>
</pre></div>
</div>
<p>Add one or more server lines in this file with the names of the NTP
servers you want to use. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>server 0.xenserver.pool.ntp.org
server 1.xenserver.pool.ntp.org
server 2.xenserver.pool.ntp.org
server 3.xenserver.pool.ntp.org
</pre></div>
</div>
</li>
<li><p class="first">Restart the NTP client.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service ntpd restart</span>
</pre></div>
</div>
</li>
<li><p class="first">Make sure NTP will start again upon reboot.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># chkconfig ntpd on</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="install-cloudstack-xenserver-support-package-csp">
<h4>Install CloudStack XenServer Support Package (CSP)<a class="headerlink" href="#install-cloudstack-xenserver-support-package-csp" title="Permalink to this headline">¶</a></h4>
<p>(Optional)</p>
<p>To enable security groups, elastic load balancing, and elastic IP on
XenServer, download and install the CloudStack XenServer Support Package
(CSP). After installing XenServer, perform the following additional
steps on each XenServer host.</p>
<p><strong>For XenServer 6.1:</strong></p>
<p>CSP functionality is already present in XenServer 6.1</p>
<ol class="arabic">
<li><p class="first">Run the below command</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>xe-switch-network-backend bridge
</pre></div>
</div>
</li>
<li><p class="first">update sysctl.conf with the following</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>net.bridge.bridge-nf-call-iptables <span class="o">=</span> 1
net.bridge.bridge-nf-call-ip6tables <span class="o">=</span> 0
net.bridge.bridge-nf-call-arptables <span class="o">=</span> 1

$ sysctl -p /etc/sysctl.conf
</pre></div>
</div>
</li>
</ol>
<p><strong>For XenServer 6.0.2, 6.0, 5.6 SP2:</strong></p>
<ol class="arabic">
<li><p class="first">Download the CSP software onto the XenServer host from one of the
following links:</p>
<p>For XenServer 6.0.2:</p>
<p><a class="reference external" href="http://download.cloud.com/releases/3.0.1/XS-6.0.2/xenserver-cloud-supp.tgz">http://download.cloud.com/releases/3.0.1/XS-6.0.2/xenserver-cloud-supp.tgz</a></p>
<p>For XenServer 5.6 SP2:</p>
<p><a class="reference external" href="http://download.cloud.com/releases/2.2.0/xenserver-cloud-supp.tgz">http://download.cloud.com/releases/2.2.0/xenserver-cloud-supp.tgz</a></p>
<p>For XenServer 6.0:</p>
<p><a class="reference external" href="http://download.cloud.com/releases/3.0/xenserver-cloud-supp.tgz">http://download.cloud.com/releases/3.0/xenserver-cloud-supp.tgz</a></p>
</li>
<li><p class="first">Extract the file:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># tar xf xenserver-cloud-supp.tgz</span>
</pre></div>
</div>
</li>
<li><p class="first">Run the following script:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe-install-supplemental-pack xenserver-cloud-supp.iso</span>
</pre></div>
</div>
</li>
<li><p class="first">If the XenServer host is part of a zone that uses basic networking,
disable Open vSwitch (OVS):</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe-switch-network-backend  bridge</span>
</pre></div>
</div>
<p>Restart the host machine when prompted.</p>
</li>
</ol>
<p>The XenServer host is now ready to be added to CloudStack.</p>
</div>
<div class="section" id="primary-storage-setup-for-xenserver">
<h4>Primary Storage Setup for XenServer<a class="headerlink" href="#primary-storage-setup-for-xenserver" title="Permalink to this headline">¶</a></h4>
<p>CloudStack natively supports NFS, iSCSI and local storage. If you are
using one of these storage types, there is no need to create the
XenServer Storage Repository (&#8220;SR&#8221;).</p>
<p>If, however, you would like to use storage connected via some other
technology, such as FiberChannel, you must set up the SR yourself. To do
so, perform the following steps. If you have your hosts in a XenServer
pool, perform the steps on the master node. If you are working with a
single XenServer which is not part of a cluster, perform the steps on
that XenServer.</p>
<ol class="arabic">
<li><p class="first">Connect FiberChannel cable to all hosts in the cluster and to the
FiberChannel storage host.</p>
</li>
<li><p class="first">Rescan the SCSI bus. Either use the following command or use
XenCenter to perform an HBA rescan.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># scsi-rescan</span>
</pre></div>
</div>
</li>
<li><p class="first">Repeat step 2 on every host.</p>
</li>
<li><p class="first">Check to be sure you see the new SCSI disk.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># ls /dev/disk/by-id/scsi-360a98000503365344e6f6177615a516b -l</span>
</pre></div>
</div>
<p>The output should look like this, although the specific file name
will be different (scsi-&lt;scsiID&gt;):</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>lrwxrwxrwx <span class="m">1</span> root root <span class="m">9</span> Mar <span class="m">16</span> 13:47
/dev/disk/by-id/scsi-360a98000503365344e6f6177615a516b -&gt; ../../sdc
</pre></div>
</div>
</li>
<li><p class="first">Repeat step 4 on every host.</p>
</li>
<li><p class="first">On the storage server, run this command to get a unique ID for the
new SR.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># uuidgen</span>
</pre></div>
</div>
<p>The output should look like this, although the specific ID will be
different:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>e6849e96-86c3-4f2c-8fcc-350cc711be3d
</pre></div>
</div>
</li>
<li><p class="first">Create the FiberChannel SR. In name-label, use the unique ID you just
generated.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe sr-create type=lvmohba shared=true</span>
device-config:SCSIid<span class="o">=</span>360a98000503365344e6f6177615a516b
name-label<span class="o">=</span><span class="s2">&quot;e6849e96-86c3-4f2c-8fcc-350cc711be3d&quot;</span>
</pre></div>
</div>
<p>This command returns a unique ID for the SR, like the following
example (your ID will be different):</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>7a143820-e893-6c6a-236e-472da6ee66bf
</pre></div>
</div>
</li>
<li><p class="first">To create a human-readable description for the SR, use the following
command. In uuid, use the SR ID returned by the previous command. In
name-description, set whatever friendly text you prefer.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe sr-param-set uuid=7a143820-e893-6c6a-236e-472da6ee66bf name-description=&quot;Fiber Channel storage repository&quot;</span>
</pre></div>
</div>
<p>Make note of the values you will need when you add this storage to
CloudStack later (see <a class="reference external" href="configuration.html#add-primary-storage">&#8220;Add Primary Storage&#8221;</a>). In the Add Primary Storage
dialog, in Protocol, you will choose PreSetup. In SR Name-Label, you
will enter the name-label you set earlier (in this example,
e6849e96-86c3-4f2c-8fcc-350cc711be3d).</p>
</li>
<li><p class="first">(Optional) If you want to enable multipath I/O on a FiberChannel SAN,
refer to the documentation provided by the SAN vendor.</p>
</li>
</ol>
</div>
<div class="section" id="iscsi-multipath-setup-for-xenserver-optional">
<h4>iSCSI Multipath Setup for XenServer (Optional)<a class="headerlink" href="#iscsi-multipath-setup-for-xenserver-optional" title="Permalink to this headline">¶</a></h4>
<p>When setting up the storage repository on a Citrix XenServer, you can
enable multipath I/O, which uses redundant physical components to
provide greater reliability in the connection between the server and the
SAN. To enable multipathing, use a SAN solution that is supported for
Citrix servers and follow the procedures in Citrix documentation. The
following links provide a starting point:</p>
<ul class="simple">
<li><a class="reference external" href="http://support.citrix.com/article/CTX118791">http://support.citrix.com/article/CTX118791</a></li>
<li><a class="reference external" href="http://support.citrix.com/article/CTX125403">http://support.citrix.com/article/CTX125403</a></li>
</ul>
<p>You can also ask your SAN vendor for advice about setting up your Citrix
repository for multipathing.</p>
<p>Make note of the values you will need when you add this storage to the
CloudStack later (see <a class="reference external" href="configuration.html#add-primary-storage">&#8220;Add Primary Storage&#8221;</a>). In the Add Primary Storage
dialog, in Protocol, you will choose PreSetup. In SR Name-Label, you will
enter the same name used to create the SR.</p>
<p>If you encounter difficulty, address the support team for the SAN
provided by your vendor. If they are not able to solve your issue, see
Contacting Support.</p>
</div>
<div class="section" id="physical-networking-setup-for-xenserver">
<h4>Physical Networking Setup for XenServer<a class="headerlink" href="#physical-networking-setup-for-xenserver" title="Permalink to this headline">¶</a></h4>
<p>Once XenServer has been installed, you may need to do some additional
network configuration. At this point in the installation, you should
have a plan for what NICs the host will have and what traffic each NIC
will carry. The NICs should be cabled as necessary to implement your
plan.</p>
<p>If you plan on using NIC bonding, the NICs on all hosts in the cluster
must be cabled exactly the same. For example, if eth0 is in the private
bond on one host in a cluster, then eth0 must be in the private bond on
all hosts in the cluster.</p>
<p>The IP address assigned for the management network interface must be
static. It can be set on the host itself or obtained via static DHCP.</p>
<p>CloudStack configures network traffic of various types to use different
NICs or bonds on the XenServer host. You can control this process and
provide input to the Management Server through the use of XenServer
network name labels. The name labels are placed on physical interfaces
or bonds and configured in CloudStack. In some simple cases the name
labels are not required.</p>
<p>When configuring networks in a XenServer environment, network traffic
labels must be properly configured to ensure that the virtual interfaces
are created by CloudStack are bound to the correct physical device. The
name-label of the XenServer network must match the XenServer traffic
label specified while creating the CloudStack network. This is set by
running the following command:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>xe network-param-set <span class="nv">uuid</span><span class="o">=</span>&lt;network id&gt; name-label<span class="o">=</span>&lt;CloudStack traffic label&gt;
</pre></div>
</div>
<div class="section" id="configuring-public-network-with-a-dedicated-nic-for-xenserver-optional">
<h5>Configuring Public Network with a Dedicated NIC for XenServer (Optional)<a class="headerlink" href="#configuring-public-network-with-a-dedicated-nic-for-xenserver-optional" title="Permalink to this headline">¶</a></h5>
<p>CloudStack supports the use of a second NIC (or bonded pair of NICs,
described in <a class="reference internal" href="#nic-bonding-for-xenserver"><span>NIC Bonding for XenServer (Optional)</span></a>) for the public network. If
bonding is not used, the public network can be on any NIC and can be on
different NICs on the hosts in a cluster. For example, the public
network can be on eth0 on node A and eth1 on node B. However, the
XenServer name-label for the public network must be identical across all
hosts. The following examples set the network label to &#8220;cloud-public&#8221;.
After the management server is installed and running you must configure
it with the name of the chosen network label (e.g. &#8220;cloud-public&#8221;); this
is discussed in <a class="reference external" href="installation.html#management-server-installation">&#8220;Management Server Installation&#8221;</a>.</p>
<p>If you are using two NICs bonded together to create a public network,
see <a class="reference internal" href="#nic-bonding-for-xenserver"><span>NIC Bonding for XenServer (Optional)</span></a>.</p>
<p>If you are using a single dedicated NIC to provide public network
access, follow this procedure on each new host that is added to
CloudStack before adding the host.</p>
<ol class="arabic">
<li><p class="first">Run xe network-list and find the public network. This is usually
attached to the NIC that is public. Once you find the network make
note of its UUID. Call this &lt;UUID-Public&gt;.</p>
</li>
<li><p class="first">Run the following command.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe network-param-set name-label=cloud-public uuid=&lt;UUID-Public&gt;</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="configuring-multiple-guest-networks-for-xenserver-optional">
<h5>Configuring Multiple Guest Networks for XenServer (Optional)<a class="headerlink" href="#configuring-multiple-guest-networks-for-xenserver-optional" title="Permalink to this headline">¶</a></h5>
<p>CloudStack supports the use of multiple guest networks with the
XenServer hypervisor. Each network is assigned a name-label in
XenServer. For example, you might have two networks with the labels
&#8220;cloud-guest&#8221; and &#8220;cloud-guest2&#8221;. After the management server is
installed and running, you must add the networks and use these labels so
that CloudStack is aware of the networks.</p>
<p>Follow this procedure on each new host before adding the host to
CloudStack:</p>
<ol class="arabic">
<li><p class="first">Run xe network-list and find one of the guest networks. Once you find
the network make note of its UUID. Call this &lt;UUID-Guest&gt;.</p>
</li>
<li><p class="first">Run the following command, substituting your own name-label and uuid
values.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe network-param-set name-label=&lt;cloud-guestN&gt; uuid=&lt;UUID-Guest&gt;</span>
</pre></div>
</div>
</li>
<li><p class="first">Repeat these steps for each additional guest network, using a
different name-label and uuid each time.</p>
</li>
</ol>
</div>
<div class="section" id="separate-storage-network-for-xenserver-optional">
<h5>Separate Storage Network for XenServer (Optional)<a class="headerlink" href="#separate-storage-network-for-xenserver-optional" title="Permalink to this headline">¶</a></h5>
<p>You can optionally set up a separate storage network. This should be
done first on the host, before implementing the bonding steps below.
This can be done using one or two available NICs. With two NICs bonding
may be done as above. It is the administrator&#8217;s responsibility to set up
a separate storage network.</p>
<p>Give the storage network a different name-label than what will be given
for other networks.</p>
<p>For the separate storage network to work correctly, it must be the only
interface that can ping the primary storage device&#8217;s IP address. For
example, if eth0 is the management network NIC, ping -I eth0 &lt;primary
storage device IP&gt; must fail. In all deployments, secondary storage
devices must be pingable from the management network NIC or bond. If a
secondary storage device has been placed on the storage network, it must
also be pingable via the storage network NIC or bond on the hosts as
well.</p>
<p>You can set up two separate storage networks as well. For example, if
you intend to implement iSCSI multipath, dedicate two non-bonded NICs to
multipath. Each of the two networks needs a unique name-label.</p>
<p>If no bonding is done, the administrator must set up and name-label the
separate storage network on all hosts (masters and slaves).</p>
<p>Here is an example to set up eth5 to access a storage network on
172.16.0.0/24.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe pif-list host-name-label=&#39;hostname&#39; device=eth5</span>
uuid<span class="o">(</span>RO<span class="o">)</span>: ab0d3dd4-5744-8fae-9693-a022c7a3471d
device <span class="o">(</span> RO<span class="o">)</span>: eth5
<span class="c1">#xe pif-reconfigure-ip DNS=172.16.3.3 gateway=172.16.0.1 IP=172.16.0.55 mode=static netmask=255.255.255.0 uuid=ab0d3dd4-5744-8fae-9693-a022c7a3471d</span>
</pre></div>
</div>
</div>
<div class="section" id="nic-bonding-for-xenserver-optional">
<span id="nic-bonding-for-xenserver"></span><h5>NIC Bonding for XenServer (Optional)<a class="headerlink" href="#nic-bonding-for-xenserver-optional" title="Permalink to this headline">¶</a></h5>
<p>XenServer supports Source Level Balancing (SLB) NIC bonding. Two NICs
can be bonded together to carry public, private, and guest traffic, or
some combination of these. Separate storage networks are also possible.
Here are some example supported configurations:</p>
<ul class="simple">
<li>2 NICs on private, 2 NICs on public, 2 NICs on storage</li>
<li>2 NICs on private, 1 NIC on public, storage uses management network</li>
<li>2 NICs on private, 2 NICs on public, storage uses management network</li>
<li>1 NIC for private, public, and storage</li>
</ul>
<p>All NIC bonding is optional.</p>
<p>XenServer expects all nodes in a cluster will have the same network
cabling and same bonds implemented. In an installation the master will
be the first host that was added to the cluster and the slave hosts will
be all subsequent hosts added to the cluster. The bonds present on the
master set the expectation for hosts added to the cluster later. The
procedure to set up bonds on the master and slaves are different, and
are described below. There are several important implications of this:</p>
<ul class="simple">
<li>You must set bonds on the first host added to a cluster. Then you
must use xe commands as below to establish the same bonds in the
second and subsequent hosts added to a cluster.</li>
<li>Slave hosts in a cluster must be cabled exactly the same as the
master. For example, if eth0 is in the private bond on the master, it
must be in the management network for added slave hosts.</li>
</ul>
<div class="section" id="management-network-bonding">
<h6>Management Network Bonding<a class="headerlink" href="#management-network-bonding" title="Permalink to this headline">¶</a></h6>
<p>The administrator must bond the management network NICs prior to adding
the host to CloudStack.</p>
</div>
<div class="section" id="creating-a-private-bond-on-the-first-host-in-the-cluster">
<h6>Creating a Private Bond on the First Host in the Cluster<a class="headerlink" href="#creating-a-private-bond-on-the-first-host-in-the-cluster" title="Permalink to this headline">¶</a></h6>
<p>Use the following steps to create a bond in XenServer. These steps
should be run on only the first host in a cluster. This example creates
the cloud-private network with two physical NICs (eth0 and eth1) bonded
into it.</p>
<ol class="arabic">
<li><p class="first">Find the physical NICs that you want to bond together.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe pif-list host-name-label=&#39;hostname&#39; device=eth0</span>
<span class="c1"># xe pif-list host-name-label=&#39;hostname&#39; device=eth1</span>
</pre></div>
</div>
<p>These command shows the eth0 and eth1 NICs and their UUIDs.
Substitute the ethX devices of your choice. Call the UUID&#8217;s returned
by the above command slave1-UUID and slave2-UUID.</p>
</li>
<li><p class="first">Create a new network for the bond. For example, a new network with
name &#8220;cloud-private&#8221;.</p>
<p><strong>This label is important. CloudStack looks for a network by a name
you configure. You must use the same name-label for all hosts in the
cloud for the management network.</strong></p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe network-create name-label=cloud-private</span>
<span class="c1"># xe bond-create network-uuid=[uuid of cloud-private created above]</span>
pif-uuids<span class="o">=[</span>slave1-uuid<span class="o">]</span>,<span class="o">[</span>slave2-uuid<span class="o">]</span>
</pre></div>
</div>
</li>
</ol>
<p>Now you have a bonded pair that can be recognized by CloudStack as the
management network.</p>
</div>
<div class="section" id="public-network-bonding">
<h6>Public Network Bonding<a class="headerlink" href="#public-network-bonding" title="Permalink to this headline">¶</a></h6>
<p>Bonding can be implemented on a separate, public network. The
administrator is responsible for creating a bond for the public network
if that network will be bonded and will be separate from the management
network.</p>
</div>
<div class="section" id="creating-a-public-bond-on-the-first-host-in-the-cluster">
<h6>Creating a Public Bond on the First Host in the Cluster<a class="headerlink" href="#creating-a-public-bond-on-the-first-host-in-the-cluster" title="Permalink to this headline">¶</a></h6>
<p>These steps should be run on only the first host in a cluster. This
example creates the cloud-public network with two physical NICs (eth2
and eth3) bonded into it.</p>
<ol class="arabic">
<li><p class="first">Find the physical NICs that you want to bond together.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe pif-list host-name-label=&#39;hostname&#39; device=eth2</span>
<span class="c1"># xe pif-list host-name-label=&#39;hostname&#39; device=eth3</span>
</pre></div>
</div>
<p>These command shows the eth2 and eth3 NICs and their UUIDs.
Substitute the ethX devices of your choice. Call the UUID&#8217;s returned
by the above command slave1-UUID and slave2-UUID.</p>
</li>
<li><p class="first">Create a new network for the bond. For example, a new network with
name &#8220;cloud-public&#8221;.</p>
<p><strong>This label is important. CloudStack looks for a network by a name
you configure. You must use the same name-label for all hosts in the
cloud for the public network.</strong></p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe network-create name-label=cloud-public</span>
<span class="c1"># xe bond-create network-uuid=[uuid of cloud-public created above]</span>
pif-uuids<span class="o">=[</span>slave1-uuid<span class="o">]</span>,<span class="o">[</span>slave2-uuid<span class="o">]</span>
</pre></div>
</div>
</li>
</ol>
<p>Now you have a bonded pair that can be recognized by CloudStack as the
public network.</p>
</div>
<div class="section" id="adding-more-hosts-to-the-cluster">
<h6>Adding More Hosts to the Cluster<a class="headerlink" href="#adding-more-hosts-to-the-cluster" title="Permalink to this headline">¶</a></h6>
<p>With the bonds (if any) established on the master, you should add
additional, slave hosts. Run the following command for all additional
hosts to be added to the cluster. This will cause the host to join the
master in a single XenServer pool.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># xe pool-join master-address=[master IP] master-username=root</span>
master-password<span class="o">=[</span>your password<span class="o">]</span>
</pre></div>
</div>
</div>
<div class="section" id="complete-the-bonding-setup-across-the-cluster">
<h6>Complete the Bonding Setup Across the Cluster<a class="headerlink" href="#complete-the-bonding-setup-across-the-cluster" title="Permalink to this headline">¶</a></h6>
<p>With all hosts added to the pool, run the cloud-setup-bond script. This
script will complete the configuration and set up of the bonds across
all hosts in the cluster.</p>
<ol class="arabic">
<li><p class="first">Copy the script from the Management Server in
/usr/share/cloudstack-common/scripts/vm/hypervisor/xenserver/cloud-setup-bonding.sh
to the master host and ensure it is executable.</p>
</li>
<li><p class="first">Run the script:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># ./cloud-setup-bonding.sh</span>
</pre></div>
</div>
</li>
</ol>
<p>Now the bonds are set up and configured properly across the cluster.</p>
</div>
</div>
</div>
<div class="section" id="upgrading-xenserver-versions">
<span id="upgrading-xenserver-version"></span><h4>Upgrading XenServer Versions<a class="headerlink" href="#upgrading-xenserver-versions" title="Permalink to this headline">¶</a></h4>
<p>This section tells how to upgrade XenServer software on CloudStack
hosts. The actual upgrade is described in XenServer documentation, but
there are some additional steps you must perform before and after the
upgrade.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Be sure the hardware is certified compatible with the new version of
XenServer.</p>
</div>
<p>To upgrade XenServer:</p>
<ol class="arabic">
<li><p class="first">Upgrade the database. On the Management Server node:</p>
<ol class="arabic">
<li><p class="first">Back up the database:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysqldump --user=root --databases cloud &gt; cloud.backup.sql</span>
<span class="c1"># mysqldump --user=root --databases cloud_usage &gt; cloud_usage.backup.sql</span>
</pre></div>
</div>
</li>
<li><p class="first">You might need to change the OS type settings for VMs running on
the upgraded hosts.</p>
<ul class="simple">
<li>If you upgraded from XenServer 5.6 GA to XenServer 5.6 SP2,
change any VMs that have the OS type CentOS 5.5 (32-bit),
Oracle Enterprise Linux 5.5 (32-bit), or Red Hat Enterprise
Linux 5.5 (32-bit) to Other Linux (32-bit). Change any VMs that
have the 64-bit versions of these same OS types to Other Linux
(64-bit).</li>
<li>If you upgraded from XenServer 5.6 SP2 to XenServer 6.0.2,
change any VMs that have the OS type CentOS 5.6 (32-bit),
CentOS 5.7 (32-bit), Oracle Enterprise Linux 5.6 (32-bit),
Oracle Enterprise Linux 5.7 (32-bit), Red Hat Enterprise Linux
5.6 (32-bit) , or Red Hat Enterprise Linux 5.7 (32-bit) to
Other Linux (32-bit). Change any VMs that have the 64-bit
versions of these same OS types to Other Linux (64-bit).</li>
<li>If you upgraded from XenServer 5.6 to XenServer 6.0.2, do all
of the above.</li>
</ul>
</li>
<li><p class="first">Restart the Management Server and Usage Server. You only need to
do this once for all clusters.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service cloudstack-management start</span>
<span class="c1"># service cloudstack-usage start</span>
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Disconnect the XenServer cluster from CloudStack.</p>
<ol class="arabic simple">
<li>Log in to the CloudStack UI as root.</li>
<li>Navigate to the XenServer cluster, and click Actions – Unmanage.</li>
<li>Watch the cluster status until it shows Unmanaged.</li>
</ol>
</li>
<li><p class="first">Log in to one of the hosts in the cluster, and run this command to
clean up the VLAN:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># . /opt/xensource/bin/cloud-clean-vlan.sh</span>
</pre></div>
</div>
</li>
<li><p class="first">Still logged in to the host, run the upgrade preparation script:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># /opt/xensource/bin/cloud-prepare-upgrade.sh</span>
</pre></div>
</div>
<p>Troubleshooting: If you see the error &#8220;can&#8217;t eject CD,&#8221; log in to the
VM and umount the CD, then run the script again.</p>
</li>
<li><p class="first">Upgrade the XenServer software on all hosts in the cluster. Upgrade
the master first.</p>
<ol class="arabic">
<li><p class="first">Live migrate all VMs on this host to other hosts. See the
instructions for live migration in the Administrator&#8217;s Guide.</p>
<p>Troubleshooting: You might see the following error when you
migrate a VM:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="o">[</span>root@xenserver-qa-2-49-4 ~<span class="o">]</span><span class="c1"># xe vm-migrate live=true host=xenserver-qa-2-49-5 vm=i-2-8-VM</span>
You attempted an operation on a VM which requires PV drivers to be installed but the drivers were not detected.
vm: b6cf79c8-02ee-050b-922f-49583d9f1a14 <span class="o">(</span>i-2-8-VM<span class="o">)</span>
</pre></div>
</div>
<p>To solve this issue, run the following:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># /opt/xensource/bin/make_migratable.sh  b6cf79c8-02ee-050b-922f-49583d9f1a14</span>
</pre></div>
</div>
</li>
<li><p class="first">Reboot the host.</p>
</li>
<li><p class="first">Upgrade to the newer version of XenServer. Use the steps in
XenServer documentation.</p>
</li>
<li><p class="first">After the upgrade is complete, copy the following files from the
management server to this host, in the directory locations shown
below:</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="68%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Copy this Management Server file</th>
<th class="head">To this location on the XenServer host</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>/usr/share/cloudstack-common/scripts/vm/hypervisor/xenserver/xenserver60/NFSSR.py</td>
<td>/opt/xensource/sm/NFSSR.py</td>
</tr>
<tr class="row-odd"><td>/usr/share/cloudstack-common/scripts/vm/hypervisor/xenserver/setupxenserver.sh</td>
<td>/opt/xensource/bin/setupxenserver.sh</td>
</tr>
<tr class="row-even"><td>/usr/share/cloudstack-common/scripts/vm/hypervisor/xenserver/make_migratable.sh</td>
<td>/opt/xensource/bin/make_migratable.sh</td>
</tr>
<tr class="row-odd"><td>/usr/share/cloudstack-common/scripts/vm/hypervisor/xenserver/cloud-clean-vlan.sh</td>
<td>/opt/xensource/bin/cloud-clean-vlan.sh</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first">Run the following script:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># /opt/xensource/bin/setupxenserver.sh</span>
</pre></div>
</div>
<p>Troubleshooting: If you see the following error message, you can
safely ignore it.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mv: cannot stat <span class="sb">`</span>/etc/cron.daily/logrotate<span class="sb">`</span>: No such file or directory
</pre></div>
</div>
</li>
<li><p class="first">Plug in the storage repositories (physical block devices) to the
XenServer host:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># for pbd in `xe pbd-list currently-attached=false| grep ^uuid | awk &#39;{print $NF}&#39;`; do xe pbd-plug uuid=$pbd ; done</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you add a host to this XenServer pool, you need to migrate all VMs
on this host to other hosts, and eject this host from XenServer pool.</p>
</div>
</li>
</ol>
</li>
<li><p class="first">Repeat these steps to upgrade every host in the cluster to the same
version of XenServer.</p>
</li>
<li><p class="first">Run the following command on one host in the XenServer cluster to
clean up the host tags:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># for host in $(xe host-list | grep ^uuid | awk &#39;{print $NF}&#39;) ; do xe host-param-clear uuid=$host param-name=tags; done;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When copying and pasting a command, be sure the command has pasted as
a single line before executing. Some document viewers may introduce
unwanted line breaks in copied text.</p>
</div>
</li>
<li><p class="first">Reconnect the XenServer cluster to CloudStack.</p>
<ol class="arabic simple">
<li>Log in to the CloudStack UI as root.</li>
<li>Navigate to the XenServer cluster, and click Actions – Manage.</li>
<li>Watch the status to see that all the hosts come up.</li>
</ol>
</li>
<li><p class="first">After all hosts are up, run the following on one host in the cluster:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># /opt/xensource/bin/cloud-clean-vlan.sh</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="section" id="network-setup">
<span id="network"></span><h2>Network Setup<a class="headerlink" href="#network-setup" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-network_setup"></span><div class="section" id="network-setup">
<h3>Network Setup<a class="headerlink" href="#network-setup" title="Permalink to this headline">¶</a></h3>
<p>Achieving the correct networking setup is crucial to a successful
CloudStack installation. This section contains information to help you
make decisions and follow the right procedures to get your network set
up correctly.</p>
<div class="section" id="basic-and-advanced-networking">
<h4>Basic and Advanced Networking<a class="headerlink" href="#basic-and-advanced-networking" title="Permalink to this headline">¶</a></h4>
<p>CloudStack provides two styles of networking:.</p>
<p><strong>Basic</strong>
For AWS-style networking. Provides a single network where guest isolation can
be provided through layer-3 means such as security groups (IP address source
filtering).</p>
<p><strong>Advanced</strong>
For more sophisticated network topologies. This network model provides the
most flexibility in defining guest networks, but requires more configuration
steps than basic networking.</p>
<p>Each zone has either basic or advanced networking. Once the choice of
networking model for a zone has been made and configured in CloudStack,
it can not be changed. A zone is either basic or advanced for its entire
lifetime.</p>
<p>The following table compares the networking features in the two networking models.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="27%" />
<col width="38%" />
<col width="34%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Networking Feature</th>
<th class="head">Basic Network</th>
<th class="head">Advanced Network</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Number of networks</td>
<td>Single network</td>
<td>Multiple networks</td>
</tr>
<tr class="row-odd"><td>Firewall type</td>
<td>Physical</td>
<td>Physical and Virtual</td>
</tr>
<tr class="row-even"><td>Load balancer</td>
<td>Physical</td>
<td>Physical and Virtual</td>
</tr>
<tr class="row-odd"><td>Isolation type</td>
<td>Layer 3</td>
<td>Layer 2 and Layer 3</td>
</tr>
<tr class="row-even"><td>VPN support</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>Port forwarding</td>
<td>Physical</td>
<td>Physical and Virtual</td>
</tr>
<tr class="row-even"><td>1:1 NAT</td>
<td>Physical</td>
<td>Physical and Virtual</td>
</tr>
<tr class="row-odd"><td>Source NAT</td>
<td>No</td>
<td>Physical and Virtual</td>
</tr>
<tr class="row-even"><td>Userdata</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>Network usage monitoring</td>
<td>sFlow / netFlow at physical router</td>
<td>Hypervisor and Virtual Router</td>
</tr>
<tr class="row-even"><td>DNS and DHCP</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p>The two types of networking may be in use in the same cloud. However, a
given zone must use either Basic Networking or Advanced Networking.</p>
<p>Different types of network traffic can be segmented on the same physical
network. Guest traffic can also be segmented by account. To isolate
traffic, you can use separate VLANs. If you are using separate VLANs on
a single physical network, make sure the VLAN tags are in separate
numerical ranges.</p>
</div>
<div class="section" id="vlan-allocation-example">
<h4>VLAN Allocation Example<a class="headerlink" href="#vlan-allocation-example" title="Permalink to this headline">¶</a></h4>
<p>VLANs are required for public and guest traffic. The following is an
example of a VLAN allocation scheme:</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="11%" />
<col width="33%" />
<col width="56%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">VLAN IDs</th>
<th class="head">Traffic type</th>
<th class="head">Scope</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>less than 500</td>
<td>Management traffic. Reserved for administrative purposes.</td>
<td>CloudStack software can access this, hypervisors, system VMs.</td>
</tr>
<tr class="row-odd"><td>500-599</td>
<td>VLAN carrying public traffic.</td>
<td>CloudStack accounts.</td>
</tr>
<tr class="row-even"><td>600-799</td>
<td>VLANs carrying guest traffic.</td>
<td>CloudStack accounts. Account-specific VLAN is chosen from this pool.</td>
</tr>
<tr class="row-odd"><td>800-899</td>
<td>VLANs carrying guest traffic.</td>
<td>CloudStack accounts. Account-specific VLAN chosen by CloudStack admin to assign to that account.</td>
</tr>
<tr class="row-even"><td>900-999</td>
<td>VLAN carrying guest traffic</td>
<td>CloudStack accounts. Can be scoped by project, domain, or all accounts.</td>
</tr>
<tr class="row-odd"><td>greater than 1000</td>
<td>Reserved for future use</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="example-hardware-configuration">
<h4>Example Hardware Configuration<a class="headerlink" href="#example-hardware-configuration" title="Permalink to this headline">¶</a></h4>
<p>This section contains an example configuration of specific switch models
for zone-level layer-3 switching. It assumes VLAN management protocols,
such as VTP or GVRP, have been disabled. The example scripts must be
changed appropriately if you choose to use VTP or GVRP.</p>
<div class="section" id="dell-62xx">
<h5>Dell 62xx<a class="headerlink" href="#dell-62xx" title="Permalink to this headline">¶</a></h5>
<p>The following steps show how a Dell 62xx is configured for zone-level
layer-3 switching. These steps assume VLAN 201 is used to route untagged
private IPs for pod 1, and pod 1’s layer-2 switch is connected to
Ethernet port 1/g1.</p>
<p>The Dell 62xx Series switch supports up to 1024 VLANs.</p>
<ol class="arabic">
<li><p class="first">Configure all the VLANs in the database.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vlan database
vlan 200-999
<span class="nb">exit</span>
</pre></div>
</div>
</li>
<li><p class="first">Configure Ethernet port 1/g1.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>interface ethernet 1/g1
switchport mode general
switchport general pvid 201
switchport general allowed vlan add <span class="m">201</span> untagged
switchport general allowed vlan add 300-999 tagged
<span class="nb">exit</span>
</pre></div>
</div>
</li>
</ol>
<p>The statements configure Ethernet port 1/g1 as follows:</p>
<ul class="simple">
<li>VLAN 201 is the native untagged VLAN for port 1/g1.</li>
<li>All VLANs (300-999) are passed to all the pod-level layer-2 switches.</li>
</ul>
</div>
<div class="section" id="cisco-3750">
<h5>Cisco 3750<a class="headerlink" href="#cisco-3750" title="Permalink to this headline">¶</a></h5>
<p>The following steps show how a Cisco 3750 is configured for zone-level
layer-3 switching. These steps assume VLAN 201 is used to route untagged
private IPs for pod 1, and pod 1’s layer-2 switch is connected to
GigabitEthernet1/0/1.</p>
<ol class="arabic">
<li><p class="first">Setting VTP mode to transparent allows us to utilize VLAN IDs above
1000. Since we only use VLANs up to 999, vtp transparent mode is not
strictly required.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vtp mode transparent
vlan 200-999
<span class="nb">exit</span>
</pre></div>
</div>
</li>
<li><p class="first">Configure GigabitEthernet1/0/1.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>interface GigabitEthernet1/0/1
switchport trunk encapsulation dot1q
switchport mode trunk
switchport trunk native vlan 201
<span class="nb">exit</span>
</pre></div>
</div>
</li>
</ol>
<p>The statements configure GigabitEthernet1/0/1 as follows:</p>
<ul class="simple">
<li>VLAN 201 is the native untagged VLAN for port GigabitEthernet1/0/1.</li>
<li>Cisco passes all VLANs by default. As a result, all VLANs (300-999)
are passed to all the pod-level layer-2 switches.</li>
</ul>
</div>
</div>
<div class="section" id="layer-2-switch">
<h4>Layer-2 Switch<a class="headerlink" href="#layer-2-switch" title="Permalink to this headline">¶</a></h4>
<p>The layer-2 switch is the access switching layer inside the pod.</p>
<ul class="simple">
<li>It should trunk all VLANs into every computing host.</li>
<li>It should switch traffic for the management network containing
computing and storage hosts. The layer-3 switch will serve as the
gateway for the management network.</li>
</ul>
<p>The following sections contain example configurations for specific switch models
for pod-level layer-2 switching. It assumes VLAN management protocols
such as VTP or GVRP have been disabled. The scripts must be changed
appropriately if you choose to use VTP or GVRP.</p>
<div class="section" id="id1">
<h5>Dell 62xx<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>The following steps show how a Dell 62xx is configured for pod-level
layer-2 switching.</p>
<ol class="arabic">
<li><p class="first">Configure all the VLANs in the database.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vlan database
vlan 300-999
<span class="nb">exit</span>
</pre></div>
</div>
</li>
<li><p class="first">VLAN 201 is used to route untagged private IP addresses for pod 1,
and pod 1 is connected to this layer-2 switch.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>interface range ethernet all
switchport mode general
switchport general allowed vlan add 300-999 tagged
<span class="nb">exit</span>
</pre></div>
</div>
</li>
</ol>
<p>The statements configure all Ethernet ports to function as follows:</p>
<ul class="simple">
<li>All ports are configured the same way.</li>
<li>All VLANs (300-999) are passed through all the ports of the layer-2
switch.</li>
</ul>
</div>
<div class="section" id="id2">
<h5>Cisco 3750<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<p>The following steps show how a Cisco 3750 is configured for pod-level
layer-2 switching.</p>
<ol class="arabic">
<li><p class="first">Setting VTP mode to transparent allows us to utilize VLAN IDs above
1000. Since we only use VLANs up to 999, vtp transparent mode is not
strictly required.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>vtp mode transparent
vlan 300-999
<span class="nb">exit</span>
</pre></div>
</div>
</li>
<li><p class="first">Configure all ports to dot1q and set 201 as the native VLAN.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>interface range GigabitEthernet 1/0/1-24
switchport trunk encapsulation dot1q
switchport mode trunk
switchport trunk native vlan 201
<span class="nb">exit</span>
</pre></div>
</div>
</li>
</ol>
<p>By default, Cisco passes all VLANs. Cisco switches complain of the
native VLAN IDs are different when 2 ports are connected together.
That’s why you must specify VLAN 201 as the native VLAN on the layer-2
switch.</p>
</div>
</div>
<div class="section" id="hardware-firewall">
<h4>Hardware Firewall<a class="headerlink" href="#hardware-firewall" title="Permalink to this headline">¶</a></h4>
<p>All deployments should have a firewall protecting the management server;
see Generic Firewall Provisions. Optionally, some deployments may also
have a Juniper SRX firewall that will be the default gateway for the
guest networks; see <a class="reference external" href="#external-guest-firewall-integration-for-juniper-srx-optional">“External Guest Firewall Integration for Juniper SRX (Optional)”</a>.</p>
<div class="section" id="generic-firewall-provisions">
<h5>Generic Firewall Provisions<a class="headerlink" href="#generic-firewall-provisions" title="Permalink to this headline">¶</a></h5>
<p>The hardware firewall is required to serve two purposes:</p>
<ul class="simple">
<li>Protect the Management Servers. NAT and port forwarding should be
configured to direct traffic from the public Internet to the
Management Servers.</li>
<li>Route management network traffic between multiple zones. Site-to-site
VPN should be configured between multiple zones.</li>
</ul>
<p>To achieve the above purposes you must set up fixed configurations for
the firewall. Firewall rules and policies need not change as users are
provisioned into the cloud. Any brand of hardware firewall that supports
NAT and site-to-site VPN can be used.</p>
</div>
<div class="section" id="id3">
<h5>External Guest Firewall Integration for Juniper SRX (Optional)<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Available only for guests using advanced networking.</p>
</div>
<p>CloudStack provides for direct management of the Juniper SRX series of
firewalls. This enables CloudStack to establish static NAT mappings from
public IPs to guest VMs, and to use the Juniper device in place of the
virtual router for firewall services. You can have one or more Juniper
SRX per zone. This feature is optional. If Juniper integration is not
provisioned, CloudStack will use the virtual router for these services.</p>
<p>The Juniper SRX can optionally be used in conjunction with an external
load balancer. External Network elements can be deployed in a
side-by-side or inline configuration.</p>
<p><img alt="parallel-mode.png: adding a firewall and load balancer in parallel mode." src="_images/parallel-mode.png" /></p>
<p>CloudStack requires the Juniper SRX firewall to be configured as follows:</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Supported SRX software version is 10.3 or higher.</p>
</div>
<ol class="arabic">
<li><p class="first">Install your SRX appliance according to the vendor&#8217;s instructions.</p>
</li>
<li><p class="first">Connect one interface to the management network and one interface to
the public network. Alternatively, you can connect the same interface
to both networks and a use a VLAN for the public network.</p>
</li>
<li><p class="first">Make sure &#8220;vlan-tagging&#8221; is enabled on the private interface.</p>
</li>
<li><p class="first">Record the public and private interface names. If you used a VLAN for
the public interface, add a &#8221;.[VLAN TAG]&#8221; after the interface name.
For example, if you are using ge-0/0/3 for your public interface and
VLAN tag 301, your public interface name would be &#8220;ge-0/0/3.301&#8221;.
Your private interface name should always be untagged because the
CloudStack software automatically creates tagged logical interfaces.</p>
</li>
<li><p class="first">Create a public security zone and a private security zone. By
default, these will already exist and will be called &#8220;untrust&#8221; and
&#8220;trust&#8221;. Add the public interface to the public zone and the private
interface to the private zone. Note down the security zone names.</p>
</li>
<li><p class="first">Make sure there is a security policy from the private zone to the
public zone that allows all traffic.</p>
</li>
<li><p class="first">Note the username and password of the account you want the CloudStack
software to log in to when it is programming rules.</p>
</li>
<li><p class="first">Make sure the &#8220;ssh&#8221; and &#8220;xnm-clear-text&#8221; system services are enabled.</p>
</li>
<li><p class="first">If traffic metering is desired:</p>
<ol class="arabic">
<li><p class="first">Create an incoming firewall filter and an outgoing firewall
filter. These filters should be the same names as your public
security zone name and private security zone name respectively.
The filters should be set to be &#8220;interface-specific&#8221;. For example,
here is the configuration where the public zone is &#8220;untrust&#8221; and
the private zone is &#8220;trust&#8221;:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>root@cloud-srx# show firewall
filter trust <span class="o">{</span>
    interface-specific<span class="p">;</span>
<span class="o">}</span>
filter untrust <span class="o">{</span>
    interface-specific<span class="p">;</span>
<span class="o">}</span>
</pre></div>
</div>
</li>
<li><p class="first">Add the firewall filters to your public interface. For example, a
sample configuration output (for public interface ge-0/0/3.0,
public security zone untrust, and private security zone trust) is:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>ge-0/0/3 <span class="o">{</span>
    unit <span class="m">0</span> <span class="o">{</span>
        family inet <span class="o">{</span>
            filter <span class="o">{</span>
                input untrust<span class="p">;</span>
                output trust<span class="p">;</span>
            <span class="o">}</span>
            address 172.25.0.252/16<span class="p">;</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
</li>
</ol>
</li>
<li><p class="first">Make sure all VLANs are brought to the private interface of the SRX.</p>
</li>
<li><p class="first">After the CloudStack Management Server is installed, log in to the
CloudStack UI as administrator.</p>
</li>
<li><p class="first">In the left navigation bar, click Infrastructure.</p>
</li>
<li><p class="first">In Zones, click View More.</p>
</li>
<li><p class="first">Choose the zone you want to work with.</p>
</li>
<li><p class="first">Click the Network tab.</p>
</li>
<li><p class="first">In the Network Service Providers node of the diagram, click
Configure. (You might have to scroll down to see this.)</p>
</li>
<li><p class="first">Click SRX.</p>
</li>
<li><p class="first">Click the Add New SRX button (+) and provide the following:</p>
<ul class="simple">
<li>IP Address: The IP address of the SRX.</li>
<li>Username: The user name of the account on the SRX that CloudStack
should use.</li>
<li>Password: The password of the account.</li>
<li>Public Interface. The name of the public interface on the SRX. For
example, ge-0/0/2. A &#8221;.x&#8221; at the end of the interface indicates
the VLAN that is in use.</li>
<li>Private Interface: The name of the private interface on the SRX.
For example, ge-0/0/1.</li>
<li>Usage Interface: (Optional) Typically, the public interface is
used to meter traffic. If you want to use a different interface,
specify its name here</li>
<li>Number of Retries: The number of times to attempt a command on the
SRX before failing. The default value is 2.</li>
<li>Timeout (seconds): The time to wait for a command on the SRX
before considering it failed. Default is 300 seconds.</li>
<li>Public Network: The name of the public network on the SRX. For
example, trust.</li>
<li>Private Network: The name of the private network on the SRX. For
example, untrust.</li>
<li>Capacity: The number of networks the device can handle</li>
<li>Dedicated: When marked as dedicated, this device will be dedicated
to a single account. When Dedicated is checked, the value in the
Capacity field has no significance implicitly, its value is 1</li>
</ul>
</li>
<li><p class="first">Click OK.</p>
</li>
<li><p class="first">Click Global Settings. Set the parameter
external.network.stats.interval to indicate how often you want
CloudStack to fetch network usage statistics from the Juniper SRX. If
you are not using the SRX to gather network usage statistics, set to 0.</p>
</li>
</ol>
</div>
<div class="section" id="external-guest-firewall-integration-for-cisco-vnmc-optional">
<h5>External Guest Firewall Integration for Cisco VNMC (Optional)<a class="headerlink" href="#external-guest-firewall-integration-for-cisco-vnmc-optional" title="Permalink to this headline">¶</a></h5>
<p>Cisco Virtual Network Management Center (VNMC) provides centralized
multi-device and policy management for Cisco Network Virtual Services.
You can integrate Cisco VNMC with CloudStack to leverage the firewall
and NAT service offered by ASA 1000v Cloud Firewall. Use it in a Cisco
Nexus 1000v dvSwitch-enabled cluster in CloudStack. In such a
deployment, you will be able to:</p>
<ul class="simple">
<li>Configure Cisco ASA 1000v firewalls. You can configure one per guest
network.</li>
<li>Use Cisco ASA 1000v firewalls to create and apply security profiles
that contain ACL policy sets for both ingress and egress traffic.</li>
<li>Use Cisco ASA 1000v firewalls to create and apply Source NAT, Port
Forwarding, and Static NAT policy sets.</li>
</ul>
<p>CloudStack supports Cisco VNMC on Cisco Nexus 1000v dvSwich-enabled
VMware hypervisors.</p>
<div class="section" id="using-cisco-asa-1000v-firewall-cisco-nexus-1000v-dvswitch-and-cisco-vnmc-in-a-deployment">
<h6>Using Cisco ASA 1000v Firewall, Cisco Nexus 1000v dvSwitch, and Cisco VNMC in a Deployment<a class="headerlink" href="#using-cisco-asa-1000v-firewall-cisco-nexus-1000v-dvswitch-and-cisco-vnmc-in-a-deployment" title="Permalink to this headline">¶</a></h6>
<div class="section" id="guidelines">
<h7>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h7>
<ul>
<li><p class="first">Cisco ASA 1000v firewall is supported only in Isolated Guest
Networks.</p>
</li>
<li><p class="first">Cisco ASA 1000v firewall is not supported on VPC.</p>
</li>
<li><p class="first">Cisco ASA 1000v firewall is not supported for load balancing.</p>
</li>
<li><p class="first">When a guest network is created with Cisco VNMC firewall provider, an
additional public IP is acquired along with the Source NAT IP. The
Source NAT IP is used for the rules, whereas the additional IP is
used to for the ASA outside interface. Ensure that this additional
public IP is not released. You can identify this IP as soon as the
network is in implemented state and before acquiring any further
public IPs. The additional IP is the one that is not marked as Source
NAT. You can find the IP used for the ASA outside interface by
looking at the Cisco VNMC used in your guest network.</p>
</li>
<li><p class="first">Use the public IP address range from a single subnet. You cannot add
IP addresses from different subnets.</p>
</li>
<li><p class="first">Only one ASA instance per VLAN is allowed because multiple VLANS
cannot be trunked to ASA ports. Therefore, you can use only one ASA
instance in a guest network.</p>
</li>
<li><p class="first">Only one Cisco VNMC per zone is allowed.</p>
</li>
<li><p class="first">Supported only in Inline mode deployment with load balancer.</p>
</li>
<li><p class="first">The ASA firewall rule is applicable to all the public IPs in the
guest network. Unlike the firewall rules created on virtual router, a
rule created on the ASA device is not tied to a specific public IP.</p>
</li>
<li><p class="first">Use a version of Cisco Nexus 1000v dvSwitch that support the vservice
command. For example: nexus-1000v.4.2.1.SV1.5.2b.bin</p>
<p>Cisco VNMC requires the vservice command to be available on the Nexus
switch to create a guest network in CloudStack.</p>
</li>
</ul>
</div>
<div class="section" id="prerequisites">
<h7>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h7>
<ol class="arabic">
<li><p class="first">Configure Cisco Nexus 1000v dvSwitch in a vCenter environment.</p>
<p>Create Port profiles for both internal and external network
interfaces on Cisco Nexus 1000v dvSwitch. Note down the inside port
profile, which needs to be provided while adding the ASA appliance to
CloudStack.</p>
<p>For information on configuration, see
<a class="reference external" href="hypervisor_installation.html#configuring-a-vsphere-cluster-with-nexus-1000v-virtual-switch">“Configuring a vSphere Cluster with Nexus 1000v Virtual Switch”</a>.</p>
</li>
<li><p class="first">Deploy and configure Cisco VNMC.</p>
<p>For more information, see
<a class="reference external" href="http://www.cisco.com/en/US/docs/switches/datacenter/vsg/sw/4_2_1_VSG_2_1_1/install_upgrade/guide/b_Cisco_VSG_for_VMware_vSphere_Rel_4_2_1_VSG_2_1_1_and_Cisco_VNMC_Rel_2_1_Installation_and_Upgrade_Guide_chapter_011.html">Installing Cisco Virtual Network Management Center</a>
and <a class="reference external" href="http://www.cisco.com/en/US/docs/unified_computing/vnmc/sw/1.2/VNMC_GUI_Configuration/b_VNMC_GUI_Configuration_Guide_1_2_chapter_010.html">Configuring Cisco Virtual Network Management Center</a>.</p>
</li>
<li><p class="first">Register Cisco Nexus 1000v dvSwitch with Cisco VNMC.</p>
<p>For more information, see <a class="reference external" href="http://www.cisco.com/en/US/docs/switches/datacenter/vsg/sw/4_2_1_VSG_1_2/vnmc_and_vsg_qi/guide/vnmc_vsg_install_5register.html#wp1064301">Registering a Cisco Nexus 1000V with Cisco VNMC</a>.</p>
</li>
<li><p class="first">Create Inside and Outside port profiles in Cisco Nexus 1000v dvSwitch.</p>
<p>For more information, see
<a class="reference external" href="hypervisor_installation.html#configuring-a-vsphere-cluster-with-nexus-1000v-virtual-switch">“Configuring a vSphere Cluster with Nexus 1000v Virtual Switch”</a>.</p>
</li>
<li><p class="first">Deploy and Cisco ASA 1000v appliance.</p>
<p>For more information, see <a class="reference external" href="http://www.cisco.com/en/US/docs/security/asa/quick_start/asa1000V/setup_vnmc.html">Setting Up the ASA 1000V Using VNMC</a>.</p>
<p>Typically, you create a pool of ASA 1000v appliances and register
them with CloudStack.</p>
<p>Specify the following while setting up a Cisco ASA 1000v instance:</p>
<ul class="simple">
<li>VNMC host IP.</li>
<li>Ensure that you add ASA appliance in VNMC mode.</li>
<li>Port profiles for the Management and HA network interfaces. This
need to be pre-created on Cisco Nexus 1000v dvSwitch.</li>
<li>Internal and external port profiles.</li>
<li>The Management IP for Cisco ASA 1000v appliance. Specify the
gateway such that the VNMC IP is reachable.</li>
<li>Administrator credentials</li>
<li>VNMC credentials</li>
</ul>
</li>
<li><p class="first">Register Cisco ASA 1000v with VNMC.</p>
<p>After Cisco ASA 1000v instance is powered on, register VNMC from the
ASA console.</p>
</li>
</ol>
</div>
<div class="section" id="using-cisco-asa-1000v-services">
<h7>Using Cisco ASA 1000v Services<a class="headerlink" href="#using-cisco-asa-1000v-services" title="Permalink to this headline">¶</a></h7>
<ol class="arabic">
<li><p class="first">Ensure that all the prerequisites are met.</p>
<p>See <a class="reference external" href="#prerequisites">“Prerequisites”</a>.</p>
</li>
<li><p class="first">Add a VNMC instance.</p>
<p>See <a class="reference external" href="#adding-a-vnmc-instance">“Adding a VNMC Instance”</a>.</p>
</li>
<li><p class="first">Add a ASA 1000v instance.</p>
<p>See <a class="reference external" href="#adding-an-asa-1000v-instance">“Adding an ASA 1000v Instance”</a>.</p>
</li>
<li><p class="first">Create a Network Offering and use Cisco VNMC as the service provider
for desired services.</p>
<p>See <a class="reference external" href="#creating-a-network-offering-using-cisco-asa-1000v">“Creating a Network Offering Using Cisco ASA 1000v”</a>.</p>
</li>
<li><p class="first">Create an Isolated Guest Network by using the network offering you
just created.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="id6">
<h6>Adding a VNMC Instance<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h6>
<ol class="arabic">
<li><p class="first">Log in to the CloudStack UI as administrator.</p>
</li>
<li><p class="first">In the left navigation bar, click Infrastructure.</p>
</li>
<li><p class="first">In Zones, click View More.</p>
</li>
<li><p class="first">Choose the zone you want to work with.</p>
</li>
<li><p class="first">Click the Physical Network tab.</p>
</li>
<li><p class="first">In the Network Service Providers node of the diagram, click
Configure.</p>
<p>You might have to scroll down to see this.</p>
</li>
<li><p class="first">Click Cisco VNMC.</p>
</li>
<li><p class="first">Click View VNMC Devices.</p>
</li>
<li><p class="first">Click the Add VNMC Device and provide the following:</p>
<ul class="simple">
<li>Host: The IP address of the VNMC instance.</li>
<li>Username: The user name of the account on the VNMC instance that
CloudStack should use.</li>
<li>Password: The password of the account.</li>
</ul>
</li>
<li><p class="first">Click OK.</p>
</li>
</ol>
</div>
<div class="section" id="id7">
<h6>Adding an ASA 1000v Instance<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h6>
<ol class="arabic">
<li><p class="first">Log in to the CloudStack UI as administrator.</p>
</li>
<li><p class="first">In the left navigation bar, click Infrastructure.</p>
</li>
<li><p class="first">In Zones, click View More.</p>
</li>
<li><p class="first">Choose the zone you want to work with.</p>
</li>
<li><p class="first">Click the Physical Network tab.</p>
</li>
<li><p class="first">In the Network Service Providers node of the diagram, click
Configure.</p>
<p>You might have to scroll down to see this.</p>
</li>
<li><p class="first">Click Cisco VNMC.</p>
</li>
<li><p class="first">Click View ASA 1000v.</p>
</li>
<li><p class="first">Click the Add CiscoASA1000v Resource and provide the following:</p>
<ul>
<li><p class="first"><strong>Host</strong>: The management IP address of the ASA 1000v instance. The
IP address is used to connect to ASA 1000V.</p>
</li>
<li><p class="first"><strong>Inside Port Profile</strong>: The Inside Port Profile configured on
Cisco Nexus1000v dvSwitch.</p>
</li>
<li><p class="first"><strong>Cluster</strong>: The VMware cluster to which you are adding the ASA
1000v instance.</p>
<p>Ensure that the cluster is Cisco Nexus 1000v dvSwitch enabled.</p>
</li>
</ul>
</li>
<li><p class="first">Click OK.</p>
</li>
</ol>
</div>
<div class="section" id="id8">
<h6>Creating a Network Offering Using Cisco ASA 1000v<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h6>
<p>To have Cisco ASA 1000v support for a guest network, create a network
offering as follows:</p>
<ol class="arabic">
<li><p class="first">Log in to the CloudStack UI as a user or admin.</p>
</li>
<li><p class="first">From the Select Offering drop-down, choose Network Offering.</p>
</li>
<li><p class="first">Click Add Network Offering.</p>
</li>
<li><p class="first">In the dialog, make the following choices:</p>
<ul class="simple">
<li><strong>Name</strong>: Any desired name for the network offering.</li>
<li><strong>Description</strong>: A short description of the offering that can be
displayed to users.</li>
<li><strong>Network Rate</strong>: Allowed data transfer rate in MB per second.</li>
<li><strong>Traffic Type</strong>: The type of network traffic that will be carried
on the network.</li>
<li><strong>Guest Type</strong>: Choose whether the guest network is isolated or
shared.</li>
<li><strong>Persistent</strong>: Indicate whether the guest network is persistent
or not. The network that you can provision without having to
deploy a VM on it is termed persistent network.</li>
<li><strong>VPC</strong>: This option indicate whether the guest network is Virtual
Private Cloud-enabled. A Virtual Private Cloud (VPC) is a private,
isolated part of CloudStack. A VPC can have its own virtual
network topology that resembles a traditional physical network.
For more information on VPCs, see <a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-administration/en/latest/networking2.html#about-virtual-private-clouds">“About Virtual Private Clouds”</a>.</li>
<li><strong>Specify VLAN</strong>: (Isolated guest networks only) Indicate whether
a VLAN should be specified when this offering is used.</li>
<li><strong>Supported Services</strong>: Use Cisco VNMC as the service provider for
Firewall, Source NAT, Port Forwarding, and Static NAT to create an
Isolated guest network offering.</li>
<li><strong>System Offering</strong>: Choose the system service offering that you
want virtual routers to use in this network.</li>
<li><strong>Conserve mode</strong>: Indicate whether to use conserve mode. In this
mode, network resources are allocated only when the first virtual
machine starts in the network.</li>
</ul>
</li>
<li><p class="first">Click OK</p>
<p>The network offering is created.</p>
</li>
</ol>
</div>
<div class="section" id="reusing-asa-1000v-appliance-in-new-guest-networks">
<h6>Reusing ASA 1000v Appliance in new Guest Networks<a class="headerlink" href="#reusing-asa-1000v-appliance-in-new-guest-networks" title="Permalink to this headline">¶</a></h6>
<p>You can reuse an ASA 1000v appliance in a new guest network after the
necessary cleanup. Typically, ASA 1000v is cleaned up when the logical
edge firewall is cleaned up in VNMC. If this cleanup does not happen,
you need to reset the appliance to its factory settings for use in new
guest networks. As part of this, enable SSH on the appliance and store
the SSH credentials by registering on VNMC.</p>
<ol class="arabic">
<li><p class="first">Open a command line on the ASA appliance:</p>
<ol class="arabic">
<li><p class="first">Run the following:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>ASA1000V<span class="o">(</span>config<span class="o">)</span><span class="c1"># reload</span>
</pre></div>
</div>
<p>You are prompted with the following message:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>System config has been modified. Save? <span class="o">[</span>Y<span class="o">]</span>es/<span class="o">[</span>N<span class="o">]</span>o:<span class="s2">&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">Enter N.</p>
<p>You will get the following confirmation message:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="s2">&quot;Proceed with reload? [confirm]&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">Restart the appliance.</p>
</li>
</ol>
</li>
<li><p class="first">Register the ASA 1000v appliance with the VNMC:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>ASA1000V<span class="o">(</span>config<span class="o">)</span><span class="c1"># vnmc policy-agent</span>
ASA1000V<span class="o">(</span>config-vnmc-policy-agent<span class="o">)</span><span class="c1"># registration host vnmc_ip_address</span>
ASA1000V<span class="o">(</span>config-vnmc-policy-agent<span class="o">)</span><span class="c1"># shared-secret key where key is the shared secret for authentication of the ASA 1000V connection to the Cisco VNMC</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="external-guest-load-balancer-integration-optional">
<h5>External Guest Load Balancer Integration (Optional)<a class="headerlink" href="#external-guest-load-balancer-integration-optional" title="Permalink to this headline">¶</a></h5>
<p>CloudStack can optionally use a Citrix NetScaler or BigIP F5 load
balancer to provide load balancing services to guests. If this is not
enabled, CloudStack will use the software load balancer in the virtual
router.</p>
<p>To install and enable an external load balancer for CloudStack
management:</p>
<ol class="arabic">
<li><p class="first">Set up the appliance according to the vendor&#8217;s directions.</p>
</li>
<li><p class="first">Connect it to the networks carrying public traffic and management
traffic (these could be the same network).</p>
</li>
<li><p class="first">Record the IP address, username, password, public interface name, and
private interface name. The interface names will be something like
&#8220;1.1&#8221; or &#8220;1.2&#8221;.</p>
</li>
<li><p class="first">Make sure that the VLANs are trunked to the management network
interface.</p>
</li>
<li><p class="first">After the CloudStack Management Server is installed, log in as
administrator to the CloudStack UI.</p>
</li>
<li><p class="first">In the left navigation bar, click Infrastructure.</p>
</li>
<li><p class="first">In Zones, click View More.</p>
</li>
<li><p class="first">Choose the zone you want to work with.</p>
</li>
<li><p class="first">Click the Network tab.</p>
</li>
<li><p class="first">In the Network Service Providers node of the diagram, click
Configure. (You might have to scroll down to see this.)</p>
</li>
<li><p class="first">Click NetScaler or F5.</p>
</li>
<li><p class="first">Click the Add button (+) and provide the following:</p>
<p>For NetScaler:</p>
<ul class="simple">
<li>IP Address: The IP address of the SRX.</li>
<li>Username/Password: The authentication credentials to access the
device. CloudStack uses these credentials to access the device.</li>
<li>Type: The type of device that is being added. It could be F5 Big
Ip Load Balancer, NetScaler VPX, NetScaler MPX, or NetScaler SDX.
For a comparison of the NetScaler types, see the CloudStack
Administration Guide.</li>
<li>Public interface: Interface of device that is configured to be
part of the public network.</li>
<li>Private interface: Interface of device that is configured to be
part of the private network.</li>
<li>Number of retries. Number of times to attempt a command on the
device before considering the operation failed. Default is 2.</li>
<li>Capacity: The number of networks the device can handle.</li>
<li>Dedicated: When marked as dedicated, this device will be dedicated
to a single account. When Dedicated is checked, the value in the
Capacity field has no significance implicitly, its value is 1.</li>
</ul>
</li>
<li><p class="first">Click OK.</p>
</li>
</ol>
<p>The installation and provisioning of the external load balancer is
finished. You can proceed to add VMs and NAT or load balancing rules.</p>
</div>
</div>
<div class="section" id="management-server-load-balancing">
<h4>Management Server Load Balancing<a class="headerlink" href="#management-server-load-balancing" title="Permalink to this headline">¶</a></h4>
<p>CloudStack can use a load balancer to provide a virtual IP for multiple
Management Servers. The administrator is responsible for creating the
load balancer rules for the Management Servers. The application requires
persistence or stickiness across multiple sessions. The following chart
lists the ports that should be load balanced and whether or not
persistence is required.</p>
<p>Even if persistence is not required, enabling it is permitted.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="16%" />
<col width="35%" />
<col width="19%" />
<col width="30%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Source Port</th>
<th class="head">Destination Port</th>
<th class="head">Protocol</th>
<th class="head">Persistence Required?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>80 or 443</td>
<td>8080 (or 20400 with AJP)</td>
<td>HTTP (or AJP)</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td>8250</td>
<td>8250</td>
<td>TCP</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td>8096</td>
<td>8096</td>
<td>HTTP</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>In addition to above settings, the administrator is responsible for
setting the &#8216;host&#8217; global config value from the management server IP to
load balancer virtual IP address. If the &#8216;host&#8217; value is not set to the
VIP for Port 8250 and one of your management servers crashes, the UI is
still available but the system VMs will not be able to contact the
management server.</p>
</div>
<div class="section" id="topology-requirements">
<h4>Topology Requirements<a class="headerlink" href="#topology-requirements" title="Permalink to this headline">¶</a></h4>
<div class="section" id="security-requirements">
<h5>Security Requirements<a class="headerlink" href="#security-requirements" title="Permalink to this headline">¶</a></h5>
<p>The public Internet must not be able to access port 8096 or port 8250 on
the Management Server.</p>
</div>
<div class="section" id="runtime-internal-communications-requirements">
<h5>Runtime Internal Communications Requirements<a class="headerlink" href="#runtime-internal-communications-requirements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>The Management Servers communicate with each other to coordinate
tasks. This communication uses TCP on ports 8250 and 9090.</li>
<li>The console proxy VMs connect to all hosts in the zone over the
management traffic network. Therefore the management traffic network
of any given pod in the zone must have connectivity to the management
traffic network of all other pods in the zone.</li>
<li>The secondary storage VMs and console proxy VMs connect to the
Management Server on port 8250. If you are using multiple Management
Servers, the load balanced IP address of the Management Servers on
port 8250 must be reachable.</li>
</ul>
</div>
<div class="section" id="storage-network-topology-requirements">
<h5>Storage Network Topology Requirements<a class="headerlink" href="#storage-network-topology-requirements" title="Permalink to this headline">¶</a></h5>
<p>The secondary storage NFS export is mounted by the secondary storage VM.
Secondary storage traffic goes over the management traffic network, even
if there is a separate storage network. Primary storage traffic goes
over the storage network, if available. If you choose to place secondary
storage NFS servers on the storage network, you must make sure there is
a route from the management traffic network to the storage network.</p>
</div>
<div class="section" id="external-firewall-topology-requirements">
<h5>External Firewall Topology Requirements<a class="headerlink" href="#external-firewall-topology-requirements" title="Permalink to this headline">¶</a></h5>
<p>When external firewall integration is in place, the public IP VLAN must
still be trunked to the Hosts. This is required to support the Secondary
Storage VM and Console Proxy VM.</p>
</div>
<div class="section" id="advanced-zone-topology-requirements">
<h5>Advanced Zone Topology Requirements<a class="headerlink" href="#advanced-zone-topology-requirements" title="Permalink to this headline">¶</a></h5>
<p>With Advanced Networking, separate subnets must be used for private and
public networks.</p>
</div>
<div class="section" id="xenserver-topology-requirements">
<h5>XenServer Topology Requirements<a class="headerlink" href="#xenserver-topology-requirements" title="Permalink to this headline">¶</a></h5>
<p>The Management Servers communicate with XenServer hosts on ports 22
(ssh), 80 (HTTP), and 443 (HTTPs).</p>
</div>
<div class="section" id="vmware-topology-requirements">
<h5>VMware Topology Requirements<a class="headerlink" href="#vmware-topology-requirements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>The Management Server and secondary storage VMs must be able to
access vCenter and all ESXi hosts in the zone. To allow the necessary
access through the firewall, keep port 443 open.</li>
<li>The Management Servers communicate with VMware vCenter servers on
port 443 (HTTPs).</li>
<li>The Management Servers communicate with the System VMs on port 3922
(ssh) on the management traffic network.</li>
</ul>
</div>
<div class="section" id="hyper-v-topology-requirements">
<h5>Hyper-V Topology Requirements<a class="headerlink" href="#hyper-v-topology-requirements" title="Permalink to this headline">¶</a></h5>
<p>CloudStack Management Server communicates with Hyper-V Agent by using
HTTPS. For secure communication between the Management Server and the
Hyper-V host, open port 8250.</p>
</div>
<div class="section" id="kvm-topology-requirements">
<h5>KVM Topology Requirements<a class="headerlink" href="#kvm-topology-requirements" title="Permalink to this headline">¶</a></h5>
<p>The Management Servers communicate with KVM hosts on port 22 (ssh).</p>
</div>
<div class="section" id="lxc-topology-requirements">
<h5>LXC Topology Requirements<a class="headerlink" href="#lxc-topology-requirements" title="Permalink to this headline">¶</a></h5>
<p>The Management Servers communicate with LXC hosts on port 22 (ssh).</p>
</div>
</div>
<div class="section" id="guest-network-usage-integration-for-traffic-sentinel">
<h4>Guest Network Usage Integration for Traffic Sentinel<a class="headerlink" href="#guest-network-usage-integration-for-traffic-sentinel" title="Permalink to this headline">¶</a></h4>
<p>To collect usage data for a guest network, CloudStack needs to pull the
data from an external network statistics collector installed on the
network. Metering statistics for guest networks are available through
CloudStack’s integration with inMon Traffic Sentinel.</p>
<p>Traffic Sentinel is a network traffic usage data collection package.
CloudStack can feed statistics from Traffic Sentinel into its own usage
records, providing a basis for billing users of cloud infrastructure.
Traffic Sentinel uses the traffic monitoring protocol sFlow. Routers
and switches generate sFlow records and provide them for collection by
Traffic Sentinel, then CloudStack queries the Traffic Sentinel database
to obtain this information</p>
<p>To construct the query, CloudStack determines what guest IPs were in use
during the current query interval. This includes both newly assigned IPs
and IPs that were assigned in a previous time period and continued to be
in use. CloudStack queries Traffic Sentinel for network statistics that
apply to these IPs during the time period they remained allocated in
CloudStack. The returned data is correlated with the customer account
that owned each IP and the timestamps when IPs were assigned and
released in order to create billable metering records in CloudStack.
When the Usage Server runs, it collects this data.</p>
<p>To set up the integration between CloudStack and Traffic Sentinel:</p>
<ol class="arabic">
<li><p class="first">On your network infrastructure, install Traffic Sentinel and
configure it to gather traffic data. For installation and
configuration steps, see inMon documentation at
<a class="reference external" href="http://inmon.com.">Traffic Sentinel Documentation</a>.</p>
</li>
<li><p class="first">In the Traffic Sentinel UI, configure Traffic Sentinel to accept
script querying from guest users. CloudStack will be the guest user
performing the remote queries to gather network usage for one or more
IP addresses.</p>
<p>Click File &gt; Users &gt; Access Control &gt; Reports Query, then select
Guest from the drop-down list.</p>
</li>
<li><p class="first">On CloudStack, add the Traffic Sentinel host by calling the
CloudStack API command addTrafficMonitor. Pass in the URL of the
Traffic Sentinel as protocol + host + port (optional); for example,
<a class="reference external" href="http://10.147.28.100:8080">http://10.147.28.100:8080</a>. For the addTrafficMonitor command syntax,
see the API Reference at <a class="reference external" href="http://cloudstack.apache.org/docs/api/index.html">API Documentation</a>.</p>
<p>For information about how to call the CloudStack API, see the
Developer’s Guide at <a class="reference external" href="http://docs.cloudstack.apache.org/en/latest/index.html#developers">CloudStack API Developer&#8217;s Guide</a>.</p>
</li>
<li><p class="first">Log in to the CloudStack UI as administrator.</p>
</li>
<li><p class="first">Select Configuration from the Global Settings page, and set the
following:</p>
<p>direct.network.stats.interval: How often you want CloudStack to query
Traffic Sentinel.</p>
</li>
</ol>
</div>
<div class="section" id="setting-zone-vlan-and-running-vm-maximums">
<h4>Setting Zone VLAN and Running VM Maximums<a class="headerlink" href="#setting-zone-vlan-and-running-vm-maximums" title="Permalink to this headline">¶</a></h4>
<p>In the external networking case, every VM in a zone must have a unique
guest IP address. There are two variables that you need to consider in
determining how to configure CloudStack to support this: how many Zone
VLANs do you expect to have and how many VMs do you expect to have
running in the Zone at any one time.</p>
<p>Use the following table to determine how to configure CloudStack for
your deployment.</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="25%" />
<col width="46%" />
<col width="30%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">guest.vlan.bits</th>
<th class="head">Maximum Running VMs per Zone</th>
<th class="head">Maximum Zone VLANs</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>12</td>
<td>4096</td>
<td>4094</td>
</tr>
<tr class="row-odd"><td>11</td>
<td>8192</td>
<td>2048</td>
</tr>
<tr class="row-even"><td>10</td>
<td>16384</td>
<td>1024</td>
</tr>
<tr class="row-odd"><td>10</td>
<td>32768</td>
<td>512</td>
</tr>
</tbody>
</table>
<p>Based on your deployment&#8217;s needs, choose the appropriate value of
guest.vlan.bits. Set it as described in Edit the Global Configuration
Settings (Optional) section and restart the Management Server.</p>
</div>
</div>
</div>
</div>
<div class="section" id="storage-setup">
<span id="storage"></span><h2>Storage Setup<a class="headerlink" href="#storage-setup" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-storage_setup"></span><div class="section" id="storage-setup">
<h3>Storage Setup<a class="headerlink" href="#storage-setup" title="Permalink to this headline">¶</a></h3>
<div class="section" id="primary-storage">
<h4>Primary Storage<a class="headerlink" href="#primary-storage" title="Permalink to this headline">¶</a></h4>
<p>CloudStack is designed to work with a wide variety of commodity and enterprise-rated storage systems.
CloudStack can also leverage the local disks within the hypervisor hosts if supported by the selected
hypervisor. Storage type support for guest virtual disks differs based on hypervisor selection.</p>
<table border="1" class="docutils">
<colgroup>
<col width="14%" />
<col width="31%" />
<col width="19%" />
<col width="36%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Storage Type</th>
<th class="head">XenServer</th>
<th class="head">vSphere</th>
<th class="head">KVM</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>NFS</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported</td>
</tr>
<tr class="row-odd"><td>iSCSI</td>
<td>Supported</td>
<td>Supported via VMFS</td>
<td>Supported via Clustered Filesystems</td>
</tr>
<tr class="row-even"><td>Fiber Channel</td>
<td>Supported via Pre-existing SR</td>
<td>Supported</td>
<td>Supported via Clustered Filesystems</td>
</tr>
<tr class="row-odd"><td>Local Disk</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported</td>
</tr>
</tbody>
</table>
<p>The use of the Cluster Logical Volume Manager (CLVM) for KVM is not officially supported with
CloudStack.</p>
</div>
<div class="section" id="secondary-storage">
<h4>Secondary Storage<a class="headerlink" href="#secondary-storage" title="Permalink to this headline">¶</a></h4>
<p>CloudStack is designed to work with any scalable secondary storage system. The only requirement is
that the secondary storage system supports the NFS protocol. For large, multi-zone deployments, S3
compatible storage is also supported for secondary storage. This allows for secondary storage which can
span an entire region, however an NFS staging area must be maintained in each zone as most hypervisors
are not capable of directly mounting S3 type storage.</p>
</div>
</div>
<div class="section" id="small-scale-setup">
<h3>Small-Scale Setup<a class="headerlink" href="#small-scale-setup" title="Permalink to this headline">¶</a></h3>
<p>In a small-scale setup, a single NFS server can function as both primary and secondary storage. The NFS
server must export two separate shares, one for primary storage and the other for secondary storage. This
could be a VM or physical host running an NFS service on a Linux OS or a virtual software appliance. Disk
and network performance are still important in a small scale setup to get a good experience when deploying,
running or snapshotting VMs.</p>
</div>
<div class="section" id="large-scale-setup">
<h3>Large-Scale Setup<a class="headerlink" href="#large-scale-setup" title="Permalink to this headline">¶</a></h3>
<p>In large-scale environments primary and secondary storage typically consist of independent physical storage arrays.</p>
<p>Primary storage is likely to have to support mostly random read/write I/O once a template has been
deployed.  Secondary storage is only going to experience sustained sequential reads or writes.</p>
<p>In clouds which will experience a large number of users taking snapshots or deploying VMs at the
same time, secondary storage performance will be important to maintain a good user experience.</p>
<p>It is important to start the design of your storage with the a rough profile of the workloads which it will
be required to support. Care should be taken to consider the IOPS demands of your guest VMs as much as the
volume of data to be stored and the bandwidth (MB/s) available at the storage interfaces.</p>
</div>
<div class="section" id="storage-architecture">
<h3>Storage Architecture<a class="headerlink" href="#storage-architecture" title="Permalink to this headline">¶</a></h3>
<p>There are many different storage types available which are generally suitable for CloudStack environments.
Specific use cases should be considered when deciding the best one for your environment and financial
constraints often make the &#8216;perfect&#8217; storage architecture economically unrealistic.</p>
<p>Broadly, the architectures of the available primary storage types can be split into 3 types:</p>
<div class="section" id="local-storage">
<h4>Local Storage<a class="headerlink" href="#local-storage" title="Permalink to this headline">¶</a></h4>
<p>Local storage works best for pure &#8216;cloud-era&#8217; workloads which rarely need to be migrated between storage
pools and where HA of individual VMs is not required. As SSDs become more mainstream/affordable, local
storage based VMs can now be served with the size of IOPS which previously could only be generated by
large arrays with 10s of spindles. Local storage is highly scalable because as you add hosts you would
add the same proportion of storage. Local Storage is relatively inefficent as it can not take advantage
of linked clones or any deduplication.</p>
</div>
<div class="section" id="traditional-node-based-shared-storage">
<h4>&#8216;Traditional&#8217; node-based Shared Storage<a class="headerlink" href="#traditional-node-based-shared-storage" title="Permalink to this headline">¶</a></h4>
<p>Traditional node-based storage are arrays which consist of a controller/controller pair attached to a
number of disks in shelves.
Ideally a cloud architecture would have one of these physical arrays per CloudStack pod to limit the
&#8216;blast-radius&#8217; of a failure to a single pod.  This is often not economically viable, however one should
look to try to reduce the scale of any incident relative to any zone with any single array where
possible.
The use of shared storage enables workloads to be immediately restarted on an alternate host should a
host fail. These shared storage arrays often have the ability to create &#8216;tiers&#8217; of storage utilising
say large SATA disks, 15k SAS disks and SSDs. These differently performing tiers can then be presented as
different offerings to users.
The sizing of an array should take into account the IOPS required by the workload as well as the volume
of data to be stored.  One should also consider the number of VMs which a storage array will be expected
to support, and the maximum network bandwidth possible through the controllers.</p>
</div>
<div class="section" id="clustered-shared-storage">
<h4>Clustered Shared Storage<a class="headerlink" href="#clustered-shared-storage" title="Permalink to this headline">¶</a></h4>
<p>Clustered shared storage arrays are the new generation of storage which do not have a single set of
interfaces where data enters and exits the array.  Instead it is distributed between all of the active
nodes giving greatly improved scalability and performance.  Some shared storage arrays enable all data
to continue to be accessible even in the event of the loss of an entire node.</p>
<p>The network topology should be carefully considered when using clustered shared storage to avoid creating
bottlenecks in the network fabric.</p>
</div>
</div>
<div class="section" id="network-configuration-for-storage">
<h3>Network Configuration For Storage<a class="headerlink" href="#network-configuration-for-storage" title="Permalink to this headline">¶</a></h3>
<p>Care should be taken when designing your cloud to take into consideration not only the performance
of your disk arrays but also the bandwidth available to move that traffic between the switch fabric and
the array interfaces.</p>
<div class="section" id="cloudstack-networking-for-storage">
<h4>CloudStack Networking For Storage<a class="headerlink" href="#cloudstack-networking-for-storage" title="Permalink to this headline">¶</a></h4>
<p>The first thing to understand is the process of provisioning primary storage. When you create a primary
storage pool for any given cluster, the CloudStack management server tells each hosts’ hypervisor to
mount the NFS share or (iSCSI LUN). The storage pool will be presented within the hypervisor as a
datastore (VMware), storage repository (XenServer/XCP) or a mount point (KVM), the important point is
that it is the hypervisor itself that communicates with the primary storage, the CloudStack management
server only communicates with the host hypervisor. Now, all hypervisors communicate with the outside
world via some kind of management interface – think VMKernel port on ESXi or ‘Management Interface’ on
XenServer. As the CloudStack management server needs to communicate with the hypervisor in the host,
this management interface must be on the CloudStack ‘management’ or ‘private’ network.  There may be
other interfaces configured on your host carrying guest and public traffic to/from VMs within the hosts
but the hypervisor itself doesn’t/can’t communicate over these interfaces.</p>
<p><img alt="hypervisor storage communication" src="_images/hypervisorcomms.png" />
<em>Figure 1</em>: Hypervisor communications</p>
<p>Separating Primary Storage traffic
For those from a pure virtualisation background, the concept of creating a specific interface for storage
traffic will not be new; it has long been best practice for iSCSI traffic to have a dedicated switch
fabric to avoid any latency or contention issues.
Sometimes in the cloud(Stack) world we forget that we are simply orchestrating processes that the
hypervisors already carry out and that many ‘normal’ hypervisor configurations still apply.
The logical reasoning which explains how this splitting of traffic works is as follows:</p>
<ol class="arabic simple">
<li>If you want an additional interface over which the hypervisor can communicate (excluding teamed or bonded interfaces) you need to give it an IP address.</li>
<li>The mechanism to create an additional interface that the hypervisor can use is to create an additional management interface</li>
<li>So that the hypervisor can differentiate between the management interfaces they have to be in different (non-overlapping) subnets</li>
<li>In order for the ‘primary storage’ management interface to communicate with the primary storage, the interfaces on the primary storage arrays must be in the same CIDR as the ‘primary storage’ management interface.</li>
<li>Therefore the primary storage must be in a different subnet to the management network</li>
</ol>
<p><img alt="subnetted storage interfaces" src="_images/subnetting_storage.png" />
<em>Figure 2</em>: Subnetting of Storage Traffic</p>
<p><img alt="hypervisor communications to secondary storage" src="_images/hypervisorcomms-secstorage.png" />
<em>Figure 3</em>: Hypervisor Communications with Separated Storage Traffic</p>
<p>Other Primary Storage Types
If you are using PreSetup or SharedMountPoints to connect to IP based storage then the same principles
apply; if the primary storage and ‘primary storage interface’ are in a different subnet to the ‘management
subnet’ then the hypervisor will use the ‘primary storage interface’ to communicate with the primary
storage.</p>
</div>
<div class="section" id="small-scale-example-configurations">
<h4>Small-Scale Example Configurations<a class="headerlink" href="#small-scale-example-configurations" title="Permalink to this headline">¶</a></h4>
<p>In this section we go through a few examples of how to set up storage to
work properly on a few types of NFS and iSCSI storage systems.</p>
<div class="section" id="linux-nfs-on-local-disks-and-das">
<h5>Linux NFS on Local Disks and DAS<a class="headerlink" href="#linux-nfs-on-local-disks-and-das" title="Permalink to this headline">¶</a></h5>
<p>This section describes how to configure an NFS export on a standard
Linux installation. The exact commands might vary depending on the
operating system version.</p>
<ol class="arabic">
<li><p class="first">Install the RHEL/CentOS distribution on the storage server.</p>
</li>
<li><p class="first">If the root volume is more than 2 TB in size, create a smaller boot
volume to install RHEL/CentOS. A root volume of 20 GB should be
sufficient.</p>
</li>
<li><p class="first">After the system is installed, create a directory called /export.
This can each be a directory in the root partition itself or a mount
point for a large disk volume.</p>
</li>
<li><p class="first">If you have more than 16TB of storage on one host, create multiple
EXT3 file systems and multiple NFS exports. Individual EXT3 file
systems cannot exceed 16TB.</p>
</li>
<li><p class="first">After /export directory is created, run the following command to
configure it as an NFS export.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># echo &quot;/export &lt;CIDR&gt;(rw,async,no_root_squash,no_subtree_check)&quot; &gt;&gt; /etc/exports</span>
</pre></div>
</div>
<p>Adjust the above command to suit your deployment needs.</p>
</li>
</ol>
<ul class="simple">
<li><strong>Limiting NFS export.</strong> It is highly recommended that you limit the NFS export to a particular subnet by specifying a subnet mask (e.g.,”192.168.1.0/24”). By allowing access from only within the expected cluster, you avoid having non-pool member mount the storage. The limit you place must include the management network(s) and the storage network(s). If the two are the same network then one CIDR is sufficient. If you have a separate storage network you must provide separate CIDR’s for both or one CIDR that is broad enough to span both.</li>
</ul>
<blockquote>
<div><p>The following is an example with separate CIDRs:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/export 192.168.1.0/24<span class="o">(</span>rw,async,no_root_squash,no_subtree_check<span class="o">)</span> 10.50.1.0/24<span class="o">(</span>rw,async,no_root_squash,no_subtree_check<span class="o">)</span>
</pre></div>
</div>
</div></blockquote>
<ul class="simple">
<li><strong>Removing the async flag.</strong> The async flag improves performance by allowing the NFS server to respond before writes are committed to the disk. Remove the async flag in your mission critical production deployment.</li>
</ul>
<ol class="arabic" start="6">
<li><p class="first">Run the following command to enable NFS service.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># chkconfig nfs on</span>
</pre></div>
</div>
</li>
<li><p class="first">Edit the /etc/sysconfig/nfs file and uncomment the following lines.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">LOCKD_TCPPORT</span><span class="o">=</span>32803
<span class="nv">LOCKD_UDPPORT</span><span class="o">=</span>32769
<span class="nv">MOUNTD_PORT</span><span class="o">=</span>892
<span class="nv">RQUOTAD_PORT</span><span class="o">=</span>875
<span class="nv">STATD_PORT</span><span class="o">=</span>662
<span class="nv">STATD_OUTGOING_PORT</span><span class="o">=</span>2020
</pre></div>
</div>
</li>
<li><p class="first">Edit the /etc/sysconfig/iptables file and add the following lines at
the beginning of the INPUT chain.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>-A INPUT -m state --state NEW -p udp --dport <span class="m">111</span> -j ACCEPT
-A INPUT -m state --state NEW -p tcp --dport <span class="m">111</span> -j ACCEPT
-A INPUT -m state --state NEW -p tcp --dport <span class="m">2049</span> -j ACCEPT
-A INPUT -m state --state NEW -p tcp --dport <span class="m">32803</span> -j ACCEPT
-A INPUT -m state --state NEW -p udp --dport <span class="m">32769</span> -j ACCEPT
-A INPUT -m state --state NEW -p tcp --dport <span class="m">892</span> -j ACCEPT
-A INPUT -m state --state NEW -p udp --dport <span class="m">892</span> -j ACCEPT
-A INPUT -m state --state NEW -p tcp --dport <span class="m">875</span> -j ACCEPT
-A INPUT -m state --state NEW -p udp --dport <span class="m">875</span> -j ACCEPT
-A INPUT -m state --state NEW -p tcp --dport <span class="m">662</span> -j ACCEPT
-A INPUT -m state --state NEW -p udp --dport <span class="m">662</span> -j ACCEPT
</pre></div>
</div>
</li>
<li><p class="first">Reboot the server.</p>
<p>An NFS share called /export is now set up.</p>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When copying and pasting a command, be sure the command has pasted as a single line before executing. Some document viewers may introduce unwanted line breaks in copied text.</p>
</div>
</div>
<div class="section" id="linux-nfs-on-iscsi">
<h5>Linux NFS on iSCSI<a class="headerlink" href="#linux-nfs-on-iscsi" title="Permalink to this headline">¶</a></h5>
<p>Use the following steps to set up a Linux NFS server export on an iSCSI
volume. These steps apply to RHEL/CentOS 5 distributions.</p>
<ol class="arabic">
<li><p class="first">Install iscsiadm.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum install iscsi-initiator-utils</span>
<span class="c1"># service iscsi start</span>
<span class="c1"># chkconfig --add iscsi</span>
<span class="c1"># chkconfig iscsi on</span>
</pre></div>
</div>
</li>
<li><p class="first">Discover the iSCSI target.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># iscsiadm -m discovery -t st -p &lt;iSCSI Server IP address&gt;:3260</span>
</pre></div>
</div>
<p>For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># iscsiadm -m discovery -t st -p 172.23.10.240:3260 172.23.10.240:3260,1 iqn.2001-05.com.equallogic:0-8a0906-83bcb3401-16e0002fd0a46f3d-rhel5-test</span>
</pre></div>
</div>
</li>
<li><p class="first">Log in.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># iscsiadm -m node -T &lt;Complete Target Name&gt; -l -p &lt;Group IP&gt;:3260</span>
</pre></div>
</div>
<p>For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># iscsiadm -m node -l -T iqn.2001-05.com.equallogic:83bcb3401-16e0002fd0a46f3d-rhel5-test -p 172.23.10.240:3260</span>
</pre></div>
</div>
</li>
<li><p class="first">Discover the SCSI disk. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># iscsiadm -m session -P3 | grep Attached</span>
Attached scsi disk sdb State: running
</pre></div>
</div>
</li>
<li><p class="first">Format the disk as ext3 and mount the volume.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mkfs.ext3 /dev/sdb</span>
<span class="c1"># mkdir -p /export</span>
<span class="c1"># mount /dev/sdb /export</span>
</pre></div>
</div>
</li>
<li><p class="first">Add the disk to /etc/fstab to make sure it gets mounted on boot.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/dev/sdb /export ext3 _netdev <span class="m">0</span> 0
</pre></div>
</div>
</li>
</ol>
<p>Now you can set up /export as an NFS share.</p>
<ul>
<li><p class="first"><strong>Limiting NFS export.</strong> In order to avoid data loss, it is highly
recommended that you limit the NFS export to a particular subnet by
specifying a subnet mask (e.g.,”192.168.1.0/24”). By allowing access
from only within the expected cluster, you avoid having non-pool
member mount the storage and inadvertently delete all its data. The
limit you place must include the management network(s) and the
storage network(s). If the two are the same network then one CIDR is
sufficient. If you have a separate storage network you must provide
separate CIDRs for both or one CIDR that is broad enough to span
both.</p>
<p>The following is an example with separate CIDRs:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>/export 192.168.1.0/24<span class="o">(</span>rw,async,no_root_squash,no_subtree_check<span class="o">)</span> 10.50.1.0/24<span class="o">(</span>rw,async,no_root_squash,no_subtree_check<span class="o">)</span>
</pre></div>
</div>
</li>
<li><p class="first"><strong>Removing the async flag.</strong> The async flag improves performance by
allowing the NFS server to respond before writes are committed to the
disk. Remove the async flag in your mission critical production
deployment.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="optional-installation">
<h2>Optional Installation<a class="headerlink" href="#optional-installation" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-optional_installation"></span><div class="section" id="additional-installation-options">
<h3>Additional Installation Options<a class="headerlink" href="#additional-installation-options" title="Permalink to this headline">¶</a></h3>
<p>The next few sections describe CloudStack features above and beyond the
basic deployment options.</p>
<div class="section" id="installing-the-usage-server-optional">
<h4>Installing the Usage Server (Optional)<a class="headerlink" href="#installing-the-usage-server-optional" title="Permalink to this headline">¶</a></h4>
<p>You can optionally install the Usage Server once the Management Server
is configured properly. The Usage Server takes data from the events in
the system and enables usage-based billing for accounts.</p>
<p>When multiple Management Servers are present, the Usage Server may be
installed on any number of them. The Usage Servers will coordinate usage
processing. A site that is concerned about availability should install
Usage Servers on at least two Management Servers.</p>
<div class="section" id="requirements-for-installing-the-usage-server">
<h5>Requirements for Installing the Usage Server<a class="headerlink" href="#requirements-for-installing-the-usage-server" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>The Management Server must be running when the Usage Server is
installed.</li>
<li>The Usage Server must be installed on the same server as a Management
Server.</li>
</ul>
</div>
<div class="section" id="steps-to-install-the-usage-server">
<h5>Steps to Install the Usage Server<a class="headerlink" href="#steps-to-install-the-usage-server" title="Permalink to this headline">¶</a></h5>
<ol class="arabic">
<li><p class="first">Package repository should already being configured. Refer to
<a class="reference external" href="http://cloudstack-installation.readthedocs.org/en/latest/installation.html#configure-package-repository">Configure Package Repository</a></p>
</li>
<li><p class="first">Install package cloudstack-usage</p>
<p>On RHEL/CentOS systems, use:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum install cloudstack-usage</span>
</pre></div>
</div>
<p>On Debian/Ubuntu systems, use:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># apt-get install cloudstack-usage</span>
</pre></div>
</div>
</li>
<li><p class="first">Once installed, start the Usage Server with the following command.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service cloudstack-usage start</span>
</pre></div>
</div>
</li>
<li><p class="first">Enable the service at boot</p>
<p>On RHEL/CentOS systems, use:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># chkconfig cloudstack-usage on</span>
</pre></div>
</div>
<p>On Debian/Ubuntu systems, use:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># update-rc.d cloudstack-usage defaults</span>
</pre></div>
</div>
</li>
</ol>
<p>The <a class="reference external" href="http://docs.cloudstack.apache.org/projects/cloudstack-administration/en/latest/usage.html">Administration Guide</a> discusses further configuration of the Usage
Server.</p>
</div>
</div>
<div class="section" id="ssl-optional">
<h4>SSL (Optional)<a class="headerlink" href="#ssl-optional" title="Permalink to this headline">¶</a></h4>
<p>CloudStack provides HTTP access in its default installation. There are a
number of technologies and sites which choose to implement SSL. As a
result, we have left CloudStack to expose HTTP under the assumption that
a site will implement its typical practice.</p>
<p>CloudStack uses Tomcat as its servlet container. For sites that would
like CloudStack to terminate the SSL session, Tomcat’s SSL access may be
enabled. Tomcat SSL configuration is described at
<a class="reference external" href="http://tomcat.apache.org/tomcat-6.0-doc/ssl-howto.html">http://tomcat.apache.org/tomcat-6.0-doc/ssl-howto.html</a>.</p>
</div>
<div class="section" id="database-replication-optional">
<h4>Database Replication (Optional)<a class="headerlink" href="#database-replication-optional" title="Permalink to this headline">¶</a></h4>
<p>CloudStack supports database replication from one MySQL node to another.
This is achieved using standard MySQL replication. You may want to do
this as insurance against MySQL server or storage loss. MySQL
replication is implemented using a master/slave model. The master is the
node that the Management Servers are configured to use. The slave is a
standby node that receives all write operations from the master and
applies them to a local, redundant copy of the database. The following
steps are a guide to implementing MySQL replication.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Creating a replica is not a backup solution. You should develop a backup
procedure for the MySQL data that is distinct from replication.</p>
</div>
<ol class="arabic">
<li><p class="first">Ensure that this is a fresh install with no data in the master.</p>
</li>
<li><p class="first">Edit my.cnf on the master and add the following in the [mysqld]
section below datadir.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">log_bin</span><span class="o">=</span>mysql-bin
<span class="nv">server_id</span><span class="o">=</span>1
</pre></div>
</div>
<p>The server_id must be unique with respect to other servers. The
recommended way to achieve this is to give the master an ID of 1 and
each slave a sequential number greater than 1, so that the servers
are numbered 1, 2, 3, etc.</p>
</li>
<li><p class="first">Restart the MySQL service. On RHEL/CentOS systems, use:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service mysqld restart</span>
</pre></div>
</div>
<p>On Debian/Ubuntu systems, use:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service mysql restart</span>
</pre></div>
</div>
</li>
<li><p class="first">Create a replication account on the master and give it privileges. We
will use the &#8220;cloud-repl&#8221; user with the password &#8220;password&#8221;. This
assumes that master and slave run on the 172.16.1.0/24 network.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysql -u root</span>
mysql&gt; create user <span class="s1">&#39;cloud-repl&#39;</span>@<span class="s1">&#39;172.16.1.%&#39;</span> identified by <span class="s1">&#39;password&#39;</span><span class="p">;</span>
mysql&gt; grant replication slave on *.* TO <span class="s1">&#39;cloud-repl&#39;</span>@<span class="s1">&#39;172.16.1.%&#39;</span><span class="p">;</span>
mysql&gt; flush privileges<span class="p">;</span>
mysql&gt; flush tables with <span class="nb">read</span> lock<span class="p">;</span>
</pre></div>
</div>
</li>
<li><p class="first">Leave the current MySQL session running.</p>
</li>
<li><p class="first">In a new shell start a second MySQL session.</p>
</li>
<li><p class="first">Retrieve the current position of the database.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># mysql -u root</span>
mysql&gt; show master status<span class="p">;</span>
+------------------+----------+--------------+------------------+
<span class="p">|</span> File             <span class="p">|</span> Position <span class="p">|</span> Binlog_Do_DB <span class="p">|</span> Binlog_Ignore_DB <span class="p">|</span>
+------------------+----------+--------------+------------------+
<span class="p">|</span> mysql-bin.000001 <span class="p">|</span>      <span class="m">412</span> <span class="p">|</span>              <span class="p">|</span>                  <span class="p">|</span>
+------------------+----------+--------------+------------------+
</pre></div>
</div>
</li>
<li><p class="first">Note the file and the position that are returned by your instance.</p>
</li>
<li><p class="first">Exit from this session.</p>
</li>
<li><p class="first">Complete the master setup. Returning to your first session on the
master, release the locks and exit MySQL.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mysql&gt; unlock tables<span class="p">;</span>
</pre></div>
</div>
</li>
<li><p class="first">Install and configure the slave. On the slave server, run the
following commands.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># yum install mysql-server</span>
<span class="c1"># chkconfig mysqld on</span>
</pre></div>
</div>
</li>
<li><p class="first">Edit my.cnf and add the following lines in the [mysqld] section below
datadir.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">server_id</span><span class="o">=</span>2
<span class="nv">innodb_rollback_on_timeout</span><span class="o">=</span>1
<span class="nv">innodb_lock_wait_timeout</span><span class="o">=</span>600
</pre></div>
</div>
</li>
<li><p class="first">Restart MySQL. Use &#8220;mysqld&#8221; on RHEL/CentOS systems:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service mysqld restart</span>
</pre></div>
</div>
<p>On Ubuntu/Debian systems use &#8220;mysql.&#8221;</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service mysql restart</span>
</pre></div>
</div>
</li>
<li><p class="first">Instruct the slave to connect to and replicate from the master.
Replace the IP address, password, log file, and position with the
values you have used in the previous steps.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mysql&gt; change master to
    -&gt; <span class="nv">master_host</span><span class="o">=</span><span class="s1">&#39;172.16.1.217&#39;</span>,
    -&gt; <span class="nv">master_user</span><span class="o">=</span><span class="s1">&#39;cloud-repl&#39;</span>,
    -&gt; <span class="nv">master_password</span><span class="o">=</span><span class="s1">&#39;password&#39;</span>,
    -&gt; <span class="nv">master_log_file</span><span class="o">=</span><span class="s1">&#39;mysql-bin.000001&#39;</span>,
    -&gt; <span class="nv">master_log_pos</span><span class="o">=</span>412<span class="p">;</span>
</pre></div>
</div>
</li>
<li><p class="first">Then start replication on the slave.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mysql&gt; start slave<span class="p">;</span>
</pre></div>
</div>
</li>
<li><p class="first">Optionally, open port 3306 on the slave as was done on the master
earlier.</p>
<p>This is not required for replication to work. But if you choose not
to do this, you will need to do it when failover to the replica
occurs.</p>
</li>
</ol>
<div class="section" id="failover">
<h5>Failover<a class="headerlink" href="#failover" title="Permalink to this headline">¶</a></h5>
<p>This will provide for a replicated database that can be used to
implement manual failover for the Management Servers. CloudStack
failover from one MySQL instance to another is performed by the
administrator. In the event of a database failure you should:</p>
<ol class="arabic">
<li><p class="first">Stop the Management Servers (via service cloudstack-management stop).</p>
</li>
<li><p class="first">Change the replica&#8217;s configuration to be a master and restart it.</p>
</li>
<li><p class="first">Ensure that the replica&#8217;s port 3306 is open to the Management
Servers.</p>
</li>
<li><p class="first">Make a change so that the Management Server uses the new database.
The simplest process here is to put the IP address of the new
database server into each Management Server&#8217;s
/etc/cloudstack/management/db.properties.</p>
</li>
<li><p class="first">Restart the Management Servers:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service cloudstack-management start</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="amazon-web-services-interface">
<h4>Amazon Web Services Interface<a class="headerlink" href="#amazon-web-services-interface" title="Permalink to this headline">¶</a></h4>
<div class="section" id="amazon-web-services-compatible-interface">
<h5>Amazon Web Services Compatible Interface<a class="headerlink" href="#amazon-web-services-compatible-interface" title="Permalink to this headline">¶</a></h5>
<p>CloudStack can translate Amazon Web Services (AWS) API calls to native
CloudStack API calls so that users can continue using existing
AWS-compatible tools. This translation service runs as a separate web
application in the same tomcat server as the management server of
CloudStack, listening on a different port. The Amazon Web Services (AWS)
compatible interface provides the EC2 SOAP and Query APIs as well as the
S3 REST API.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This service was previously enabled by separate software called CloudBridge.
It is now fully integrated with the CloudStack management server.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">The compatible interface for the EC2 Query API and the S3 API are Work In
Progress. The S3 compatible API offers a way to store data on the
management server file system, it is not an implementation of the S3
backend.</p>
</div>
<p>Limitations</p>
<ul class="simple">
<li>Supported only in zones that use basic networking.</li>
<li>Available in fresh installations of CloudStack. Not available through
upgrade of previous versions.</li>
<li>Features such as Elastic IP (EIP) and Elastic Load Balancing (ELB)
are only available in an infrastructure with a Citrix NetScaler
device. Users accessing a Zone with a NetScaler device will need to
use a NetScaler-enabled network offering (DefaultSharedNetscalerEIP
and ELBNetworkOffering).</li>
</ul>
</div>
<div class="section" id="supported-api-version">
<h5>Supported API Version<a class="headerlink" href="#supported-api-version" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>The EC2 interface complies with Amazon&#8217;s WDSL version dated November
15, 2010, available at <a class="reference external" href="http://ec2.amazonaws.com/doc/2010-11-15/">http://ec2.amazonaws.com/doc/2010-11-15/</a>.</li>
<li>The interface is compatible with the EC2 command-line tools <em>EC2
tools v. 1.3.6230</em>, which can be downloaded at
<a class="reference external" href="http://s3.amazonaws.com/ec2-downloads/ec2-api-tools-1.3-62308.zip">http://s3.amazonaws.com/ec2-downloads/ec2-api-tools-1.3-62308.zip</a>.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Work is underway to support a more recent version of the EC2 API</p>
</div>
</div>
<div class="section" id="enabling-the-ec2-and-s3-compatible-interface">
<h5>Enabling the EC2 and S3 Compatible Interface<a class="headerlink" href="#enabling-the-ec2-and-s3-compatible-interface" title="Permalink to this headline">¶</a></h5>
<p>The software that provides AWS API compatibility is installed along with
CloudStack. You must enable the services and perform some setup steps
prior to using it.</p>
<ol class="arabic">
<li><p class="first">Set the global configuration parameters for each service to true. See
<a class="reference external" href="configuration.html#setting-global-configuration-parameters">*Setting Global Configuration Parameters*</a>.</p>
</li>
<li><p class="first">Create a set of CloudStack service offerings with names that match
the Amazon service offerings. You can do this through the CloudStack
UI as described in the Administration Guide.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Be sure you have included the Amazon default service offering, m1.small.
As well as any EC2 instance types that you will use.</p>
</div>
</li>
<li><p class="first">If you did not already do so when you set the configuration parameter
in step 1, restart the Management Server.</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># service cloudstack-management restart</span>
</pre></div>
</div>
</li>
</ol>
<p>The following sections provides details to perform these steps</p>
<div class="section" id="enabling-the-services">
<h6>Enabling the Services<a class="headerlink" href="#enabling-the-services" title="Permalink to this headline">¶</a></h6>
<p>To enable the EC2 and S3 compatible services you need to set the
configuration variables <em>enable.ec2.api</em> and <em>enable.s3.api</em> to true.
You do not have to enable both at the same time. Enable the ones you
need. This can be done via the CloudStack GUI by going in <em>Global
Settings</em> or via the API.</p>
<p>The snapshot below shows you how to use the GUI to enable these services</p>
<p><img alt="Use the GUI to set the configuration variable to true" src="_images/ec2-s3-configuration.png" /></p>
<p>Using the CloudStack API, the easiest is to use the so-called
integration port on which you can make unauthenticated calls. In Global
Settings set the port to 8096 and subsequently call the
<em>updateConfiguration</em> method. The following urls shows you how:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>http://localhost:8096/client/api?command<span class="o">=</span>updateConfiguration<span class="p">&amp;</span><span class="nv">name</span><span class="o">=</span>enable.ec2.api<span class="p">&amp;</span><span class="nv">value</span><span class="o">=</span><span class="nb">true</span>
http://localhost:8096/client/api?command<span class="o">=</span>updateConfiguration<span class="p">&amp;</span><span class="nv">name</span><span class="o">=</span>enable.ec2.api<span class="p">&amp;</span><span class="nv">value</span><span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>Once you have enabled the services, restart the server.</p>
</div>
<div class="section" id="creating-ec2-compatible-service-offerings">
<h6>Creating EC2 Compatible Service Offerings<a class="headerlink" href="#creating-ec2-compatible-service-offerings" title="Permalink to this headline">¶</a></h6>
<p>You will also need to define compute service offerings with names
compatible with the <a class="reference external" href="http://aws.amazon.com/ec2/instance-types/">Amazon EC2 instance
types</a> API names (e.g
m1.small,m1.large). This can be done via the CloudStack GUI. Go under
<em>Service Offerings</em> select <em>Compute offering</em> and either create a new
compute offering or modify an existing one, ensuring that the name
matches an EC2 instance type API name. The snapshot below shows you how:</p>
<p><img alt="Use the GUI to set the name of a compute service offering to an EC2 instance type API name." src="_images/compute-service-offerings.png" /></p>
</div>
<div class="section" id="modifying-the-aws-api-port">
<h6>Modifying the AWS API Port<a class="headerlink" href="#modifying-the-aws-api-port" title="Permalink to this headline">¶</a></h6>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>(Optional) The AWS API listens for requests on port 7080. If you prefer AWS
API to listen on another port, you can change it as follows:</p>
<ol class="last arabic simple">
<li>Edit the files <code class="docutils literal"><span class="pre">/etc/cloudstack/management/server.xml</span></code>,
<code class="docutils literal"><span class="pre">/etc/cloudstack/management/server-nonssl.xml</span></code>, and
<code class="docutils literal"><span class="pre">/etc/cloudstack/management/server-ssl.xml</span></code>.</li>
<li>In each file, find the tag &lt;Service name=&#8221;Catalina7080&#8221;&gt;. Under this tag, locate &lt;Connector executor=&#8221;tomcatThreadPool-internal&#8221; port=   ....&lt;.</li>
<li>Change the port to whatever port you want to use, then save the files.</li>
<li>Restart the Management Server.</li>
</ol>
</div>
<p>If you re-install CloudStack, you will have to re-enable the services
and if need be update the port.</p>
</div>
</div>
<div class="section" id="aws-api-user-setup">
<h5>AWS API User Setup<a class="headerlink" href="#aws-api-user-setup" title="Permalink to this headline">¶</a></h5>
<p>In general, users need not be aware that they are using a translation
service provided by CloudStack. They only need to send AWS API calls to
CloudStack&#8217;s endpoint, and it will translate the calls to the native
CloudStack API. Users of the Amazon EC2 compatible interface will be
able to keep their existing EC2 tools and scripts and use them with
their CloudStack deployment, by specifying the endpoint of the
management server and using the proper user credentials. In order to do
this, each user must perform the following configuration steps:</p>
<ul class="simple">
<li>Generate user credentials.</li>
<li>Register with the service.</li>
<li>For convenience, set up environment variables for the EC2 SOAP
command-line tools.</li>
</ul>
</div>
<div class="section" id="aws-api-command-line-tools-setup">
<h5>AWS API Command-Line Tools Setup<a class="headerlink" href="#aws-api-command-line-tools-setup" title="Permalink to this headline">¶</a></h5>
<p>To use the EC2 command-line tools, the user must perform these steps:</p>
<ol class="arabic simple">
<li>Be sure you have the right version of EC2 Tools. The supported
version is available at
<a class="reference external" href="http://s3.amazonaws.com/ec2-downloads/ec2-api-tools-1.3-62308.zip">http://s3.amazonaws.com/ec2-downloads/ec2-api-tools-1.3-62308.zip</a>.</li>
<li>Set up the EC2 environment variables. This can be done every time you
use the service or you can set them up in the proper shell profile.
Replace the endpoint (i.e EC2_URL) with the proper address of your
CloudStack management server and port. In a bash shell do the
following.</li>
</ol>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ <span class="nb">export</span> <span class="nv">EC2_CERT</span><span class="o">=</span>/path/to/cert.pem
$ <span class="nb">export</span> <span class="nv">EC2_PRIVATE_KEY</span><span class="o">=</span>/path/to/private_key.pem
$ <span class="nb">export</span> <span class="nv">EC2_URL</span><span class="o">=</span>http://localhost:7080/awsapi
$ <span class="nb">export</span> <span class="nv">EC2_HOME</span><span class="o">=</span>/path/to/EC2_tools_directory
</pre></div>
</div>
</div>
<div class="section" id="using-timeouts-to-ensure-aws-api-command-completion">
<h5>Using Timeouts to Ensure AWS API Command Completion<a class="headerlink" href="#using-timeouts-to-ensure-aws-api-command-completion" title="Permalink to this headline">¶</a></h5>
<p>The Amazon EC2 command-line tools have a default connection timeout.
When used with CloudStack, a longer timeout might be needed for some
commands. If you find that commands are not completing due to timeouts,
you can specify a custom timeouts. You can add the following optional
command-line parameters to any CloudStack-supported EC2 command:</p>
<p>Specifies a connection timeout (in seconds)</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>--connection-timeout TIMEOUT
</pre></div>
</div>
<p>Specifies a request timeout (in seconds)</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>--request-timeout TIMEOUT
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>ec2-run-instances <span class="m">2</span> –z us-test1 –n 1-3 --connection-timeout <span class="m">120</span> --request-timeout 120
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The timeouts optional arguments are not specific to CloudStack.</p>
</div>
</div>
<div class="section" id="supported-aws-api-calls">
<h5>Supported AWS API Calls<a class="headerlink" href="#supported-aws-api-calls" title="Permalink to this headline">¶</a></h5>
<p>The following Amazon EC2 commands are supported by CloudStack when the
AWS API compatible interface is enabled. For a few commands, there are
differences between the CloudStack and Amazon EC2 versions, and these
differences are noted. The underlying SOAP call for each command is also
given, for those who have built tools using those calls.</p>
<p>Table&nbsp;1.&nbsp;Elastic IP API mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="36%" />
<col width="31%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-allocate-address</td>
<td>AllocateAddress</td>
<td>associateIpAddress</td>
</tr>
<tr class="row-odd"><td>ec2-associate-address</td>
<td>AssociateAddress</td>
<td>enableStaticNat</td>
</tr>
<tr class="row-even"><td>ec2-describe-addresses</td>
<td>DescribeAddresses</td>
<td>listPublicIpAddresses</td>
</tr>
<tr class="row-odd"><td>ec2-diassociate-address</td>
<td>DisassociateAddress</td>
<td>disableStaticNat</td>
</tr>
<tr class="row-even"><td>ec2-release-address</td>
<td>ReleaseAddress</td>
<td>disassociateIpAddress</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;2.&nbsp;Availability Zone API mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="40%" />
<col width="33%" />
<col width="26%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-describe-availability-zones</td>
<td>DescribeAvailabilityZones</td>
<td>listZones</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;3.&nbsp;Images API mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="35%" />
<col width="29%" />
<col width="35%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-create-image</td>
<td>CreateImage</td>
<td>createTemplate</td>
</tr>
<tr class="row-odd"><td>ec2-deregister</td>
<td>DeregisterImage</td>
<td>DeleteTemplate</td>
</tr>
<tr class="row-even"><td>ec2-describe-images</td>
<td>DescribeImages</td>
<td>listTemplates</td>
</tr>
<tr class="row-odd"><td>ec2-register</td>
<td>RegisterImage</td>
<td>registerTemplate</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;4.&nbsp;Image Attributes API mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="37%" />
<col width="30%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-describe-image-attribute</td>
<td>DescribeImageAttribute</td>
<td>listTemplatePermissions</td>
</tr>
<tr class="row-odd"><td>ec2-modify-image-attribute</td>
<td>ModifyImageAttribute</td>
<td>updateTemplatePermissions</td>
</tr>
<tr class="row-even"><td>ec2-reset-image-attribute</td>
<td>ResetImageAttribute</td>
<td>updateTemplatePermissions</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;5.&nbsp;Instances API mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="36%" />
<col width="30%" />
<col width="34%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-describe-instances</td>
<td>DescribeInstances</td>
<td>listVirtualMachines</td>
</tr>
<tr class="row-odd"><td>ec2-run-instances</td>
<td>RunInstances</td>
<td>deployVirtualMachine</td>
</tr>
<tr class="row-even"><td>ec2-reboot-instances</td>
<td>RebootInstances</td>
<td>rebootVirtualMachine</td>
</tr>
<tr class="row-odd"><td>ec2-start-instances</td>
<td>StartInstances</td>
<td>startVirtualMachine</td>
</tr>
<tr class="row-even"><td>ec2-stop-instances</td>
<td>StopInstances</td>
<td>stopVirtualMachine</td>
</tr>
<tr class="row-odd"><td>ec2-terminate-instances</td>
<td>TerminateInstances</td>
<td>destroyVirtualMachine</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;6.&nbsp;Instance Attributes Mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="40%" />
<col width="33%" />
<col width="26%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-describe-instance-attribute</td>
<td>DescribeInstanceAttribute</td>
<td>listVirtualMachines</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;7.&nbsp;Keys Pairs Mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="37%" />
<col width="29%" />
<col width="34%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-add-keypair</td>
<td>CreateKeyPair</td>
<td>createSSHKeyPair</td>
</tr>
<tr class="row-odd"><td>ec2-delete-keypair</td>
<td>DeleteKeyPair</td>
<td>deleteSSHKeyPair</td>
</tr>
<tr class="row-even"><td>ec2-describe-keypairs</td>
<td>DescribeKeyPairs</td>
<td>listSSHKeyPairs</td>
</tr>
<tr class="row-odd"><td>ec2-import-keypair</td>
<td>ImportKeyPair</td>
<td>registerSSHKeyPair</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;8.&nbsp;Passwords API Mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="32%" />
<col width="31%" />
<col width="37%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-get-password</td>
<td>GetPasswordData</td>
<td>getVMPassword</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;9.&nbsp;Security Groups API Mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="25%" />
<col width="38%" />
<col width="38%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-authorize</td>
<td>AuthorizeSecurityGroupIngress</td>
<td>authorizeSecurityGroupIngress</td>
</tr>
<tr class="row-odd"><td>ec2-add-group</td>
<td>CreateSecurityGroup</td>
<td>createSecurityGroup</td>
</tr>
<tr class="row-even"><td>ec2-delete-group</td>
<td>DeleteSecurityGroup</td>
<td>deleteSecurityGroup</td>
</tr>
<tr class="row-odd"><td>ec2-describe-group</td>
<td>DescribeSecurityGroups</td>
<td>listSecurityGroups</td>
</tr>
<tr class="row-even"><td>ec2-revoke</td>
<td>RevokeSecurityGroupIngress</td>
<td>revokeSecurityGroupIngress</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;10.&nbsp;Snapshots API Mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="37%" />
<col width="30%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-create-snapshot</td>
<td>CreateSnapshot</td>
<td>createSnapshot</td>
</tr>
<tr class="row-odd"><td>ec2-delete-snapshot</td>
<td>DeleteSnapshot</td>
<td>deleteSnapshot</td>
</tr>
<tr class="row-even"><td>ec2-describe-snapshots</td>
<td>DescribeSnapshots</td>
<td>listSnapshots</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Table&nbsp;11.&nbsp;Volumes API Mapping</p>
<table border="1" class="table-striped table-bordered table-hover docutils">
<colgroup>
<col width="36%" />
<col width="28%" />
<col width="36%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">EC2 command</th>
<th class="head">SOAP call</th>
<th class="head">CloudStack API call</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ec2-attach-volume</td>
<td>AttachVolume</td>
<td>attachVolume</td>
</tr>
<tr class="row-odd"><td>ec2-create-volume</td>
<td>CreateVolume</td>
<td>createVolume</td>
</tr>
<tr class="row-even"><td>ec2-delete-volume</td>
<td>DeleteVolume</td>
<td>deleteVolume</td>
</tr>
<tr class="row-odd"><td>ec2-describe-volume</td>
<td>DescribeVolume</td>
<td>listVolumes</td>
</tr>
<tr class="row-even"><td>ec2-detach-volume</td>
<td>DetachVolume</td>
<td>detachVolume</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="examples">
<h5>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h5>
<p>There are many tools available to interface with a AWS compatible API.
In this section we provide a few examples that users of CloudStack can
build upon.</p>
<div class="section" id="boto-examples">
<h6>Boto Examples<a class="headerlink" href="#boto-examples" title="Permalink to this headline">¶</a></h6>
<p>Boto is one of them. It is a Python package available at
<a class="reference external" href="https://github.com/boto/boto">https://github.com/boto/boto</a>. In this section we provide two examples of
Python scripts that use Boto and have been tested with the CloudStack
AWS API Interface.</p>
<p>First is an EC2 example. Replace the Access and Secret Keys with your
own and update the endpoint.</p>
<p>Example&nbsp;1.&nbsp;An EC2 Boto example</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">boto</span>
<span class="kn">import</span> <span class="nn">boto.ec2</span>

<span class="n">region</span> <span class="o">=</span> <span class="n">boto</span><span class="o">.</span><span class="n">ec2</span><span class="o">.</span><span class="n">regioninfo</span><span class="o">.</span><span class="n">RegionInfo</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;ROOT&quot;</span><span class="p">,</span><span class="n">endpoint</span><span class="o">=</span><span class="s2">&quot;localhost&quot;</span><span class="p">)</span>
<span class="n">apikey</span><span class="o">=</span><span class="s1">&#39;GwNnpUPrO6KgIdZu01z_ZhhZnKjtSdRwuYd4DvpzvFpyxGMvrzno2q05MB0ViBoFYtdqKd&#39;</span>
<span class="n">secretkey</span><span class="o">=</span><span class="s1">&#39;t4eXLEYWw7chBhDlaKf38adCMSHx_wlds6JfSx3z9fSpSOm0AbP9Moj0oGIzy2LSC8iw&#39;</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="sd">&#39;&#39;&#39;Establish connection to EC2 cloud&#39;&#39;&#39;</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">boto</span><span class="o">.</span><span class="n">connect_ec2</span><span class="p">(</span><span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">apikey</span><span class="p">,</span>
                            <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">secretkey</span><span class="p">,</span>
                            <span class="n">is_secure</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                            <span class="n">region</span><span class="o">=</span><span class="n">region</span><span class="p">,</span>
                            <span class="n">port</span><span class="o">=</span><span class="mi">7080</span><span class="p">,</span>
                            <span class="n">path</span><span class="o">=</span><span class="s2">&quot;/awsapi&quot;</span><span class="p">,</span>
                            <span class="n">api_version</span><span class="o">=</span><span class="s2">&quot;2010-11-15&quot;</span><span class="p">)</span>

    <span class="sd">&#39;&#39;&#39;Get list of images that I own&#39;&#39;&#39;</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">get_all_images</span><span class="p">()</span>
    <span class="k">print</span> <span class="n">images</span>
    <span class="n">myimage</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="sd">&#39;&#39;&#39;Pick an instance type&#39;&#39;&#39;</span>
    <span class="n">vm_type</span><span class="o">=</span><span class="s1">&#39;m1.small&#39;</span>
    <span class="n">reservation</span> <span class="o">=</span> <span class="n">myimage</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">instance_type</span><span class="o">=</span><span class="n">vm_type</span><span class="p">,</span><span class="n">security_groups</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;default&#39;</span><span class="p">])</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
     <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Second is an S3 example. The S3 interface in CloudStack is obsolete. If you need an S3 interface you should look at systems like RiakCS, Ceph or GlusterFS. This example is here for completeness and can be adapted to other S3 endpoint.</p>
<p>Example&nbsp;2.&nbsp;An S3 Boto Example</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">boto.s3.key</span> <span class="kn">import</span> <span class="n">Key</span>
<span class="kn">from</span> <span class="nn">boto.s3.connection</span> <span class="kn">import</span> <span class="n">S3Connection</span>
<span class="kn">from</span> <span class="nn">boto.s3.connection</span> <span class="kn">import</span> <span class="n">OrdinaryCallingFormat</span>

<span class="n">apikey</span><span class="o">=</span><span class="s1">&#39;ChOw-pwdcCFy6fpeyv6kUaR0NnhzmG3tE7HLN2z3OB_s-ogF5HjZtN4rnzKnq2UjtnHeg_yLA5gOw&#39;</span>
<span class="n">secretkey</span><span class="o">=</span><span class="s1">&#39;IMY8R7CJQiSGFk4cHwfXXN3DUFXz07cCiU80eM3MCmfLs7kusgyOfm0g9qzXRXhoAPCH-IRxXc3w&#39;</span>

<span class="n">cf</span><span class="o">=</span><span class="n">OrdinaryCallingFormat</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="sd">&#39;&#39;&#39;Establish connection to S3 service&#39;&#39;&#39;</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">S3Connection</span><span class="p">(</span><span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">apikey</span><span class="p">,</span><span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">secretkey</span><span class="p">,</span> \
                        <span class="n">is_secure</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> \
                        <span class="n">host</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span> \
                        <span class="n">port</span><span class="o">=</span><span class="mi">7080</span><span class="p">,</span> \
                        <span class="n">calling_format</span><span class="o">=</span><span class="n">cf</span><span class="p">,</span> \
                        <span class="n">path</span><span class="o">=</span><span class="s2">&quot;/awsapi/rest/AmazonS3&quot;</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">bucket</span><span class="o">=</span><span class="n">conn</span><span class="o">.</span><span class="n">create_bucket</span><span class="p">(</span><span class="s1">&#39;cloudstack&#39;</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">Key</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
        <span class="n">k</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">k</span><span class="o">.</span><span class="n">set_contents_from_filename</span><span class="p">(</span><span class="s1">&#39;/Users/runseb/Desktop/s3cs.py&#39;</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span> <span class="s1">&#39;could not write file&#39;</span>
            <span class="k">pass</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">get_bucket</span><span class="p">(</span><span class="s1">&#39;cloudstack&#39;</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">Key</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
        <span class="n">k</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">k</span><span class="o">.</span><span class="n">get_contents_to_filename</span><span class="p">(</span><span class="s1">&#39;/Users/runseb/Desktop/foobar&#39;</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span> <span class="s1">&#39;Could not get file&#39;</span>
            <span class="k">pass</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">bucket1</span><span class="o">=</span><span class="n">conn</span><span class="o">.</span><span class="n">create_bucket</span><span class="p">(</span><span class="s1">&#39;teststring&#39;</span><span class="p">)</span>
        <span class="n">k</span><span class="o">=</span><span class="n">Key</span><span class="p">(</span><span class="n">bucket1</span><span class="p">)</span>
        <span class="n">k</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="s1">&#39;foobar&#39;</span><span class="p">)</span>
        <span class="n">k</span><span class="o">.</span><span class="n">set_contents_from_string</span><span class="p">(</span><span class="s1">&#39;This is my silly test&#39;</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">bucket1</span><span class="o">=</span><span class="n">conn</span><span class="o">.</span><span class="n">get_bucket</span><span class="p">(</span><span class="s1">&#39;teststring&#39;</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">Key</span><span class="p">(</span><span class="n">bucket1</span><span class="p">)</span>
        <span class="n">k</span><span class="o">.</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;foobar&#39;</span>
        <span class="n">k</span><span class="o">.</span><span class="n">get_contents_as_string</span><span class="p">()</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<span id="document-encryption"></span><div class="section" id="about-password-and-key-encryption">
<span id="about-password-key-encryption"></span><h3>About Password and Key Encryption<a class="headerlink" href="#about-password-and-key-encryption" title="Permalink to this headline">¶</a></h3>
<p>CloudStack stores several sensitive passwords and secret keys that are
used to provide security. These values are always automatically
encrypted:</p>
<ul class="simple">
<li>Database secret key</li>
<li>Database password</li>
<li>SSH keys</li>
<li>Compute node root password</li>
<li>VPN password</li>
<li>User API secret key</li>
<li>VNC password</li>
</ul>
<p>CloudStack uses the Java Simplified Encryption (JASYPT) library. The
data values are encrypted and decrypted using a database secret key,
which is stored in one of CloudStack’s internal properties files along
with the database password. The other encrypted values listed above,
such as SSH keys, are in the CloudStack internal database.</p>
<p>Of course, the database secret key itself can not be stored in the open
– it must be encrypted. How then does CloudStack read it? A second
secret key must be provided from an external source during Management
Server startup. This key can be provided in one of two ways: loaded from
a file or provided by the CloudStack administrator. The CloudStack
database has a configuration setting that lets it know which of these
methods will be used. If the encryption type is set to &#8220;file,&#8221; the key
must be in a file in a known location. If the encryption type is set to
&#8220;web,&#8221; the administrator runs the utility
com.cloud.utils.crypt.EncryptionSecretKeySender, which relays the key to
the Management Server over a known port.</p>
<p>The encryption type, database secret key, and Management Server secret
key are set during CloudStack installation. They are all parameters to
the CloudStack database setup script (cloudstack-setup-databases). The
default values are file, password, and password. It is, of course,
highly recommended that you change these to more secure keys.</p>
<div class="section" id="changing-the-default-password-encryption">
<h4>Changing the Default Password Encryption<a class="headerlink" href="#changing-the-default-password-encryption" title="Permalink to this headline">¶</a></h4>
<p>Passwords are encoded when creating or updating users. CloudStack allows
you to determine the default encoding and authentication mechanism for
admin and user logins. Two new configurable lists have been
introduced—userPasswordEncoders and userAuthenticators.
userPasswordEncoders allows you to configure the order of preference for
encoding passwords, whereas userAuthenticators allows you to configure
the order in which authentication schemes are invoked to validate user
passwords.</p>
<p>Additionally, the plain text user authenticator has been modified not to
convert supplied passwords to their md5 sums before checking them with
the database entries. It performs a simple string comparison between
retrieved and supplied login passwords instead of comparing the
retrieved md5 hash of the stored password against the supplied md5 hash
of the password because clients no longer hash the password. The
following method determines what encoding scheme is used to encode the
password supplied during user creation or modification.</p>
<p>When a new user is created, the user password is encoded by using the
first valid encoder loaded as per the sequence specified in the
<code class="docutils literal"><span class="pre">UserPasswordEncoders</span></code> property in the <code class="docutils literal"><span class="pre">ComponentContext.xml</span></code> or
<code class="docutils literal"><span class="pre">nonossComponentContext.xml</span></code> files. The order of authentication
schemes is determined by the <code class="docutils literal"><span class="pre">UserAuthenticators</span></code> property in the same
files. If Non-OSS components, such as VMware environments, are to be
deployed, modify the <code class="docutils literal"><span class="pre">UserPasswordEncoders</span></code> and <code class="docutils literal"><span class="pre">UserAuthenticators</span></code>
lists in the <code class="docutils literal"><span class="pre">nonossComponentContext.xml</span></code> file, for OSS environments,
such as XenServer or KVM, modify the <code class="docutils literal"><span class="pre">ComponentContext.xml</span></code> file. It
is recommended to make uniform changes across both the files. When a new
authenticator or encoder is added, you can add them to this list. While
doing so, ensure that the new authenticator or encoder is specified as a
bean in both these files. The administrator can change the ordering of
both these properties as preferred to change the order of schemes.
Modify the following list properties available in
<code class="docutils literal"><span class="pre">client/tomcatconf/nonossComponentContext.xml.in</span></code> or
<code class="docutils literal"><span class="pre">client/tomcatconf/componentContext.xml.in</span></code> as applicable, to the
desired order:</p>
<div class="highlight-xml"><div class="highlight"><pre><span></span><span class="nt">&lt;property</span> <span class="na">name=</span><span class="s">&quot;UserAuthenticators&quot;</span><span class="nt">&gt;</span>
   <span class="nt">&lt;list&gt;</span>
      <span class="nt">&lt;ref</span> <span class="na">bean=</span><span class="s">&quot;SHA256SaltedUserAuthenticator&quot;</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;ref</span> <span class="na">bean=</span><span class="s">&quot;MD5UserAuthenticator&quot;</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;ref</span> <span class="na">bean=</span><span class="s">&quot;LDAPUserAuthenticator&quot;</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;ref</span> <span class="na">bean=</span><span class="s">&quot;PlainTextUserAuthenticator&quot;</span><span class="nt">/&gt;</span>
   <span class="nt">&lt;/list&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property</span> <span class="na">name=</span><span class="s">&quot;UserPasswordEncoders&quot;</span><span class="nt">&gt;</span>
   <span class="nt">&lt;list&gt;</span>
      <span class="nt">&lt;ref</span> <span class="na">bean=</span><span class="s">&quot;SHA256SaltedUserAuthenticator&quot;</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;ref</span> <span class="na">bean=</span><span class="s">&quot;MD5UserAuthenticator&quot;</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;ref</span> <span class="na">bean=</span><span class="s">&quot;LDAPUserAuthenticator&quot;</span><span class="nt">/&gt;</span>
      <span class="nt">&lt;ref</span> <span class="na">bean=</span><span class="s">&quot;PlainTextUserAuthenticator&quot;</span><span class="nt">/&gt;</span>
   <span class="nt">&lt;/list&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</pre></div>
</div>
<p>In the above default ordering, SHA256Salt is used first for
<code class="docutils literal"><span class="pre">UserPasswordEncoders</span></code>. If the module is found and encoding returns a
valid value, the encoded password is stored in the user table&#8217;s password
column. If it fails for any reason, the MD5UserAuthenticator will be
tried next, and the order continues. For <code class="docutils literal"><span class="pre">UserAuthenticators</span></code>,
SHA256Salt authentication is tried first. If it succeeds, the user is
logged into the Management server. If it fails, md5 is tried next, and
attempts continues until any of them succeeds and the user logs in . If
none of them works, the user is returned an invalid credential message.</p>
</div>
</div>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright 2016, Apache Software Foundation.<br/>
    </p>
  </div>
</footer>
  </body>
</html>
