




<!DOCTYPE html><html lang="en"
      class="chekov"><head><script>var a=window.devsite||{};window.devsite=a;a.readyCallbacks=[];window.devsite.readyCallbacks=a.readyCallbacks;a.ready=function(b){a.readyCallbacks.push(b)};window.devsite.ready=a.ready;
</script><meta charset="utf-8"><meta name="xsrf_token" content="V43RzizZ91ZrthEwy5-72kRPQTV2noPJfl4c_9LLAtY6MTUxNzg3OTExMzcyNjA3MA" /><link rel="canonical" href="https://www.tensorflow.org/tutorials/seq2seq"><link rel="alternate" href="https://www.tensorflow.org/tutorials/seq2seq" hreflang="en"><link rel="alternate" href="seq2seq" hreflang="en-cn"><link rel="alternate" href="https://www.tensorflow.org/tutorials/seq2seq" hreflang="x-default"><link rel="shortcut icon" href="../_static/18908e5f48/images/tensorflow/favicon.png?hl=zh-cn"><link rel="apple-touch-icon" href="../images/apple-touch-icon-180x180.png?hl=zh-cn"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Roboto:300,400,400italic,500,500italic,700,700italic|Roboto+Mono:400,500,700|Material+Icons"><link rel="stylesheet"
        href="../_static/18908e5f48/css/devsite-tensorflow-orange.css?hl=zh-cn"><script src="../_static/18908e5f48/js/jquery-bundle.js?hl=zh-cn"></script><meta property="og:site_name" content="TensorFlow"><meta property="og:type" content="website"><meta property="og:url" content="https://www.tensorflow.org/tutorials/seq2seq"><meta property="og:locale" content="en"><script>
    var ___gcfg = ___gcfg || {};
    ___gcfg.lang = 'zh-cn';
  </script><title>Neural Machine Translation (seq2seq) Tutorial &nbsp;|&nbsp; TensorFlow</title><meta property="og:title" content="Neural Machine Translation (seq2seq) Tutorial &nbsp;|&nbsp; TensorFlow"></head><body class="
               devsite-doc-page
               
               
               
               
               "
        id="top_of_page"><div class="devsite-wrapper"><div class="devsite-top-section-wrapper
            "><header class="devsite-top-section nocontent"><div class="devsite-top-logo-row-wrapper-wrapper"><div class="devsite-top-logo-row-wrapper"><div class="devsite-top-logo-row devsite-full-site-width"><button type="button" class="devsite-expand-section-nav devsite-header-icon-button
                                       button-flat material-icons gc-analytics-event"
                  data-category="Site-Wide Custom Events" data-label="Hamburger menu"></button><div class="devsite-product-name-wrapper"><a href="../index.html?hl=zh-cn" class="devsite-site-logo-link gc-analytics-event"
   data-category="Site-Wide Custom Events" data-label="Site logo"><img src="../_static/18908e5f48/images/tensorflow/lockup.png?hl=zh-cn"
       class="devsite-site-logo" alt="TensorFlow"><span class="devsite-site-name devsite-product-name">TensorFlow</span></a></div><div class="devsite-header-upper-tabs"><nav class="devsite-doc-set-nav devsite-nav devsite-overflow-tabs-scroll-wrapper"><ul class="devsite-doc-set-nav-tab-list devsite-overflow-tabs-scroll"><li class="devsite-doc-set-nav-tab-container"><a href="../install/index?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Install"
       >
        Install
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../get_started/index?hl=zh-cn"
         class="devsite-doc-set-nav-active
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Develop"
       >
        Develop
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../api_docs?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: API r1.5"
       >
        API r1.5
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../deploy/index?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Deploy"
       >
        Deploy
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../extend/index.html?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Extend"
       >
        Extend
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../community/index.html?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Community"
       >
        Community
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../versions/index.html?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Versions"
       >
        Versions
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../tfrc/index.html?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: TFRC"
       >
        TFRC
      </a></li></ul></nav></div><form class="devsite-search-form"
                action="https://tensorflow.google.cn/s/results/?hl=zh-cn"
                method="GET"
                id="top-search"
                search-placeholder='搜索'><div id="searchbox" class="devsite-searchbox"><input placeholder='搜索'
         
         type="text"
         class="devsite-search-field devsite-search-query"
         name="q"
         value=""
         autocomplete="off"><div class="devsite-search-image material-icons"></div></div><input type="hidden"
       name="p"
       id="search_project"
       value="/"
       data-project-name="TensorFlow"
       data-project-path="/"
       data-query-match=""><input type="hidden" class="suggest-project" value="TensorFlow" /><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Android Developers" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Android Open Source Project" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Google Cloud Platform" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Dialogflow" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Firebase" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Link.app" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Nest Developers" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="OpenThread" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="TensorFlow" ></div></form><button type="button" class="devsite-search-button devsite-header-icon-button button-flat
                                       material-icons"></button></div></div></div><div class="devsite-collapsible-section"><div class="devsite-header-background devsite-full-site-width"><div class="devsite-product-id-row devsite-full-site-width"><div class="devsite-product-description-row"><ul class="devsite-breadcrumb-list"><li class="devsite-breadcrumb-item">
    
    
    
      Develop
    
    
  </li></ul></div></div><div class="devsite-doc-set-nav-row devsite-full-site-width"><nav class="devsite-doc-set-nav devsite-nav devsite-overflow-tabs-scroll-wrapper"><ul class="devsite-doc-set-nav-tab-list devsite-overflow-tabs-scroll"><li class="devsite-doc-set-nav-tab-container"><a href="../get_started/index?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Get Started"
       >
        Get Started
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../programmers_guide/index.html?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Programmer&#39;s Guide"
       >
        Programmer&#39;s Guide
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="index.html?hl=zh-cn"
         class="devsite-doc-set-nav-active
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Tutorials"
       >
        Tutorials
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../performance/index.html?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Performance"
       >
        Performance
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../mobile/index.html?hl=zh-cn"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Mobile"
       >
        Mobile
      </a></li></ul></nav></div></div></div></header><script>
    if (window.jQuery) {
      $(document).ready(function() {
        if (window.devsite && window.devsite.search) {
          
          window.devsite.search.init('zh-cn')
        }
      });
    }
  </script></div><div id="gc-wrapper"itemscope itemtype="http://schema.org/Article"><input class="google-analytics-id-json" type="hidden" value="{&quot;dimensions&quot;: {&quot;dimension6&quot;: &quot;en&quot;, &quot;dimension4&quot;: &quot;TensorFlow&quot;, &quot;dimension5&quot;: &quot;zh-cn&quot;, &quot;dimension3&quot;: false, &quot;dimension1&quot;: &quot;Signed out&quot;, &quot;dimension8&quot;: null}, &quot;gaid&quot;: &quot;UA-69864048-6&quot;}"><script>
      var dataLayer = [{"freeTrialEligibleUser": "False", "userCountry": "US", "language": {"requested": "zh-cn", "served": "en"}, "projectName": "TensorFlow", "scriptsafe": null, "signedIn": "False", "internalUser": "False"}];
    </script>

      
        <div class="devsite-site-mask"></div>
        
  

<nav class="devsite-nav-responsive devsite-nav nocontent" tabindex="0">
  
  <div class="devsite-nav-responsive-tabs-panel">
    
      
        



<nav class="devsite-nav-responsive-tabs devsite-nav">
  <ul class="devsite-nav-list">
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../install/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Install">
        Install
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../get_started/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                devsite-nav-active"
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Develop">
        Develop
      </a>
      
        



<nav class="devsite-nav-responsive-tabs devsite-nav">
  <ul class="devsite-nav-list">
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../get_started/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Get Started">
        Get Started
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../programmers_guide/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Programmer&#39;s Guide">
        Programmer&#39;s Guide
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <span class="devsite-nav-responsive-forward devsite-nav-responsive-tab devsite-nav-title
                   devsite-nav-active gc-analytics-event"
            data-category="Site-Wide Custom Events" data-label="Responsive Tab: Tutorials"
            tabindex="0">
        Tutorials
      </span>
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../performance/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Performance">
        Performance
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../mobile/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Mobile">
        Mobile
      </a>
      
    
    </li>
  
  
  </ul>
</nav>

      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../api_docs/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: API r1.5">
        API r1.5
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../deploy/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Deploy">
        Deploy
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../extend/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Extend">
        Extend
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../community/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Community">
        Community
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../versions/index.html?nav=true&amp;hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Versions">
        Versions
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../tfrc/index.html?hl=zh-cn"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: TFRC">
        TFRC
      </a>
      
    
    </li>
  
  
  </ul>
</nav>

      
    
  </div>
  
  
  <div class="devsite-nav-responsive-sidebar-panel">
    
    <div class="devsite-nav-responsive-back" tabindex="0"></div>
    
    <nav class="devsite-nav-responsive-sidebar">
      <ul class="devsite-nav-list"><li class="devsite-nav-item"><a href="index.html?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Tutorials</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Images</span></li><li class="devsite-nav-item"><a href="mnist/beginners/index.html?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">A Guide to TF Layers: Building a Convolutional Neural Network</a></li><li class="devsite-nav-item"><a href="image_recognition?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Image Recognition</a></li><li class="devsite-nav-item"><a href="image_retraining?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">How to Retrain Inception's Final Layer for New Categories</a></li><li class="devsite-nav-item"><a href="deep_cnn?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Convolutional Neural Networks</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Sequences</span></li><li class="devsite-nav-item"><a href="recurrent?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Recurrent Neural Networks</a></li><li class="devsite-nav-item devsite-nav-active"><a href="seq2seq?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Neural Machine Translation (seq2seq) Tutorial</a></li><li class="devsite-nav-item"><a href="recurrent_quickdraw?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Recurrent Neural Networks for Drawing Classification</a></li><li class="devsite-nav-item"><a href="audio_recognition?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Simple Audio Recognition</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Data Representation</span></li><li class="devsite-nav-item"><a href="wide?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Linear Model Tutorial</a></li><li class="devsite-nav-item"><a href="wide_and_deep?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Wide &amp; Deep Learning Tutorial</a></li><li class="devsite-nav-item"><a href="word2vec?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Vector Representations of Words</a></li><li class="devsite-nav-item"><a href="kernel_methods?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Improving Linear Models Using Explicit Kernel Methods</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Non-ML</span></li><li class="devsite-nav-item"><a href="mandelbrot?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Mandelbrot Set</a></li><li class="devsite-nav-item"><a href="pdes?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Partial Differential Equations</a></li><li class="devsite-nav-item"><hr class="devsite-nav-break"></li><li class="devsite-nav-item"><a href="../versions/index.html?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Versions</a></li></ul>
    </nav>
  </div>
  
</nav>


        <div class="devsite-main-content clearfix">

        
        

        
  
  
    
    
      
  <nav class="devsite-section-nav devsite-nav nocontent">
    <ul class="devsite-nav-list"><li class="devsite-nav-item"><a href="index.html?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Tutorials</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Images</span></li><li class="devsite-nav-item"><a href="mnist/beginners/index.html?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">A Guide to TF Layers: Building a Convolutional Neural Network</a></li><li class="devsite-nav-item"><a href="image_recognition?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Image Recognition</a></li><li class="devsite-nav-item"><a href="image_retraining?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">How to Retrain Inception's Final Layer for New Categories</a></li><li class="devsite-nav-item"><a href="deep_cnn?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Convolutional Neural Networks</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Sequences</span></li><li class="devsite-nav-item"><a href="recurrent?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Recurrent Neural Networks</a></li><li class="devsite-nav-item devsite-nav-active"><a href="seq2seq?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Neural Machine Translation (seq2seq) Tutorial</a></li><li class="devsite-nav-item"><a href="recurrent_quickdraw?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Recurrent Neural Networks for Drawing Classification</a></li><li class="devsite-nav-item"><a href="audio_recognition?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Simple Audio Recognition</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Data Representation</span></li><li class="devsite-nav-item"><a href="wide?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Linear Model Tutorial</a></li><li class="devsite-nav-item"><a href="wide_and_deep?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Wide &amp; Deep Learning Tutorial</a></li><li class="devsite-nav-item"><a href="word2vec?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Vector Representations of Words</a></li><li class="devsite-nav-item"><a href="kernel_methods?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Improving Linear Models Using Explicit Kernel Methods</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Non-ML</span></li><li class="devsite-nav-item"><a href="mandelbrot?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Mandelbrot Set</a></li><li class="devsite-nav-item"><a href="pdes?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Partial Differential Equations</a></li><li class="devsite-nav-item"><hr class="devsite-nav-break"></li><li class="devsite-nav-item"><a href="../versions/index.html?hl=zh-cn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Versions</a></li></ul>
  </nav>

    

    
  <nav class="devsite-page-nav devsite-nav"></nav>


      <article class="devsite-article">
        <article class="devsite-article-inner">
  
          
  



  
  <nav class="devsite-breadcrumb-nav devsite-nav">
    


<ul class="devsite-breadcrumb-list">
  
  <li class="devsite-breadcrumb-item">
    
    
    <a href="../index.html?hl=zh-cn" class="devsite-breadcrumb-link gc-analytics-event"
       data-category="Site-Wide Custom Events" data-label="Breadcrumbs"
       data-value="1">
    
    
      TensorFlow
    
    
    </a>
    
  </li>
  
  <li class="devsite-breadcrumb-item">
    
    
    <div class="devsite-breadcrumb-guillemet material-icons"></div>
    
    
    <a href="../get_started/index?hl=zh-cn" class="devsite-breadcrumb-link gc-analytics-event"
       data-category="Site-Wide Custom Events" data-label="Breadcrumbs"
       data-value="2">
    
    
      Develop
    
    
    </a>
    
  </li>
  
  <li class="devsite-breadcrumb-item">
    
    
    <div class="devsite-breadcrumb-guillemet material-icons"></div>
    
    
    <a href="index.html?hl=zh-cn" class="devsite-breadcrumb-link gc-analytics-event"
       data-category="Site-Wide Custom Events" data-label="Breadcrumbs"
       data-value="3">
    
    
      Tutorials
    
    
    </a>
    
  </li>
  
</ul>

  </nav>
  
  
  <h1 itemprop="name" class="devsite-page-title">
    Neural Machine Translation (seq2seq) Tutorial
  </h1>
  
  
  <nav class="devsite-page-nav-embedded devsite-nav"></nav>
  
  <div class="devsite-article-body clearfix
            "
       itemprop="articleBody">
    
<script src="../_static/18908e5f48/js/managed/mathjax/MathJax.js?config=TeX-AMS-MML_SVG&amp;hl=zh-cn"></script>

<!-- DO NOT EDIT! Automatically generated file. -->


<p><em>Authors: Thang Luong, Eugene Brevdo, Rui Zhao (<a href="https://research.googleblog.com/2017/07/building-your-own-neural-machine.html">Google Research Blogpost</a>, <a href="https://github.com/tensorflow/nmt">Github</a>)</em></p>
<p><em>This version of the tutorial requires <a href="https://github.com/tensorflow/tensorflow/#installation">TensorFlow Nightly</a>.
For using the stable TensorFlow versions, please consider other branches such as
<a href="https://github.com/tensorflow/nmt/tree/tf-1.4">tf-1.4</a>.</em></p>
<p><em>If make use of this codebase for your research, please cite
<a href="seq2seq?hl=zh-cn#bibtex">this</a>.</em></p>
<h1 class="page-title" id="introduction">Introduction</h1>
<p>Sequence-to-sequence (seq2seq) models
(<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever et al., 2014</a>,
<a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf">Cho et al., 2014</a>) have
enjoyed great success in a variety of tasks such as machine translation, speech
recognition, and text summarization. This tutorial gives readers a full
understanding of seq2seq models and shows how to build a competitive seq2seq
model from scratch. We focus on the task of Neural Machine Translation (NMT)
which was the very first testbed for seq2seq models with
wild
<a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">success</a>. The
included code is lightweight, high-quality, production-ready, and incorporated
with the latest research ideas. We achieve this goal by:</p>
<ol>
<li>Using the recent decoder / attention
   wrapper
   <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops">API</a>,
   TensorFlow 1.2 data iterator</li>
<li>Incorporating our strong expertise in building recurrent and seq2seq models</li>
<li>Providing tips and tricks for building the very best NMT models and replicating
   <a href="https://research.google.com/pubs/pub45610.html?hl=zh-cn">Google’s NMT (GNMT) system</a>.</li>
</ol>
<p>We believe that it is important to provide benchmarks that people can easily
replicate. As a result, we have provided full experimental results and
pretrained on models on the following publicly available datasets:</p>
<ol>
<li><em>Small-scale</em>: English-Vietnamese parallel corpus of TED talks (133K sentence
   pairs) provided by
   the
   <a href="https://sites.google.com/site/iwsltevaluation2015/?hl=zh-cn">IWSLT Evaluation Campaign</a>.</li>
<li><em>Large-scale</em>: German-English parallel corpus (4.5M sentence pairs) provided
   by the <a href="http://www.statmt.org/wmt16/translation-task.html">WMT Evaluation Campaign</a>.</li>
</ol>
<p>We first build up some basic knowledge about seq2seq models for NMT, explaining
how to build and train a vanilla NMT model. The second part will go into details
of building a competitive NMT model with attention mechanism. We then discuss
tips and tricks to build the best possible NMT models (both in speed and
translation quality) such as TensorFlow best practices (batching, bucketing),
bidirectional RNNs, beam search, as well as scaling up to multiple GPUs using GNMT attention.</p>
<h1 class="page-title" id="basic">Basic</h1>
<h2 id="background_on_neural_machine_translation">Background on Neural Machine Translation</h2>
<p>Back in the old days, traditional phrase-based translation systems performed
their task by breaking up source sentences into multiple chunks and then
translated them phrase-by-phrase. This led to disfluency in the translation
outputs and was not quite like how we, humans, translate. We read the entire
source sentence, understand its meaning, and then produce a translation. Neural
Machine Translation (NMT) mimics that!</p>
<p align="center">
<img width="80%" src="../images/seq2seq/encdec.jpg?hl=zh-cn" />
<br>
Figure 1. <b>Encoder-decoder architecture</b> – example of a general approach for
NMT. An encoder converts a source sentence into a "meaning" vector which is
passed through a <i>decoder</i> to produce a translation.
</p>

<p>Specifically, an NMT system first reads the source sentence using an <em>encoder</em>
to build
a
<a href="https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence">"thought" vector</a>,
a sequence of numbers that represents the sentence meaning; a <em>decoder</em>, then,
processes the sentence vector to emit a translation, as illustrated in
Figure 1. This is often referred to as the <em>encoder-decoder architecture</em>. In
this manner, NMT addresses the local translation problem in the traditional
phrase-based approach: it can capture <em>long-range dependencies</em> in languages,
e.g., gender agreements; syntax structures; etc., and produce much more fluent
translations as demonstrated
by
<a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">Google Neural Machine Translation systems</a>.</p>
<p>NMT models vary in terms of their exact architectures. A natural choice for
sequential data is the recurrent neural network (RNN), used by most NMT models.
Usually an RNN is used for both the encoder and decoder. The RNN models,
however, differ in terms of: (a) <em>directionality</em> – unidirectional or
bidirectional; (b) <em>depth</em> – single- or multi-layer; and (c) <em>type</em> – often
either a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit
(GRU). Interested readers can find more information about RNNs and LSTM on
this <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">blog post</a>.</p>
<p>In this tutorial, we consider as examples a <em>deep multi-layer RNN</em> which is
unidirectional and uses LSTM as a recurrent unit. We show an example of such a
model in Figure 2. In this example, we build a model to translate a source
sentence "I am a student" into a target sentence "Je suis étudiant". At a high
level, the NMT model consists of two recurrent neural networks: the <em>encoder</em>
RNN simply consumes the input source words without making any prediction; the
<em>decoder</em>, on the other hand, processes the target sentence while predicting the
next words.</p>
<p>For more information, we refer readers
to <a href="https://github.com/lmthang/thesis">Luong (2016)</a> which this tutorial is
based on.</p>
<p align="center">
<img width="48%" src="../images/seq2seq/seq2seq.jpg?hl=zh-cn" />
<br>
Figure 2. <b>Neural machine translation</b> – example of a deep recurrent
architecture proposed by for translating a source sentence "I am a student" into
a target sentence "Je suis étudiant". Here, "&lts&gt" marks the start of the
decoding process while "&lt/s&gt" tells the decoder to stop.
</p>

<h2 id="installing_the_tutorial">Installing the Tutorial</h2>
<p>To install this tutorial, you need to have TensorFlow installed on your system.
This tutorial requires TensorFlow Nightly. To install TensorFlow, follow
the <a href="../install/index?hl=zh-cn">installation instructions here</a>.</p>
<p>Once TensorFlow is installed, you can download the source code of this tutorial
by running:</p>
<pre class="prettyprint lang-shell"><code>git clone https://github.com/tensorflow/nmt/
</code></pre>

<h2 id="training_how_to_build_our_first_nmt_system">Training – How to build our first NMT system</h2>
<p>Let's first dive into the heart of building an NMT model with concrete code
snippets through which we will explain Figure 2 in more detail. We defer data
preparation and the full code to later. This part refers to
file
<a href="https://github.com/tensorflow/nmt/tree/master/nmt/model.py"><strong>model.py</strong></a>.</p>
<p>At the bottom layer, the encoder and decoder RNNs receive as input the
following: first, the source sentence, then a boundary marker "\&lt;s>" which
indicates the transition from the encoding to the decoding mode, and the target
sentence.  For <em>training</em>, we will feed the system with the following tensors,
which are in time-major format and contain word indices:</p>
<ul>
<li><strong>encoder_inputs</strong> [max_encoder_time, batch_size]: source input words.</li>
<li><strong>decoder_inputs</strong> [max_decoder_time, batch_size]: target input words.</li>
<li><strong>decoder_outputs</strong> [max_decoder_time, batch_size]: target output words,
   these are decoder_inputs shifted to the left by one time step with an
   end-of-sentence tag appended on the right.</li>
</ul>
<p>Here for efficiency, we train with multiple sentences (batch_size) at
once. Testing is slightly different, so we will discuss it later.</p>
<h3 id="embedding">Embedding</h3>
<p>Given the categorical nature of words, the model must first look up the source
and target embeddings to retrieve the corresponding word representations. For
this <em>embedding layer</em> to work, a vocabulary is first chosen for each language.
Usually, a vocabulary size V is selected, and only the most frequent V words are
treated as unique.  All other words are converted to an "unknown" token and all
get the same embedding.  The embedding weights, one set per language, are
usually learned during training.</p>
<pre class="prettyprint lang-python"><code># Embedding
embedding_encoder = variable_scope.get_variable(
    &quot;embedding_encoder&quot;, [src_vocab_size, embedding_size], ...)
# Look up embedding:
#   encoder_inputs: [max_time, batch_size]
#   encoder_emb_inp: [max_time, batch_size, embedding_size]
encoder_emb_inp = embedding_ops.embedding_lookup(
    embedding_encoder, encoder_inputs)
</code></pre>

<p>Similarly, we can build <em>embedding_decoder</em> and <em>decoder_emb_inp</em>. Note that one
can choose to initialize embedding weights with pretrained word representations
such as word2vec or Glove vectors. In general, given a large amount of training
data we can learn these embeddings from scratch.</p>
<h3 id="encoder">Encoder</h3>
<p>Once retrieved, the word embeddings are then fed as input into the main network,
which consists of two multi-layer RNNs – an encoder for the source language and
a decoder for the target language. These two RNNs, in principle, can share the
same weights; however, in practice, we often use two different RNN parameters
(such models do a better job when fitting large training datasets). The
<em>encoder</em> RNN uses zero vectors as its starting states and is built as follows:</p>
<pre class="prettyprint lang-python"><code># Build RNN cell
encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

# Run Dynamic RNN
#   encoder_outputs: [max_time, batch_size, num_units]
#   encoder_state: [batch_size, num_units]
encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=True)
</code></pre>

<p>Note that sentences have different lengths to avoid wasting computation, we tell
<em>dynamic_rnn</em> the exact source sentence lengths through
<em>source_sequence_length</em>. Since our input is time major, we set
<em>time_major=True</em>. Here, we build only a single layer LSTM, <em>encoder_cell</em>. We
will describe how to build multi-layer LSTMs, add dropout, and use attention in
a later section.</p>
<h3 id="decoder">Decoder</h3>
<p>The <em>decoder</em> also needs to have access to the source information, and one
simple way to achieve that is to initialize it with the last hidden state of the
encoder, <em>encoder_state</em>. In Figure 2, we pass the hidden state at the source
word "student" to the decoder side.</p>
<pre class="prettyprint lang-python"><code># Build RNN cell
decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)
</code></pre>

<pre class="prettyprint lang-python"><code># Helper
helper = tf.contrib.seq2seq.TrainingHelper(
    decoder_emb_inp, decoder_lengths, time_major=True)
# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
logits = outputs.rnn_output
</code></pre>

<p>Here, the core part of this code is the <em>BasicDecoder</em> object, <em>decoder</em>, which
receives <em>decoder_cell</em> (similar to encoder_cell), a <em>helper</em>, and the previous
<em>encoder_state</em> as inputs. By separating out decoders and helpers, we can reuse
different codebases, e.g., <em>TrainingHelper</em> can be substituted with
<em>GreedyEmbeddingHelper</em> to do greedy decoding. See more
in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py">helper.py</a>.</p>
<p>Lastly, we haven't mentioned <em>projection_layer</em> which is a dense matrix to turn
the top hidden states to logit vectors of dimension V. We illustrate this
process at the top of Figure 2.</p>
<pre class="prettyprint lang-python"><code>projection_layer = layers_core.Dense(
    tgt_vocab_size, use_bias=False)
</code></pre>

<h3 id="loss">Loss</h3>
<p>Given the <em>logits</em> above, we are now ready to compute our training loss:</p>
<pre class="prettyprint lang-python"><code>crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=decoder_outputs, logits=logits)
train_loss = (tf.reduce_sum(crossent * target_weights) /
    batch_size)
</code></pre>

<p>Here, <em>target_weights</em> is a zero-one matrix of the same size as
<em>decoder_outputs</em>. It masks padding positions outside of the target sequence
lengths with values 0.</p>
<p><strong><em>Important note</em></strong>: It's worth pointing out that we divide the loss by
<em>batch_size</em>, so our hyperparameters are "invariant" to batch_size. Some people
divide the loss by (<em>batch_size</em> * <em>num_time_steps</em>), which plays down the
errors made on short sentences. More subtly, our hyperparameters (applied to the
former way) can't be used for the latter way. For example, if both approaches
use SGD with a learning of 1.0, the latter approach effectively uses a much
smaller learning rate of 1 / <em>num_time_steps</em>.</p>
<h3 id="gradient_computation_optimization">Gradient computation &amp; optimization</h3>
<p>We have now defined the forward pass of our NMT model. Computing the
backpropagation pass is just a matter of a few lines of code:</p>
<pre class="prettyprint lang-python"><code># Calculate and clip gradients
params = tf.trainable_variables()
gradients = tf.gradients(train_loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(
    gradients, max_gradient_norm)
</code></pre>

<p>One of the important steps in training RNNs is gradient clipping. Here, we clip
by the global norm.  The max value, <em>max_gradient_norm</em>, is often set to a value
like 5 or 1. The last step is selecting the optimizer.  The Adam optimizer is a
common choice.  We also select a learning rate.  The value of <em>learning_rate</em>
can is usually in the range 0.0001 to 0.001; and can be set to decrease as
training progresses.</p>
<pre class="prettyprint lang-python"><code># Optimization
optimizer = tf.train.AdamOptimizer(learning_rate)
update_step = optimizer.apply_gradients(
    zip(clipped_gradients, params))
</code></pre>

<p>In our own experiments, we use standard SGD (tf.train.GradientDescentOptimizer)
with a decreasing learning rate schedule, which yields better performance. See
the <a href="seq2seq?hl=zh-cn#benchmarks">benchmarks</a>.</p>
<h2 id="hands-on_lets_train_an_nmt_model">Hands-on – Let's train an NMT model</h2>
<p>Let's train our very first NMT model, translating from Vietnamese to English!
The entry point of our code
is
<a href="https://github.com/tensorflow/nmt/tree/master/nmt/nmt.py"><strong>nmt.py</strong></a>.</p>
<p>We will use a <em>small-scale parallel corpus of TED talks</em> (133K training
examples) for this exercise. All data we used here can be found
at:
<a href="https://nlp.stanford.edu/projects/nmt/">https://nlp.stanford.edu/projects/nmt/</a>. We
will use tst2012 as our dev dataset, and tst2013 as our test dataset.</p>
<p>Run the following command to download the data for training NMT model:\
    <code>nmt/scripts/download_iwslt15.sh /tmp/nmt_data</code></p>
<p>Run the following command to start the training:</p>
<pre class="prettyprint lang-shell"><code>mkdir /tmp/nmt_model
python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/nmt_data/vocab  \
    --train_prefix=/tmp/nmt_data/train \
    --dev_prefix=/tmp/nmt_data/tst2012  \
    --test_prefix=/tmp/nmt_data/tst2013 \
    --out_dir=/tmp/nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
</code></pre>

<p>The above command trains a 2-layer LSTM seq2seq model with 128-dim hidden units
and embeddings for 12 epochs. We use a dropout value of 0.2 (keep probability
0.8). If no error, we should see logs similar to the below with decreasing
perplexity values as we train.</p>
<pre class="prettyprint"><code># First evaluation, global step 0
  eval dev: perplexity 17193.66
  eval test: perplexity 17193.27
# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017
  sample train data:
    src_reverse: &lt;/s&gt; &lt;/s&gt; Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .
    ref: That , of course , was the &lt;unk&gt; distilled from the theories of Karl Marx . &lt;/s&gt; &lt;/s&gt; &lt;/s&gt;
  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00
  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00
  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00
  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00
  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00
</code></pre>

<p>See <a href="https://github.com/tensorflow/nmt/tree/master/nmt/train.py"><strong>train.py</strong></a> for more details.</p>
<p>We can start Tensorboard to view the summary of the model during training:</p>
<pre class="prettyprint lang-shell"><code>tensorboard --port 22222 --logdir /tmp/nmt_model/
</code></pre>

<p>Training the reverse direction from English and Vietnamese can be done simply by changing:\
    <code>--src=en --tgt=vi</code></p>
<h2 id="inference_how_to_generate_translations">Inference – How to generate translations</h2>
<p>While you're training your NMT models (and once you have trained models), you
can obtain translations given previously unseen source sentences. This process
is called inference. There is a clear distinction between training and inference
(<em>testing</em>): at inference time, we only have access to the source sentence,
i.e., <em>encoder_inputs</em>. There are many ways to perform decoding.  Decoding
methods include greedy, sampling, and beam-search decoding. Here, we will
discuss the greedy decoding strategy.</p>
<p>The idea is simple and we illustrate it in Figure 3:</p>
<ol>
<li>We still encode the source sentence in the same way as during training to
   obtain an <em>encoder_state</em>, and this <em>encoder_state</em> is used to initialize the
   decoder.</li>
<li>The decoding (translation) process is started as soon as the decoder receives
   a starting symbol "\&lt;s>" (refer as <em>tgt_sos_id</em> in our code);</li>
<li>For each timestep on the decoder side, we treat the RNN's output as a set of
   logits.  We choose the most likely word, the id associated with the maximum
   logit value, as the emitted word (this is the "greedy" behavior).  For
   example in Figure 3, the word "moi" has the highest translation probability
   in the first decoding step.  We then feed this word as input to the next
   timestep.</li>
<li>The process continues until the end-of-sentence marker "\&lt;/s>" is produced as
   an output symbol (refer as <em>tgt_eos_id</em> in our code).</li>
</ol>
<p align="center">
<img width="40%" src="../images/seq2seq/greedy_dec.jpg?hl=zh-cn" />
<br>
Figure 3. <b>Greedy decoding</b> – example of how a trained NMT model produces a
translation for a source sentence "Je suis étudiant" using greedy search.
</p>

<p>Step 3 is what makes inference different from training. Instead of always
feeding the correct target words as an input, inference uses words predicted by
the model. Here's the code to achieve greedy decoding.  It is very similar to
the training decoder.</p>
<pre class="prettyprint lang-python"><code># Helper
helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
    embedding_decoder,
    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)

# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder, maximum_iterations=maximum_iterations)
translations = outputs.sample_id
</code></pre>

<p>Here, we use <em>GreedyEmbeddingHelper</em> instead of <em>TrainingHelper</em>. Since we do
not know the target sequence lengths in advance, we use <em>maximum_iterations</em> to
limit the translation lengths. One heuristic is to decode up to two times the
source sentence lengths.</p>
<pre class="prettyprint lang-python"><code>maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)
</code></pre>

<p>Having trained a model, we can now create an inference file and translate some
sentences:</p>
<pre class="prettyprint lang-shell"><code>cat &gt; /tmp/my_infer_file.vi
# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)

python -m nmt.nmt \
    --out_dir=/tmp/nmt_model \
    --inference_input_file=/tmp/my_infer_file.vi \
    --inference_output_file=/tmp/nmt_model/output_infer

cat /tmp/nmt_model/output_infer # To view the inference as output
</code></pre>

<p>Note the above commands can also be run while the model is still being trained
as long as there exists a training
checkpoint. See <a href="https://github.com/tensorflow/nmt/tree/master/nmt/inference.py"><strong>inference.py</strong></a> for more details.</p>
<h1 class="page-title" id="intermediate">Intermediate</h1>
<p>Having gone through the most basic seq2seq model, let's get more advanced! To
build state-of-the-art neural machine translation systems, we will need more
"secret sauce": the <em>attention mechanism</em>, which was first introduced
by <a href="https://arxiv.org/abs/1409.0473">Bahdanau et al., 2015</a>, then later refined
by <a href="https://arxiv.org/abs/1508.04025">Luong et al., 2015</a> and others. The key
idea of the attention mechanism is to establish direct short-cut connections
between the target and the source by paying "attention" to relevant source
content as we translate. A nice byproduct of the attention mechanism is an
easy-to-visualize alignment matrix between the source and target sentences (as
shown in Figure 4).</p>
<p align="center">
<img width="40%" src="../images/seq2seq/attention_vis.jpg?hl=zh-cn" />
<br>
Figure 4. <b>Attention visualization</b> – example of the alignments between source
and target sentences. Image is taken from (Bahdanau et al., 2015).
</p>

<p>Remember that in the vanilla seq2seq model, we pass the last source state from
the encoder to the decoder when starting the decoding process. This works well
for short and medium-length sentences; however, for long sentences, the single
fixed-size hidden state becomes an information bottleneck. Instead of discarding
all of the hidden states computed in the source RNN, the attention mechanism
provides an approach that allows the decoder to peek at them (treating them as a
dynamic memory of the source information). By doing so, the attention mechanism
improves the translation of longer sentences. Nowadays, attention mechanisms are
the defacto standard and have been successfully applied to many other tasks
(including image caption generation, speech recognition, and text
summarization).</p>
<h2 id="background_on_the_attention_mechanism">Background on the Attention Mechanism</h2>
<p>We now describe an instance of the attention mechanism proposed in (Luong et
al., 2015), which has been used in several state-of-the-art systems including
open-source toolkits such as <a href="http://opennmt.net/about/">OpenNMT</a> and in the TF
seq2seq API in this tutorial. We will also provide connections to other variants
of the attention mechanism.</p>
<p align="center">
<img width="48%" src="../images/seq2seq/attention_mechanism.jpg?hl=zh-cn" />
<br>
Figure 5. <b>Attention mechanism</b> – example of an attention-based NMT system
as described in (Luong et al., 2015) . We highlight in detail the first step of
the attention computation. For clarity, we don't show the embedding and
projection layers in Figure (2).
</p>

<p>As illustrated in Figure 5, the attention computation happens at every decoder
time step.  It consists of the following stages:</p>
<ol>
<li>The current target hidden state is compared with all source states to derive
   <em>attention weights</em> (can be visualized as in Figure 4).</li>
<li>Based on the attention weights we compute a <em>context vector</em> as the weighted
   average of the source states.</li>
<li>Combine the context vector with the current target hidden state to yield the
   final <em>attention vector</em></li>
<li>The attention vector is fed as an input to the next time step (<em>input
   feeding</em>).  The first three steps can be summarized by the equations below:</li>
</ol>
<p align="center">
<img width="80%" src="../images/seq2seq/attention_equation_0.jpg?hl=zh-cn" />
<br>
</p>

<p>Here, the function <code>score</code> is used to compared the target hidden state \( h_t \)
with each of the source hidden states \( \overline{h}_s \), and the result is normalized to
produced attention weights (a distribution over source positions). There are
various choices of the scoring function; popular scoring functions include the
multiplicative and additive forms given in Eq. (4). Once computed, the attention
vector \( a_t \) is used to derive the softmax logit and loss.  This is similar to the
target hidden state at the top layer of a vanilla seq2seq model. The function
<code>f</code> can also take other forms.</p>
<p align="center">
<img width="80%" src="../images/seq2seq/attention_equation_1.jpg?hl=zh-cn" />
<br>
</p>

<p>Various implementations of attention mechanisms can be found
in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py">attention_wrapper.py</a>.</p>
<p><strong><em>What matters in the attention mechanism?</em></strong></p>
<p>As hinted in the above equations, there are many different attention variants.
These variants depend on the form of the scoring function and the attention
function, and on whether the previous state \( h_{t-1} \) is used instead of
\( h_t \) in the scoring function as originally suggested in (Bahdanau et al.,
2015). Empirically, we found that only certain choices matter. First, the basic
form of attention, i.e., direct connections between target and source, needs to
be present. Second, it's important to feed the attention vector to the next
timestep to inform the network about past attention decisions as demonstrated in
(Luong et al., 2015). Lastly, choices of the scoring function can often result
in different performance. See more in the <a href="seq2seq?hl=zh-cn#benchmarks">benchmark results</a>
section.</p>
<h2 id="attention_wrapper_api">Attention Wrapper API</h2>
<p>In our implementation of
the
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py">AttentionWrapper</a>,
we borrow some terminology
from <a href="https://arxiv.org/abs/1410.3916">(Weston et al., 2015)</a> in their work on
<em>memory networks</em>. Instead of having readable &amp; writable memory, the attention
mechanism presented in this tutorial is a <em>read-only</em> memory. Specifically, the
set of source hidden states (or their transformed versions, e.g.,
\( W\overline{h}_s \) in Luong's scoring style or \( W_2\overline{h}_s \) in
Bahdanau's scoring style) is referred to as the <em>"memory"</em>. At each time step,
we use the current target hidden state as a <em>"query"</em> to decide on which parts
of the memory to read.  Usually, the query needs to be compared with keys
corresponding to individual memory slots. In the above presentation of the
attention mechanism, we happen to use the set of source hidden states (or their
transformed versions, e.g., \( W_1h_t \) in Bahdanau's scoring style) as
"keys". One can be inspired by this memory-network terminology to derive other
forms of attention!</p>
<p>Thanks to the attention wrapper, extending our vanilla seq2seq code with
attention is trivial. This part refers to
file <a href="https://github.com/tensorflow/nmt/tree/master/nmt/attention_model.py"><strong>attention_model.py</strong></a></p>
<p>First, we need to define an attention mechanism, e.g., from (Luong et al.,
2015):</p>
<pre class="prettyprint lang-python"><code># attention_states: [batch_size, max_time, num_units]
attention_states = tf.transpose(encoder_outputs, [1, 0, 2])

# Create an attention mechanism
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    num_units, attention_states,
    memory_sequence_length=source_sequence_length)
</code></pre>

<p>In the previous <a href="seq2seq?hl=zh-cn#encoder">Encoder</a> section, <em>encoder_outputs</em> is the set of all
source hidden states at the top layer and has the shape of <em>[max_time,
batch_size, num_units]</em> (since we use <em>dynamic_rnn</em> with <em>time_major</em> set to
<em>True</em> for efficiency). For the attention mechanism, we need to make sure the
"memory" passed in is batch major, so we need to transpose
<em>attention_states</em>. We pass <em>source_sequence_length</em> to the attention mechanism
to ensure that the attention weights are properly normalized (over non-padding
positions only).</p>
<p>Having defined an attention mechanism, we use <em>AttentionWrapper</em> to wrap the
decoding cell:</p>
<pre class="prettyprint lang-python"><code>decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    decoder_cell, attention_mechanism,
    attention_layer_size=num_units)
</code></pre>

<p>The rest of the code is almost the same as in the Section <a href="seq2seq?hl=zh-cn#decoder">Decoder</a>!</p>
<h2 id="hands-on_building_an_attention-based_nmt_model">Hands-on – building an attention-based NMT model</h2>
<p>To enable attention, we need to use one of <code>luong</code>, <code>scaled_luong</code>, <code>bahdanau</code>
or <code>normed_bahdanau</code> as the value of the <code>attention</code> flag during training. The
flag specifies which attention mechanism we are going to use. In addition, we
need to create a new directory for the attention model, so we don't reuse the
previously trained basic NMT model.</p>
<p>Run the following command to start the training:</p>
<pre class="prettyprint lang-shell"><code>mkdir /tmp/nmt_attention_model

python -m nmt.nmt \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/nmt_data/vocab  \
    --train_prefix=/tmp/nmt_data/train \
    --dev_prefix=/tmp/nmt_data/tst2012  \
    --test_prefix=/tmp/nmt_data/tst2013 \
    --out_dir=/tmp/nmt_attention_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
</code></pre>

<p>After training, we can use the same inference command with the new out_dir for
inference:</p>
<pre class="prettyprint lang-shell"><code>python -m nmt.nmt \
    --out_dir=/tmp/nmt_attention_model \
    --inference_input_file=/tmp/my_infer_file.vi \
    --inference_output_file=/tmp/nmt_attention_model/output_infer
</code></pre>

<h1 class="page-title" id="tips_tricks">Tips &amp; Tricks</h1>
<h2 id="building_training_eval_and_inference_graphs">Building Training, Eval, and Inference Graphs</h2>
<p>When building a machine learning model in TensorFlow, it's often best to build
three separate graphs:</p>
<ul>
<li>
<p>The Training graph, which:</p>
<ul>
<li>Batches, buckets, and possibly subsamples input data from a set of
   files/external inputs.</li>
<li>Includes the forward and backprop ops.</li>
<li>Constructs the optimizer, and adds the training op.</li>
</ul>
</li>
<li>
<p>The Eval graph, which:</p>
<ul>
<li>Batches and buckets input data from a set of files/external inputs.</li>
<li>Includes the training forward ops, and additional evaluation ops that
   aren't used for training.</li>
</ul>
</li>
<li>
<p>The Inference graph, which:</p>
<ul>
<li>May not batch input data.</li>
<li>Does not subsample or bucket input data.</li>
<li>Reads input data from placeholders (data can be fed directly to the graph
   via <em>feed_dict</em> or from a C++ TensorFlow serving binary).</li>
<li>Includes a subset of the model forward ops, and possibly additional
   special inputs/outputs for storing state between session.run calls.</li>
</ul>
</li>
</ul>
<p>Building separate graphs has several benefits:</p>
<ul>
<li>The inference graph is usually very different from the other two, so it makes
   sense to build it separately.</li>
<li>The eval graph becomes simpler since it no longer has all the additional
   backprop ops.</li>
<li>Data feeding can be implemented separately for each graph.</li>
<li>Variable reuse is much simpler.  For example, in the eval graph there's no
   need to reopen variable scopes with <em>reuse=True</em> just because the Training
   model created these variables already.  So the same code can be reused
   without sprinkling <em>reuse=</em> arguments everywhere.</li>
<li>In distributed training, it is commonplace to have separate workers perform
   training, eval, and inference.  These need to build their own graphs anyway.
   So building the system this way prepares you for distributed training.</li>
</ul>
<p>The primary source of complexity becomes how to share Variables across the three
graphs in a single machine setting. This is solved by using a separate session
for each graph. The training session periodically saves checkpoints, and the
eval session and the infer session restore parameters from checkpoints. The
example below shows the main differences between the two approaches.</p>
<p><strong>Before: Three models in a single graph and sharing a single Session</strong></p>
<pre class="prettyprint lang-python"><code>with tf.variable_scope('root'):
  train_inputs = tf.placeholder()
  train_op, loss = BuildTrainModel(train_inputs)
  initializer = tf.global_variables_initializer()

with tf.variable_scope('root', reuse=True):
  eval_inputs = tf.placeholder()
  eval_loss = BuildEvalModel(eval_inputs)

with tf.variable_scope('root', reuse=True):
  infer_inputs = tf.placeholder()
  inference_output = BuildInferenceModel(infer_inputs)

sess = tf.Session()

sess.run(initializer)

for i in itertools.count():
  train_input_data = ...
  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})

  if i % EVAL_STEPS == 0:
    while data_to_eval:
      eval_input_data = ...
      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})

  if i % INFER_STEPS == 0:
    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})
</code></pre>

<p><strong>After: Three models in three graphs, with three Sessions sharing the same Variables</strong></p>
<pre class="prettyprint lang-python"><code>train_graph = tf.Graph()
eval_graph = tf.Graph()
infer_graph = tf.Graph()

with train_graph.as_default():
  train_iterator = ...
  train_model = BuildTrainModel(train_iterator)
  initializer = tf.global_variables_initializer()

with eval_graph.as_default():
  eval_iterator = ...
  eval_model = BuildEvalModel(eval_iterator)

with infer_graph.as_default():
  infer_iterator, infer_inputs = ...
  infer_model = BuildInferenceModel(infer_iterator)

checkpoints_path = &quot;/tmp/model/checkpoints&quot;

train_sess = tf.Session(graph=train_graph)
eval_sess = tf.Session(graph=eval_graph)
infer_sess = tf.Session(graph=infer_graph)

train_sess.run(initializer)
train_sess.run(train_iterator.initializer)

for i in itertools.count():

  train_model.train(train_sess)

  if i % EVAL_STEPS == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)
    eval_model.saver.restore(eval_sess, checkpoint_path)
    eval_sess.run(eval_iterator.initializer)
    while data_to_eval:
      eval_model.eval(eval_sess)

  if i % INFER_STEPS == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)
    infer_model.saver.restore(infer_sess, checkpoint_path)
    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})
    while data_to_infer:
      infer_model.infer(infer_sess)
</code></pre>

<p>Notice how the latter approach is "ready" to be converted to a distributed
version.</p>
<p>One other difference in the new approach is that instead of using <em>feed_dicts</em>
to feed data at each <em>session.run</em> call (and thereby performing our own
batching, bucketing, and manipulating of data), we use stateful iterator
objects.  These iterators make the input pipeline much easier in both the
single-machine and distributed setting. We will cover the new input data
pipeline (as introduced in TensorFlow 1.2) in the next section.</p>
<h2 id="data_input_pipeline">Data Input Pipeline</h2>
<p>Prior to TensorFlow 1.2, users had two options for feeding data to the
TensorFlow training and eval pipelines:</p>
<ol>
<li>Feed data directly via <em>feed_dict</em> at each training <em>session.run</em> call.</li>
<li>Use the queueing mechanisms in <em>tf.train</em> (e.g. <em>tf.train.batch</em>) and
   <em>tf.contrib.train</em>.</li>
<li>Use helpers from a higher level framework like <em>tf.contrib.learn</em> or
   <em>tf.contrib.slim</em> (which effectively use #2).</li>
</ol>
<p>The first approach is easier for users who aren't familiar with TensorFlow or
need to do exotic input modification (i.e., their own minibatch queueing) that
can only be done in Python.  The second and third approaches are more standard
but a little less flexible; they also require starting multiple python threads
(queue runners).  Furthermore, if used incorrectly queues can lead to deadlocks
or opaque error messages.  Nevertheless, queues are significantly more efficient
than using <em>feed_dict</em> and are the standard for both single-machine and
distributed training.</p>
<p>Starting in TensorFlow 1.2, there is a new system available for reading data
into TensorFlow models: dataset iterators, as found in the <strong>tf.data</strong>
module. Data iterators are flexible, easy to reason about and to manipulate, and
provide efficiency and multithreading by leveraging the TensorFlow C++ runtime.</p>
<p>A <strong>dataset</strong> can be created from a batch data Tensor, a filename, or a Tensor
containing multiple filenames.  Some examples:</p>
<pre class="prettyprint lang-python"><code># Training dataset consists of multiple files.
train_dataset = tf.data.TextLineDataset(train_files)

# Evaluation dataset uses a single file, but we may
# point to a different file for each evaluation round.
eval_file = tf.placeholder(tf.string, shape=())
eval_dataset = tf.data.TextLineDataset(eval_file)

# For inference, feed input data to the dataset directly via feed_dict.
infer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))
infer_dataset = tf.data.Dataset.from_tensor_slices(infer_batch)
</code></pre>

<p>All datasets can be treated similarly via input processing.  This includes
reading and cleaning the data, bucketing (in the case of training and eval),
filtering, and batching.</p>
<p>To convert each sentence into vectors of word strings, for example, we use the
dataset map transformation:</p>
<pre class="prettyprint lang-python"><code>dataset = dataset.map(lambda string: tf.string_split([string]).values)
</code></pre>

<p>We can then switch each sentence vector into a tuple containing both the vector
and its dynamic length:</p>
<pre class="prettyprint lang-python"><code>dataset = dataset.map(lambda words: (words, tf.size(words))
</code></pre>

<p>Finally, we can perform a vocabulary lookup on each sentence.  Given a lookup
table object table, this map converts the first tuple elements from a vector of
strings to a vector of integers.</p>
<pre class="prettyprint lang-python"><code>dataset = dataset.map(lambda words, size: (table.lookup(words), size))
</code></pre>

<p>Joining two datasets is also easy.  If two files contain line-by-line
translations of each other and each one is read into its own dataset, then a new
dataset containing the tuples of the zipped lines can be created via:</p>
<pre class="prettyprint lang-python"><code>source_target_dataset = tf.data.Dataset.zip((source_dataset, target_dataset))
</code></pre>

<p>Batching of variable-length sentences is straightforward. The following
transformation batches <em>batch_size</em> elements from <em>source_target_dataset</em>, and
respectively pads the source and target vectors to the length of the longest
source and target vector in each batch.</p>
<pre class="prettyprint lang-python"><code>batched_dataset = source_target_dataset.padded_batch(
        batch_size,
        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size
                        tf.TensorShape([])),     # size(source)
                       (tf.TensorShape([None]),  # target vectors of unknown size
                        tf.TensorShape([]))),    # size(target)
        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id
                         0),          # size(source) -- unused
                        (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id
                         0)))         # size(target) -- unused
</code></pre>

<p>Values emitted from this dataset will be nested tuples whose tensors have a
leftmost dimension of size <em>batch_size</em>.  The structure will be:</p>
<ul>
<li>iterator[0][0] has the batched and padded source sentence matrices.</li>
<li>iterator[0][1] has the batched source size vectors.</li>
<li>iterator[1][0] has the batched and padded target sentence matrices.</li>
<li>iterator[1][1] has the batched target size vectors.</li>
</ul>
<p>Finally, bucketing that batches similarly-sized source sentences together is
also possible.  Please see the
file
<a href="https://github.com/tensorflow/nmt/tree/master/nmt/utils/iterator_utils.py">utils/iterator_utils.py</a> for
more details and the full implementation.</p>
<p>Reading data from a Dataset requires three lines of code: create the iterator,
get its values, and initialize it.</p>
<pre class="prettyprint lang-python"><code>batched_iterator = batched_dataset.make_initializable_iterator()

((source, source_lengths), (target, target_lengths)) = batched_iterator.get_next()

# At initialization time.
session.run(batched_iterator.initializer, feed_dict={...})
</code></pre>

<p>Once the iterator is initialized, every <em>session.run</em> call that accesses source
or target tensors will request the next minibatch from the underlying dataset.</p>
<h2 id="other_details_for_better_nmt_models">Other details for better NMT models</h2>
<h3 id="bidirectional_rnns">Bidirectional RNNs</h3>
<p>Bidirectionality on the encoder side generally gives better performance (with
some degradation in speed as more layers are used). Here, we give a simplified
example of how to build an encoder with a single bidirectional layer:</p>
<pre class="prettyprint lang-python"><code># Construct forward and backward cells
forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)
backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(
    forward_cell, backward_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=True)
encoder_outputs = tf.concat(bi_outputs, -1)
</code></pre>

<p>The variables <em>encoder_outputs</em> and <em>encoder_state</em> can be used in the same way
as in Section Encoder. Note that, for multiple bidirectional layers, we need to
manipulate the encoder_state a bit, see <a href="https://github.com/tensorflow/nmt/tree/master/nmt/model.py">model.py</a>, method
<em>_build_bidirectional_rnn()</em> for more details.</p>
<h3 id="beam_search">Beam search</h3>
<p>While greedy decoding can give us quite reasonable translation quality, a beam
search decoder can further boost performance. The idea of beam search is to
better explore the search space of all possible translations by keeping around a
small set of top candidates as we translate. The size of the beam is called
<em>beam width</em>; a minimal beam width of, say size 10, is generally sufficient. For
more information, we refer readers to Section 7.2.3
of <a href="https://arxiv.org/abs/1703.01619">Neubig, (2017)</a>. Here's an example of how
beam search can be done:</p>
<pre class="prettyprint lang-python"><code># Replicate encoder infos beam_width times
decoder_initial_state = tf.contrib.seq2seq.tile_batch(
    encoder_state, multiplier=hparams.beam_width)

# Define a beam-search decoder
decoder = tf.contrib.seq2seq.BeamSearchDecoder(
        cell=decoder_cell,
        embedding=embedding_decoder,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=decoder_initial_state,
        beam_width=beam_width,
        output_layer=projection_layer,
        length_penalty_weight=0.0)

# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
</code></pre>

<p>Note that the same <em>dynamic_decode()</em> API call is used, similar to the
Section <a href="seq2seq?hl=zh-cn#decoder">Decoder</a>. Once decoded, we can access the translations as
follows:</p>
<pre class="prettyprint lang-python"><code>translations = outputs.predicted_ids
# Make sure translations shape is [batch_size, beam_width, time]
if self.time_major:
   translations = tf.transpose(translations, perm=[1, 2, 0])
</code></pre>

<p>See <a href="https://github.com/tensorflow/nmt/tree/master/nmt/model.py">model.py</a>, method <em>_build_decoder()</em> for more details.</p>
<h3 id="hyperparameters">Hyperparameters</h3>
<p>There are several hyperparameters that can lead to additional
performances. Here, we list some based on our own experience [ Disclaimers:
others might not agree on things we wrote! ].</p>
<p><strong><em>Optimizer</em></strong>: while Adam can lead to reasonable results for "unfamiliar"
architectures, SGD with scheduling will generally lead to better performance if
you can train with SGD.</p>
<p><strong><em>Attention</em></strong>: Bahdanau-style attention often requires bidirectionality on the
encoder side to work well; whereas Luong-style attention tends to work well for
different settings. For this tutorial code, we recommend using the two improved
variants of Luong &amp; Bahdanau-style attentions: <em>scaled_luong</em> &amp; <em>normed
bahdanau</em>.</p>
<h3 id="multi-gpu_training">Multi-GPU training</h3>
<p>Training a NMT model may take several days. Placing different RNN layers on
different GPUs can improve the training speed. Here’s an example to create
RNN layers on multiple GPUs.</p>
<pre class="prettyprint lang-python"><code>cells = []
for i in range(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      &quot;/gpu:%d&quot; % (num_layers % num_gpus)))
cell = tf.contrib.rnn.MultiRNNCell(cells)
</code></pre>

<p>In addition, we need to enable the <code>colocate_gradients_with_ops</code> option in
<code>tf.gradients</code> to parallelize the gradients computation.</p>
<p>You may notice the speed improvement of the attention based NMT model is very
small as the number of GPUs increases. One major drawback of the standard
attention architecture is using the top (final) layer’s output to query
attention at each time step. That means each decoding step must wait its
previous step completely finished; hence, we can’t parallelize the decoding
process by simply placing RNN layers on multiple GPUs.</p>
<p>The <a href="https://arxiv.org/pdf/1609.08144.pdf">GNMT attention architecture</a>
parallelizes the decoder's computation by using the bottom (first) layer’s
output to query attention. Therefore, each decoding step can start as soon as
its previous step's first layer and attention computation finished. We
implemented the architecture in
<a href="https://github.com/tensorflow/nmt/tree/master/nmt/gnmt_model.py">GNMTAttentionMultiCell</a>,
a subclass of <em>tf.contrib.rnn.MultiRNNCell</em>. Here’s an example of how to create
a decoder cell with the <em>GNMTAttentionMultiCell</em>.</p>
<pre class="prettyprint lang-python"><code>cells = []
for i in range(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      &quot;/gpu:%d&quot; % (num_layers % num_gpus)))
attention_cell = cells.pop(0)
attention_cell = tf.contrib.seq2seq.AttentionWrapper(
    attention_cell,
    attention_mechanism,
    attention_layer_size=None,  # don't add an additional dense layer.
    output_attention=False,)
cell = GNMTAttentionMultiCell(attention_cell, cells)
</code></pre>

<h1 class="page-title" id="benchmarks">Benchmarks</h1>
<h2 id="iwslt_english-vietnamese">IWSLT English-Vietnamese</h2>
<p>Train: 133K examples, vocab=vocab.(vi|en), train=train.(vi|en)
dev=tst2012.(vi|en),
test=tst2013.(vi|en), <a href="https://github.com/tensorflow/nmt/tree/master/nmt/scripts/download_iwslt15.sh">download script</a>.</p>
<p><strong><em>Training details</em></strong>. We train 2-layer LSTMs of 512 units with bidirectional
encoder (i.e., 1 bidirectional layers for the encoder), embedding dim
is 512. LuongAttention (scale=True) is used together with dropout keep_prob of
0.8. All parameters are uniformly. We use SGD with learning rate 1.0 as follows:
train for 12K steps (~ 12 epochs); after 8K steps, we start halving learning
rate every 1K step.</p>
<p><strong><em>Results</em></strong>.</p>
<p>Below are the averaged results of 2 models
(<a href="http://download.tensorflow.org/models/nmt/envi_model_1.zip?hl=zh-cn">model 1</a>,
<a href="http://download.tensorflow.org/models/nmt/envi_model_2.zip?hl=zh-cn">model 2</a>).\
We measure the translation quality in terms of BLEU scores <a href="http://www.aclweb.org/anthology/P02-1040.pdf">(Papineni et al., 2002)</a>.</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th>tst2012 (dev)</th>
<th align="right">test2013 (test)</th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (greedy)</td>
<td>23.2</td>
<td align="right">25.5</td>
</tr>
<tr>
<td>NMT (beam=10)</td>
<td>23.8</td>
<td align="right"><strong>26.1</strong></td>
</tr>
<tr>
<td><a href="https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf">(Luong &amp; Manning, 2015)</a></td>
<td>-</td>
<td align="right">23.3</td>
</tr>
</tbody>
</table>
<p><strong>Training Speed</strong>: (0.37s step-time, 15.3K wps) on <em>K40m</em> &amp; (0.17s step-time, 32.2K wps) on <em>TitanX</em>.\
Here, step-time means the time taken to run one mini-batch (of size 128). For wps, we count words on both the source and target.</p>
<h2 id="wmt_german-english">WMT German-English</h2>
<p>Train: 4.5M examples, vocab=vocab.bpe.32000.(de|en),
train=train.tok.clean.bpe.32000.(de|en), dev=newstest2013.tok.bpe.32000.(de|en),
test=newstest2015.tok.bpe.32000.(de|en),
<a href="https://github.com/tensorflow/nmt/tree/master/nmt/scripts/wmt16_en_de.sh">download script</a></p>
<p><strong><em>Training details</em></strong>. Our training hyperparameters are similar to the
English-Vietnamese experiments except for the following details. The data is
split into subword units using <a href="https://github.com/rsennrich/subword-nmt">BPE</a>
(32K operations). We train 4-layer LSTMs of 1024 units with bidirectional
encoder (i.e., 2 bidirectional layers for the encoder), embedding dim
is 1024. We train for 350K steps (~ 10 epochs); after 170K steps, we start
halving learning rate every 17K step.</p>
<p><strong><em>Results</em></strong>.</p>
<p>The first 2 rows are the averaged results of 2 models
(<a href="http://download.tensorflow.org/models/nmt/deen_model_1.zip?hl=zh-cn">model 1</a>,
<a href="http://download.tensorflow.org/models/nmt/deen_model_2.zip?hl=zh-cn">model 2</a>).
Results in the third row is with GNMT attention
(<a href="http://download.tensorflow.org/models/nmt/10122017/deen_gnmt_model_4_layer.zip?hl=zh-cn">model</a>)
; trained with 4 GPUs.</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th>newstest2013 (dev)</th>
<th align="right">newstest2015</th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (greedy)</td>
<td>27.1</td>
<td align="right">27.6</td>
</tr>
<tr>
<td>NMT (beam=10)</td>
<td>28.0</td>
<td align="right">28.9</td>
</tr>
<tr>
<td>NMT + GNMT attention (beam=10)</td>
<td>29.0</td>
<td align="right"><strong>29.9</strong></td>
</tr>
<tr>
<td><a href="http://matrix.statmt.org/">WMT SOTA</a></td>
<td>-</td>
<td align="right">29.3</td>
</tr>
</tbody>
</table>
<p>These results show that our code builds strong baseline systems for NMT.\
(Note that WMT systems generally utilize a huge amount monolingual data which we currently do not.)</p>
<p><strong>Training Speed</strong>: (2.1s step-time, 3.4K wps) on <em>Nvidia K40m</em> &amp; (0.7s step-time, 8.7K wps) on <em>Nvidia TitanX</em> for standard models.\
To see the speed-ups with GNMT attention, we benchmark on <em>K40m</em> only:</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th>1 gpu</th>
<th>4 gpus</th>
<th align="right">8 gpus</th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (4 layers)</td>
<td>2.2s, 3.4K</td>
<td>1.9s, 3.9K</td>
<td align="right">-</td>
</tr>
<tr>
<td>NMT (8 layers)</td>
<td>3.5s, 2.0K</td>
<td>-</td>
<td align="right">2.9s, 2.4K</td>
</tr>
<tr>
<td>NMT + GNMT attention (4 layers)</td>
<td>2.6s, 2.8K</td>
<td>1.7s, 4.3K</td>
<td align="right">-</td>
</tr>
<tr>
<td>NMT + GNMT attention (8 layers)</td>
<td>4.2s, 1.7K</td>
<td>-</td>
<td align="right">1.9s, 3.8K</td>
</tr>
</tbody>
</table>
<p>These results show that without GNMT attention, the gains from using multiple gpus are minimal.\
With GNMT attention, we obtain from 50%-100% speed-ups with multiple gpus.</p>
<h2 id="wmt_english-german_wzxhzdk46_full_comparison">WMT English-German &mdash; Full Comparison</h2>
<p>The first 2 rows are our models with GNMT
attention:
<a href="http://download.tensorflow.org/models/nmt/10122017/ende_gnmt_model_4_layer.zip?hl=zh-cn">model 1 (4 layers)</a>,
<a href="http://download.tensorflow.org/models/nmt/10122017/ende_gnmt_model_8_layer.zip?hl=zh-cn">model 2 (8 layers)</a>.</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th>newstest2014</th>
<th align="right">newstest2015</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Ours</em> &mdash; NMT + GNMT attention (4 layers)</td>
<td>23.7</td>
<td align="right">26.5</td>
</tr>
<tr>
<td><em>Ours</em> &mdash; NMT + GNMT attention (8 layers)</td>
<td>24.4</td>
<td align="right"><strong>27.6</strong></td>
</tr>
<tr>
<td><a href="http://matrix.statmt.org/">WMT SOTA</a></td>
<td>20.6</td>
<td align="right">24.9</td>
</tr>
<tr>
<td>OpenNMT <a href="https://arxiv.org/abs/1701.02810">(Klein et al., 2017)</a></td>
<td>19.3</td>
<td align="right">-</td>
</tr>
<tr>
<td>tf-seq2seq <a href="https://arxiv.org/abs/1703.03906">(Britz et al., 2017)</a></td>
<td>22.2</td>
<td align="right">25.2</td>
</tr>
<tr>
<td>GNMT <a href="https://research.google.com/pubs/pub45610.html?hl=zh-cn">(Wu et al., 2016)</a></td>
<td><strong>24.6</strong></td>
<td align="right">-</td>
</tr>
</tbody>
</table>
<p>The above results show our models are very competitive among models of similar architectures.\
[Note that OpenNMT uses smaller models and the current best result (as of this writing) is 28.4 obtained by the Transformer network <a href="https://arxiv.org/abs/1706.03762">(Vaswani et al., 2017)</a> which has a significantly different architecture.]</p>
<h2 id="standard_hparams">Standard HParams</h2>
<p>We have provided
<a href="https://github.com/tensorflow/nmt/tree/master/nmt/standard_hparams/">a set of standard hparams</a>
for using pre-trained checkpoint for inference or training NMT architectures
used in the Benchmark.</p>
<p>We will use the WMT16 German-English data, you can download the data by the
following command.</p>
<pre class="prettyprint"><code>nmt/scripts/wmt16_en_de.sh /tmp/wmt16
</code></pre>

<p>Here is an example command for loading the pre-trained GNMT WMT German-English
checkpoint for inference.</p>
<pre class="prettyprint"><code>python -m nmt.nmt \
    --src=de --tgt=en \
    --ckpt=/path/to/checkpoint/translate.ckpt \
    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \
    --out_dir=/tmp/deen_gnmt \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \
    --inference_output_file=/tmp/deen_gnmt/output_infer \
    --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en
</code></pre>

<p>Here is an example command for training the GNMT WMT German-English model.</p>
<pre class="prettyprint"><code>python -m nmt.nmt \
    --src=de --tgt=en \
    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \
    --out_dir=/tmp/deen_gnmt \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --train_prefix=/tmp/wmt16/train.tok.clean.bpe.32000 \
    --dev_prefix=/tmp/wmt16/newstest2013.tok.bpe.32000 \
    --test_prefix=/tmp/wmt16/newstest2015.tok.bpe.32000
</code></pre>

<h1 class="page-title" id="other_resources">Other resources</h1>
<p>For deeper reading on Neural Machine Translation and sequence-to-sequence
models, we highly recommend the following materials
by
<a href="https://sites.google.com/site/acl16nmt/?hl=zh-cn">Luong, Cho, Manning, (2016)</a>;
<a href="https://github.com/lmthang/thesis">Luong, (2016)</a>;
and <a href="https://arxiv.org/abs/1703.01619">Neubig, (2017)</a>.</p>
<p>There's a wide variety of tools for building seq2seq models, so we pick one per
language:\
Stanford NMT
<a href="https://nlp.stanford.edu/projects/nmt/">https://nlp.stanford.edu/projects/nmt/</a>
<em>[Matlab]</em> \
tf-seq2seq
<a href="https://github.com/google/seq2seq">https://github.com/google/seq2seq</a>
<em>[TensorFlow]</em> \
Nemantus
<a href="https://github.com/rsennrich/nematus">https://github.com/rsennrich/nematus</a>
<em>[Theano]</em> \
OpenNMT <a href="http://opennmt.net/">http://opennmt.net/</a> <em>[Torch]</em>\
OpenNMT-py <a href="https://github.com/OpenNMT/OpenNMT-py">https://github.com/OpenNMT/OpenNMT-py</a> <em>[PyTorch]</em></p>
<h1 class="page-title" id="acknowledgment">Acknowledgment</h1>
<p>We would like to thank Denny Britz, Anna Goldie, Derek Murray, and Cinjon Resnick for their work bringing new features to TensorFlow and the seq2seq library. Additional thanks go to Lukasz Kaiser for the initial help on the seq2seq codebase; Quoc Le for the suggestion to replicate GNMT; Yonghui Wu and Zhifeng Chen for details on the GNMT systems; as well as the Google Brain team for their support and feedback!</p>
<h1 class="page-title" id="references">References</h1>
<ul>
<li>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
   Bengio. 2015.<a href="https://arxiv.org/pdf/1409.0473.pdf"> Neural machine translation by jointly learning to align and translate</a>. ICLR.</li>
<li>Minh-Thang Luong, Hieu Pham, and Christopher D
   Manning. 2015.<a href="https://arxiv.org/pdf/1508.04025.pdf"> Effective approaches to attention-based neural machine translation</a>. EMNLP.</li>
<li>Ilya Sutskever, Oriol Vinyals, and Quoc
   V. Le. 2014.<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"> Sequence to sequence learning with neural networks</a>. NIPS.</li>
</ul>
<h1 class="page-title" id="bibtex">BibTex</h1>
<pre class="prettyprint"><code>@article{luong17,
  author  = {Minh{-}Thang Luong and Eugene Brevdo and Rui Zhao},
  title   = {Neural Machine Translation (seq2seq) Tutorial},
  journal = {https://github.com/tensorflow/nmt},
  year    = {2017},
}
</code></pre>

  </div>
  

  
        
  







        
<div class="devsite-content-footer nocontent">
  
  
    <p>Except as otherwise noted, the content of this page is licensed under the <a href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution 3.0 License</a>, and code samples are licensed under the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 License</a>. For details, see our <a href="https://developers.google.cn/terms/site-policies?hl=zh-cn">Site Policies</a>. Java is a registered trademark of Oracle and/or its affiliates.</p>
  

  
    
    <p class="devsite-content-footer-date" itemprop="datePublished"
       content="2018-01-27T01:30:38.200010">
      
      上次更新日期：一月 27, 2018
    </p>
  

</div>

        </article>
      </article>
  

        </div>
      

<footer class="devsite-footer-linkboxes nocontent
               devsite-footer-linkboxes-all-backup
               "><nav class="devsite-full-site-width"><ul class="devsite-footer-linkboxes-list"><li class="devsite-footer-linkbox devsite-footer-linkbox-backup"><h3 class="devsite-footer-linkbox-heading">把握动态</h3><ul class="devsite-footer-linkbox-list"><li class="devsite-footer-linkbox-item"><a href="https://research.googleblog.com/search/label/TensorFlow" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Blog Link">
                博客
            </a></li><li class="devsite-footer-linkbox-item"><a href="https://github.com/tensorflow/" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer GitHub Link">
                GitHub
            </a></li><li class="devsite-footer-linkbox-item"><a href="https://twitter.com/tensorflow" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Twitter Link">
                Twitter
            </a></li></ul></li><li class="devsite-footer-linkbox devsite-footer-linkbox-backup"><h3 class="devsite-footer-linkbox-heading">支持</h3><ul class="devsite-footer-linkbox-list"><li class="devsite-footer-linkbox-item"><a href="https://github.com/tensorflow/tensorflow/issues" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Issue Tracker Link">
                问题跟踪器
            </a></li><li class="devsite-footer-linkbox-item"><a href="https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Release Notes Link">
                版本说明
            </a></li><li class="devsite-footer-linkbox-item"><a href="https://stackoverflow.com/questions/tagged/tensorflow" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Stack Overflow Link">
                Stack Overflow
            </a></li></ul></li></ul></nav></footer><footer class="devsite-utility-footer"><nav class="devsite-utility-footer-nav devsite-nav devsite-full-site-width"><div class="devsite-utility-footer-nav-left"><form class="devsite-utility-footer-language" action="https://tensorflow.google.cn/i18n/setlang/" method="post"><input type="hidden" name="xsrf_token" value="V43RzizZ91ZrthEwy5-72kRPQTV2noPJfl4c_9LLAtY6MTUxNzg3OTExMzcyNjA3MA" /><input type="hidden" name="next" value="/tutorials/seq2seq"><select class="devsite-utility-footer-language-select kd-select" name="language"
                track-type="languageSelector" track-name="click"><option value="en"
                
                track-type="languageSelector" track-name="changed"
                track-metadata-original-language="zh-cn"
                track-metadata-selected-language="en">
            English
          </option><option value="zh-cn"
                 selected="selected" 
                track-type="languageSelector" track-name="changed"
                track-metadata-original-language="zh-cn"
                track-metadata-selected-language="zh-cn">
            简体中文
          </option></select></form><span class="devsite-utility-footer-links"><a class="devsite-utility-footer-link gc-analytics-event"
           href="http://www.google.cn/policies/terms/?hl=zh-cn"
           data-category="Site-Wide Custom Events"
           data-label="Footer terms link"
           data-footer-link-id="terms"
           >条款
         </a><a class="devsite-utility-footer-link gc-analytics-event"
           href="http://www.google.cn/policies/privacy/?hl=zh-cn"
           data-category="Site-Wide Custom Events"
           data-label="Footer privacy link"
           data-footer-link-id="privacy"
           
           data-cookie-policy="//www.google.cn/policies/technologies/cookies/"
           >隐私权
         </a></span></div></nav></footer></div><script async defer src="https://www.gstatic.com/feedback/api.js"></script><script src="../_static2/18908e5f48/jsi18n/index.html?hl=zh-cn"></script><script src="../_static/18908e5f48/js/script_foot_closure__zh_cn.js?hl=zh-cn"></script><script src="../_static/18908e5f48/js/script_foot.js?hl=zh-cn"></script><script>
        (function($) {
          
          devsite.devsite.Init($, {'FULL_SITE_SEARCH_ENABLED': 0, 'ENABLE_BLOCKED_VIDEO_PLACEHOLDER': 1, 'VERSION_HASH': '18908e5f48', 'SITE_NAME': 'tensorflow_china', 'HISTORY_ENABLED': 0, 'ENABLE_BLOCKED_LINK_TOOLTIP': 1, 'ALLOWED_HOSTS': ['.android.com', '.anvato.com', '.apigee.com', '.appspot.com', '.dialogflow.com', '.gonglchuangl.net', '.google.cn', '.google.com', '.googleplex.com', '.nest.com', '.openthread.io', '.openweave.io', '.orbitera.com', '.tensorflow.org'], 'BLOCK_RSS_FEEDS': 1, 'SCRIPTSAFE_DOMAIN': 'tensorflow-dot-google-developers.gonglchuangl.net'},
                               '[]','zh-cn',
                               true, '',
                               {"f62218c009ec029abef196bba5aa34cf": true, "098dafe57affddc137df300142652cfd": false, "8e03e230de0bd8a6fe173fdf172e8b3f": true, "cb025a64a50094835616312f4774a53d": true, "51470233c56fc1fde50f00b73c52b216": false, "d169d485cf24243a263783dbe42029b1": true, "039e5d84b87fd75807ffb37b7f1bbf2c": true, "752953480de00a336d911a46966cc16d": false, "700def1a83e356c06c0925afb05de4b0": false, "6749dcb526ce9bde6993550c7d928d24": true}, '/',
                               'https://tensorflow.google.cn/?hl=zh-cn');
        })(jQuery);

        
        devsite.localInit = function() {
          
        };

      </script><script>
      $('.devsite-utility-footer-language-select').each(function() {
        $(this).change(function(){$('.devsite-utility-footer-language').submit();});
      });
      </script></div><span id="devsite-request-elapsed" data-request-elapsed="369.770050049"></span></body></html>
