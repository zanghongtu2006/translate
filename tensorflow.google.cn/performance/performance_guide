




<!DOCTYPE html><html lang="en"
      class="chekov"><head><script>var a=window.devsite||{};window.devsite=a;a.readyCallbacks=[];window.devsite.readyCallbacks=a.readyCallbacks;a.ready=function(b){a.readyCallbacks.push(b)};window.devsite.ready=a.ready;
</script><meta charset="utf-8"><meta name="xsrf_token" content="wCA8ZahBW5ET-08ILBrdvET0sYOOw2d1GoChWSRe_to6MTUxNzg3MjY0NDU5NDMxMA" /><link rel="canonical" href="https://www.tensorflow.org/performance/performance_guide"><link rel="alternate" href="https://www.tensorflow.org/performance/performance_guide" hreflang="en"><link rel="alternate" href="performance_guide" hreflang="en-cn"><link rel="alternate" href="https://www.tensorflow.org/performance/performance_guide" hreflang="x-default"><link rel="shortcut icon" href="../_static/18908e5f48/images/tensorflow/favicon.png"><link rel="apple-touch-icon" href="../images/apple-touch-icon-180x180.png"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Roboto:300,400,400italic,500,500italic,700,700italic|Roboto+Mono:400,500,700|Material+Icons"><link rel="stylesheet"
        href="../_static/18908e5f48/css/devsite-tensorflow-orange.css"><script src="../_static/18908e5f48/js/jquery-bundle.js"></script><meta property="og:site_name" content="TensorFlow"><meta property="og:type" content="website"><meta property="og:url" content="https://www.tensorflow.org/performance/performance_guide"><meta property="og:locale" content="en"><script>
    var ___gcfg = ___gcfg || {};
    ___gcfg.lang = 'zh-cn';
  </script><title>Performance Guide &nbsp;|&nbsp; TensorFlow</title><meta property="og:title" content="Performance Guide &nbsp;|&nbsp; TensorFlow"></head><body class="
               devsite-doc-page
               
               
               
               
               "
        id="top_of_page"><div class="devsite-wrapper"><div class="devsite-top-section-wrapper
            "><header class="devsite-top-section nocontent"><div class="devsite-top-logo-row-wrapper-wrapper"><div class="devsite-top-logo-row-wrapper"><div class="devsite-top-logo-row devsite-full-site-width"><button type="button" class="devsite-expand-section-nav devsite-header-icon-button
                                       button-flat material-icons gc-analytics-event"
                  data-category="Site-Wide Custom Events" data-label="Hamburger menu"></button><div class="devsite-product-name-wrapper"><a href="../index.html" class="devsite-site-logo-link gc-analytics-event"
   data-category="Site-Wide Custom Events" data-label="Site logo"><img src="../_static/18908e5f48/images/tensorflow/lockup.png"
       class="devsite-site-logo" alt="TensorFlow"><span class="devsite-site-name devsite-product-name">TensorFlow</span></a></div><div class="devsite-header-upper-tabs"><nav class="devsite-doc-set-nav devsite-nav devsite-overflow-tabs-scroll-wrapper"><ul class="devsite-doc-set-nav-tab-list devsite-overflow-tabs-scroll"><li class="devsite-doc-set-nav-tab-container"><a href="../install/index"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Install"
       >
        Install
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../get_started/index"
         class="devsite-doc-set-nav-active
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Develop"
       >
        Develop
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../api_docs.1"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: API r1.5"
       >
        API r1.5
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../deploy/index"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Deploy"
       >
        Deploy
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../extend/index.html"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Extend"
       >
        Extend
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../community/index.html"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Community"
       >
        Community
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../versions/index.html"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Versions"
       >
        Versions
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../tfrc/index.html"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: TFRC"
       >
        TFRC
      </a></li></ul></nav></div><form class="devsite-search-form"
                action="https://tensorflow.google.cn/s/results/"
                method="GET"
                id="top-search"
                search-placeholder='搜索'><div id="searchbox" class="devsite-searchbox"><input placeholder='搜索'
         
         type="text"
         class="devsite-search-field devsite-search-query"
         name="q"
         value=""
         autocomplete="off"><div class="devsite-search-image material-icons"></div></div><input type="hidden"
       name="p"
       id="search_project"
       value="/"
       data-project-name="TensorFlow"
       data-project-path="/"
       data-query-match=""><input type="hidden" class="suggest-project" value="TensorFlow" /><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Android Developers" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Android Open Source Project" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Google Cloud Platform" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Dialogflow" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Firebase" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Link.app" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Nest Developers" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="OpenThread" ></div><div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="TensorFlow" ></div></form><button type="button" class="devsite-search-button devsite-header-icon-button button-flat
                                       material-icons"></button></div></div></div><div class="devsite-collapsible-section"><div class="devsite-header-background devsite-full-site-width"><div class="devsite-product-id-row devsite-full-site-width"><div class="devsite-product-description-row"><ul class="devsite-breadcrumb-list"><li class="devsite-breadcrumb-item">
    
    
    
      Develop
    
    
  </li></ul></div></div><div class="devsite-doc-set-nav-row devsite-full-site-width"><nav class="devsite-doc-set-nav devsite-nav devsite-overflow-tabs-scroll-wrapper"><ul class="devsite-doc-set-nav-tab-list devsite-overflow-tabs-scroll"><li class="devsite-doc-set-nav-tab-container"><a href="../get_started/index"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Get Started"
       >
        Get Started
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../programmers_guide/index.html"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Programmer&#39;s Guide"
       >
        Programmer&#39;s Guide
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../tutorials/index.html"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Tutorials"
       >
        Tutorials
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="index.html"
         class="devsite-doc-set-nav-active
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Performance"
       >
        Performance
      </a></li><li class="devsite-doc-set-nav-tab-container"><a href="../mobile/index.html"
         class="
                devsite-doc-set-nav-tab gc-analytics-event"
       
         data-category="Site-Wide Custom Events"
       
         data-label="Tab: Mobile"
       >
        Mobile
      </a></li></ul></nav></div></div></div></header><script>
    if (window.jQuery) {
      $(document).ready(function() {
        if (window.devsite && window.devsite.search) {
          
          window.devsite.search.init('zh-cn')
        }
      });
    }
  </script></div><div id="gc-wrapper"itemscope itemtype="http://schema.org/Article"><input class="google-analytics-id-json" type="hidden" value="{&quot;dimensions&quot;: {&quot;dimension6&quot;: &quot;en&quot;, &quot;dimension4&quot;: &quot;TensorFlow&quot;, &quot;dimension5&quot;: &quot;zh-cn&quot;, &quot;dimension3&quot;: false, &quot;dimension1&quot;: &quot;Signed out&quot;, &quot;dimension8&quot;: null}, &quot;gaid&quot;: &quot;UA-69864048-6&quot;}"><script>
      var dataLayer = [{"freeTrialEligibleUser": "False", "userCountry": "US", "language": {"requested": "zh-cn", "served": "en"}, "projectName": "TensorFlow", "scriptsafe": null, "signedIn": "False", "internalUser": "False"}];
    </script>

      
        <div class="devsite-site-mask"></div>
        
  

<nav class="devsite-nav-responsive devsite-nav nocontent" tabindex="0">
  
  <div class="devsite-nav-responsive-tabs-panel">
    
      
        



<nav class="devsite-nav-responsive-tabs devsite-nav">
  <ul class="devsite-nav-list">
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../install/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Install">
        Install
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../get_started/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                devsite-nav-active"
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Develop">
        Develop
      </a>
      
        



<nav class="devsite-nav-responsive-tabs devsite-nav">
  <ul class="devsite-nav-list">
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../get_started/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Get Started">
        Get Started
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../programmers_guide/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Programmer&#39;s Guide">
        Programmer&#39;s Guide
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../tutorials/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Tutorials">
        Tutorials
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <span class="devsite-nav-responsive-forward devsite-nav-responsive-tab devsite-nav-title
                   devsite-nav-active gc-analytics-event"
            data-category="Site-Wide Custom Events" data-label="Responsive Tab: Performance"
            tabindex="0">
        Performance
      </span>
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../mobile/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Mobile">
        Mobile
      </a>
      
    
    </li>
  
  
  </ul>
</nav>

      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../api_docs/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: API r1.5">
        API r1.5
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../deploy/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Deploy">
        Deploy
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../extend/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Extend">
        Extend
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../community/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Community">
        Community
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../versions/index.html?nav=true"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: Versions">
        Versions
      </a>
      
    
    </li>
  
    <li class="devsite-nav-item devsite-nav-item-heading">
    
      <a 
           href="../tfrc/index.html"
         
         class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                "
         data-category="Site-Wide Custom Events" data-label="Responsive Tab: TFRC">
        TFRC
      </a>
      
    
    </li>
  
  
  </ul>
</nav>

      
    
  </div>
  
  
  <div class="devsite-nav-responsive-sidebar-panel">
    
    <div class="devsite-nav-responsive-back" tabindex="0"></div>
    
    <nav class="devsite-nav-responsive-sidebar">
      <ul class="devsite-nav-list"><li class="devsite-nav-item"><a href="index.html" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Performance</a></li><li class="devsite-nav-item devsite-nav-active"><a href="performance_guide" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Performance Guide</a></li><li class="devsite-nav-item"><a href="performance_models" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">High-Performance Models</a></li><li class="devsite-nav-item"><a href="benchmarks" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Benchmarks</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">XLA</span></li><li class="devsite-nav-item"><a href="xla/index" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">XLA Overview</a></li><li class="devsite-nav-item"><a href="xla/broadcasting" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Broadcasting semantics</a></li><li class="devsite-nav-item"><a href="xla/developing_new_backend" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Developing a new backend for XLA</a></li><li class="devsite-nav-item"><a href="xla/jit" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Using JIT Compilation</a></li><li class="devsite-nav-item"><a href="xla/operation_semantics" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Operation Semantics</a></li><li class="devsite-nav-item"><a href="xla/shapes" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Shapes and Layout</a></li><li class="devsite-nav-item"><a href="xla/tfcompile" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Using AOT compilation</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Quantization</span></li><li class="devsite-nav-item"><a href="quantization" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">How to Quantize Neural Networks with TensorFlow</a></li><li class="devsite-nav-item"><hr class="devsite-nav-break"></li><li class="devsite-nav-item"><a href="../versions/index.html" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Versions</a></li></ul>
    </nav>
  </div>
  
</nav>


        <div class="devsite-main-content clearfix">

        
        

        
  
  
    
    
      
  <nav class="devsite-section-nav devsite-nav nocontent">
    <ul class="devsite-nav-list"><li class="devsite-nav-item"><a href="index.html" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Performance</a></li><li class="devsite-nav-item devsite-nav-active"><a href="performance_guide" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Performance Guide</a></li><li class="devsite-nav-item"><a href="performance_models" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">High-Performance Models</a></li><li class="devsite-nav-item"><a href="benchmarks" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Benchmarks</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">XLA</span></li><li class="devsite-nav-item"><a href="xla/index" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">XLA Overview</a></li><li class="devsite-nav-item"><a href="xla/broadcasting" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Broadcasting semantics</a></li><li class="devsite-nav-item"><a href="xla/developing_new_backend" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Developing a new backend for XLA</a></li><li class="devsite-nav-item"><a href="xla/jit" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Using JIT Compilation</a></li><li class="devsite-nav-item"><a href="xla/operation_semantics" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Operation Semantics</a></li><li class="devsite-nav-item"><a href="xla/shapes" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Shapes and Layout</a></li><li class="devsite-nav-item"><a href="xla/tfcompile" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Using AOT compilation</a></li><li class="devsite-nav-item devsite-nav-item-heading"><span class="devsite-nav-title devsite-nav-title-no-path" track-type="leftNav" track-name="expandNavSectionNoLink" track-metadata-position="0">Quantization</span></li><li class="devsite-nav-item"><a href="quantization" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">How to Quantize Neural Networks with TensorFlow</a></li><li class="devsite-nav-item"><hr class="devsite-nav-break"></li><li class="devsite-nav-item"><a href="../versions/index.html" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Versions</a></li></ul>
  </nav>

    

    
  <nav class="devsite-page-nav devsite-nav"></nav>


      <article class="devsite-article">
        <article class="devsite-article-inner">
  
          
  



  
  <nav class="devsite-breadcrumb-nav devsite-nav">
    


<ul class="devsite-breadcrumb-list">
  
  <li class="devsite-breadcrumb-item">
    
    
    <a href="../index.html" class="devsite-breadcrumb-link gc-analytics-event"
       data-category="Site-Wide Custom Events" data-label="Breadcrumbs"
       data-value="1">
    
    
      TensorFlow
    
    
    </a>
    
  </li>
  
  <li class="devsite-breadcrumb-item">
    
    
    <div class="devsite-breadcrumb-guillemet material-icons"></div>
    
    
    <a href="../get_started/index" class="devsite-breadcrumb-link gc-analytics-event"
       data-category="Site-Wide Custom Events" data-label="Breadcrumbs"
       data-value="2">
    
    
      Develop
    
    
    </a>
    
  </li>
  
  <li class="devsite-breadcrumb-item">
    
    
    <div class="devsite-breadcrumb-guillemet material-icons"></div>
    
    
    <a href="index.html" class="devsite-breadcrumb-link gc-analytics-event"
       data-category="Site-Wide Custom Events" data-label="Breadcrumbs"
       data-value="3">
    
    
      Performance
    
    
    </a>
    
  </li>
  
</ul>

  </nav>
  
  
  <h1 itemprop="name" class="devsite-page-title">
    Performance Guide
  </h1>
  
  
  <nav class="devsite-page-nav-embedded devsite-nav"></nav>
  
  <div class="devsite-article-body clearfix
            "
       itemprop="articleBody">
    
<script src="../_static/18908e5f48/js/managed/mathjax/MathJax.js?config=TeX-AMS-MML_SVG"></script>

<!-- DO NOT EDIT! Automatically generated file. -->


<p>This guide contains a collection of best practices for optimizing TensorFlow
code. The guide is divided into a few sections:</p>
<ul>
<li><a href="performance_guide#general_best_practices">General best practices</a> covers topics that are
    common across a variety of model types and hardware.</li>
<li><a href="performance_guide#optimizing_for_gpu">Optimizing for GPU</a> details tips specifically relevant
    to GPUs.</li>
<li><a href="performance_guide#optimizing_for_cpu">Optimizing for CPU</a> details CPU specific information.</li>
</ul>
<h2 id="general_best_practices">General best practices</h2>
<p>The sections below cover best practices that are relevant to a variety of
hardware and models. The best practices section is broken down into the
following sections:</p>
<ul>
<li><a href="performance_guide#input_pipeline_optimization">Input pipeline optimizations</a></li>
<li><a href="performance_guide#data_formats">Data formats</a></li>
<li><a href="performance_guide#common_fused_ops">Common fused Ops</a></li>
<li><a href="performance_guide#building_and_installing_from_source">Building and installing from source</a></li>
</ul>
<h3 id="input_pipeline_optimization">Input pipeline optimization</h3>
<p>Typical models retrieve data from disk and preprocess it before sending the data
through the network. For example, models that process JPEG images will follow
this flow: load image from disk, decode JPEG into a tensor, crop and pad,
possibly flip and distort, and then batch. This flow is referred to as the input
pipeline. As GPUs and other hardware accelerators get faster, preprocessing of
data can be a bottleneck.</p>
<p>Determining if the input pipeline is the bottleneck can be complicated. One of
the most straightforward methods is to reduce the model to a single operation
(trivial model) after the input pipeline and measure the examples per second. If
the difference in examples per second for the full model and the trivial model
is minimal then the input pipeline is likely a bottleneck. Below are some other
approaches to identifying issues:</p>
<ul>
<li>Check if a GPU is underutilized by running <code>nvidia-smi -l 2</code>. If GPU
    utilization is not approaching 80-100%, then the input pipeline may be the
    bottleneck.</li>
<li>Generate a timeline and look for large blocks of white space (waiting). An
    example of generating a timeline exists as part of the <a href="xla/jit">XLA JIT</a>
    tutorial.</li>
<li>Check CPU usage. It is possible to have an optimized input pipeline and lack
    the CPU cycles to process the pipeline.</li>
<li>Estimate the throughput needed and verify the disk used is capable of that
    level of throughput. Some cloud solutions have network attached disks that
    start as low as 50 MB/sec, which is slower than spinning disks (150 MB/sec),
    SATA SSDs (500 MB/sec), and PCIe SSDs (2,000+ MB/sec).</li>
</ul>
<h4 id="preprocessing_on_the_cpu">Preprocessing on the CPU</h4>
<p>Placing input pipeline operations on the CPU can significantly improve
performance. Utilizing the CPU for the input pipeline frees the GPU to focus on
training. To ensure preprocessing is on the CPU, wrap the preprocessing
operations as shown below:</p>
<pre class="prettyprint lang-python"><code>with tf.device('/cpu:0'):
  # function to get and process images or data.
  distorted_inputs = load_and_distort_images()
</code></pre>

<p>If using <code>tf.estimator.Estimator</code> the input function is automatically placed on
the CPU.</p>
<h4 id="using_the_dataset_api">Using the Dataset API</h4>
<p>The <a href="../programmers_guide/datasets">Dataset API</a> is replacing <code>queue_runner</code> as the recommended API
for building input pipelines. The API was added to contrib as part of TensorFlow
1.2 and will move to core in the near future. This
<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator/cifar10_main.py">ResNet example</a>
(<a href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>)
training CIFAR-10 illustrates the use of the Dataset API along with
<code>tf.estimator.Estimator</code>. The Dataset API utilizes C++ multi-threading and has a
much lower overhead than the Python-based <code>queue_runner</code> that is limited by
Python's multi-threading performance.</p>
<p>While feeding data using a <code>feed_dict</code> offers a high level of flexibility, in
most instances using <code>feed_dict</code> does not scale optimally. However, in instances
where only a single GPU is being used the difference can be negligible. Using
the Dataset API is still strongly recommended. Try to avoid the following:</p>
<pre class="prettyprint lang-python"><code># feed_dict often results in suboptimal performance when using large inputs.
sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre>

<h4 id="fused_decode_and_crop">Fused decode and crop</h4>
<p>If inputs are JPEG images that also require cropping, use fused
<a href="../api_docs/python/tf/image/decode_and_crop_jpeg"><code>tf.image.decode_and_crop_jpeg</code></a> to speed up preprocessing.
<code>tf.image.decode_and_crop_jpeg</code> only decodes the part of
the image within the crop window. This significantly speeds up the process if
the crop window is much smaller than the full image. For imagenet data, this
approach could speed up the input pipeline by up to 30%.</p>
<p>Example Usage:</p>
<pre class="prettyprint lang-python"><code>def _image_preprocess_fn(image_buffer):
    # image_buffer 1-D string Tensor representing the raw JPEG image buffer.

    # Extract image shape from raw JPEG image buffer.
    image_shape = tf.image.extract_jpeg_shape(image_buffer)

    # Get a crop window with distorted bounding box.
    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(
      image_shape, ...)
    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box

    # Decode and crop image.
    offset_y, offset_x, _ = tf.unstack(bbox_begin)
    target_height, target_width, _ = tf.unstack(bbox_size)
    crop_window = tf.stack([offset_y, offset_x, target_height, target_width])
    cropped_image = tf.image.decode_and_crop_jpeg(image, crop_window)
</code></pre>

<p><code>tf.image.decode_and_crop_jpeg</code> is available on all platforms. There is no speed
up on Windows due to the use of <code>libjpeg</code> vs. <code>libjpeg-turbo</code> on other
platforms.</p>
<h4 id="use_large_files">Use large files</h4>
<p>Reading large numbers of small files significantly impacts I/O performance.
One approach to get maximum I/O throughput is to preprocess input data into
larger (~100MB) <code>TFRecord</code> files. For smaller data sets (200MB-1GB), the best
approach is often to load the entire data set into memory. The document
<a href="https://github.com/tensorflow/models/tree/master/research/slim#downloading-and-converting-to-tfrecord-format">Downloading and converting to TFRecord format</a>
includes information and scripts for creating <code>TFRecords</code> and this
<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator/generate_cifar10_tfrecords.py">script</a>
converts the CIFAR-10 data set into <code>TFRecords</code>.</p>
<h3 id="data_formats">Data formats</h3>
<p>Data formats refers to the structure of the Tensor passed to a given Op. The
discussion below is specifically about 4D Tensors representing images. In
TensorFlow the parts of the 4D tensor are often referred to by the following
letters:</p>
<ul>
<li>N refers to the number of images in a batch.</li>
<li>H refers to the number of pixels in the vertical (height) dimension.</li>
<li>W refers to the number of pixels in the horizontal (width) dimension.</li>
<li>C refers to the channels. For example, 1 for black and white or grayscale
    and 3 for RGB.</li>
</ul>
<p>Within TensorFlow there are two naming conventions representing the two most
common data formats:</p>
<ul>
<li><code>NCHW</code> or <code>channels_first</code></li>
<li><code>NHWC</code> or <code>channels_last</code></li>
</ul>
<p><code>NHWC</code> is the TensorFlow default and <code>NCHW</code> is the optimal format to use when
training on NVIDIA GPUs using <a href="https://developer.nvidia.com/cudnn">cuDNN</a>.</p>
<p>The best practice is to build models that work with both data formats. This
simplifies training on GPUs and then running inference on CPUs. If TensorFlow is
compiled with the <a href="performance_guide#tensorflow_with_intel_mkl_dnn">Intel MKL</a> optimizations,
many operations, especially those related to CNN based models, will be optimized
and support <code>NCHW</code>. If not using the MKL, some operations are not supported on
CPU when using <code>NCHW</code>.</p>
<p>The brief history of these two formats is that TensorFlow started by using
<code>NHWC</code> because it was a little faster on CPUs. In the long term, we are working
on tools to auto rewrite graphs to make switching between the formats
transparent and take advantages of micro optimizations where a GPU Op may be
faster using <code>NHWC</code> than the normally most efficient <code>NCHW</code>.</p>
<h3 id="common_fused_ops">Common fused Ops</h3>
<p>Fused Ops combine multiple operations into a single kernel for improved
performance. There are many fused Ops within TensorFlow and <a href="xla/index">XLA</a> will
create fused Ops when possible to automatically improve performance. Collected
below are select fused Ops that can greatly improve performance and may be
overlooked.</p>
<h4 id="fused_batch_norm">Fused batch norm</h4>
<p>Fused batch norm combines the multiple operations needed to do batch
normalization into a single kernel. Batch norm is an expensive process that for
some models makes up a large percentage of the operation time. Using fused batch
norm can result in a 12%-30% speedup.</p>
<p>There are two commonly used batch norms and both support fusing. The core
<a href="../api_docs/python/tf/layers/batch_normalization"><code>tf.layers.batch_normalization</code></a> added fused starting in TensorFlow 1.3.</p>
<pre class="prettyprint lang-python"><code>bn = tf.layers.batch_normalization(
    input_layer, fused=True, data_format='NCHW')
</code></pre>

<p>The contrib <a href="../api_docs/python/tf/contrib/layers/batch_norm"><code>tf.contrib.layers.batch_norm</code></a> method has had fused as an option
since before TensorFlow 1.0.</p>
<pre class="prettyprint lang-python"><code>bn = tf.contrib.layers.batch_norm(input_layer, fused=True, data_format='NCHW')
</code></pre>

<h3 id="building_and_installing_from_source">Building and installing from source</h3>
<p>The default TensorFlow binaries target the broadest range of hardware to make
TensorFlow accessible to everyone. If using CPUs for training or inference, it
is recommended to compile TensorFlow with all of the optimizations available for
the CPU in use. Speedups for training and inference on CPU are documented below
in <a href="performance_guide#comparing_compiler_optimizations">Comparing compiler optimizations</a>.</p>
<p>To install the most optimized version of TensorFlow,
<a href="../install/install_sources">build and install</a> from source. If there is a need to build
TensorFlow on a platform that has different hardware than the target, then
cross-compile with the highest optimizations for the target platform. The
following command is an example of using <code>bazel</code> to compile for a specific
platform:</p>
<pre class="prettyprint lang-python"><code># This command optimizes for Intel’s Broadwell processor
bazel build -c opt --copt=-march=&quot;broadwell&quot; --config=cuda //tensorflow/tools/pip_package:build_pip_package

</code></pre>

<h4 id="environment_build_and_install_tips">Environment, build, and install tips</h4>
<ul>
<li><code>./configure</code> asks which compute capability to include in the build. This
    does not impact overall performance but does impact initial startup. After
    running TensorFlow once, the compiled kernels are cached by CUDA. If using
    a docker container, the data is not cached and the penalty is paid each time
    TensorFlow starts. The best practice is to include the
    <a href="http://developer.nvidia.com/cuda-gpus">compute capabilities</a>
    of the GPUs that will be used, e.g. P100: 6.0, Titan X (Pascal): 6.1, Titan
    X (Maxwell): 5.2, and K80: 3.7.</li>
<li>Use a version of gcc that supports all of the optimizations of the target
    CPU. The recommended minimum gcc version is 4.8.3. On OS X, upgrade to the
    latest Xcode version and use the version of clang that comes with Xcode.</li>
<li>Install the latest stable CUDA platform and cuDNN libraries supported by
    TensorFlow.</li>
</ul>
<h2 id="optimizing_for_gpu">Optimizing for GPU</h2>
<p>This section contains GPU-specific tips that are not covered in the
<a href="performance_guide#general_best_practices">General best practices</a>. Obtaining optimal performance
on multi-GPUs is a challenge. A common approach is to use data parallelism.
Scaling through the use of data parallelism involves making multiple copies of
the model, which are referred to as "towers", and then placing one tower on each
of the GPUs. Each tower operates on a different mini-batch of data and then
updates variables, also known as parameters, that need to be shared between
each of the towers. How each tower gets the updated variables and how the
gradients are applied has an impact on the performance, scaling, and convergence
of the model.  The rest of this section provides an overview of variable
placement and the towering of a model on multiple GPUs.
<a href="performance_models">High-Performance Models</a> gets into more details regarding
more complex methods that can be used to share and update variables between
towers.</p>
<p>The best approach to handling variable updates depends on the model, hardware,
and even how the hardware has been configured. An example of this, is that two
systems can be built with NVIDIA Tesla P100s but one may be using PCIe and the
other <a href="http://www.nvidia.com/object/nvlink.html">NVLink</a>. In that scenario, the
optimal solution for each system may be different. For real world examples, read
the <a href="benchmarks">benchmark</a> page which details the settings that
were optimal for a variety of platforms. Below is a summary of what was learned
from benchmarking various platforms and configurations:</p>
<ul>
<li>
<p><strong>Tesla K80</strong>: If the GPUs are on the same PCI Express root complex and are
    able to use <a href="https://developer.nvidia.com/gpudirect">NVIDIA GPUDirect</a> Peer
    to Peer, then placing the variables equally across the GPUs used for
    training is the best approach. If the GPUs cannot use GPUDirect, then
    placing the variables on the CPU is the best option.</p>
</li>
<li>
<p><strong>Titan X (Maxwell and Pascal), M40, P100, and similar</strong>: For models like
    ResNet and InceptionV3, placing variables on the CPU is the optimal setting,
    but for models with a lot of variables like AlexNet and VGG, using GPUs with
    <code>NCCL</code> is better.</p>
</li>
</ul>
<p>A common approach to managing where variables are placed, is to create a method
to determine where each Op is to be placed and use that method in place of a
specific device name when calling <code>with tf.device():</code>. Consider a scenario where
a model is being trained on 2 GPUs and the variables are to be placed on the
CPU. There would be a loop for creating and placing the "towers" on each of the
2 GPUs. A custom device placement method would be created that watches for Ops
of type <code>Variable</code>, <code>VariableV2</code>, and <code>VarHandleOp</code> and indicates that they are
to be placed on the CPU. All other Ops would be placed on the target GPU.
The building of the graph would proceed as follows:</p>
<ul>
<li>
<p>On the first loop a "tower" of the model would be created for <code>gpu:0</code>.
    During the placement of the Ops, the custom device placement method would
    indicate that variables are to be placed on <code>cpu:0</code> and all other Ops on
    <code>gpu:0</code>.</p>
</li>
<li>
<p>On the second loop, <code>reuse</code> is set to <code>True</code> to indicate that variables are
    to be reused and then the "tower" is created on <code>gpu:1</code>. During the
    placement of the Ops associated with the "tower", the variables that were
    placed on <code>cpu:0</code> are reused and all other Ops are created and placed on
    <code>gpu:1</code>.</p>
</li>
</ul>
<p>The final result is all of the variables are placed on the CPU with each GPU
having a copy of all of the computational Ops associated with the model.</p>
<p>The code snippet below illustrates two different approaches for variable
placement: one is placing variables on the CPU; the other is placing variables
equally across the GPUs.</p>
<pre class="prettyprint lang-python"><code>
class GpuParamServerDeviceSetter(object):
  &quot;&quot;&quot;Used with tf.device() to place variables on the least loaded GPU.

    A common use for this class is to pass a list of GPU devices, e.g. ['gpu:0',
    'gpu:1','gpu:2'], as ps_devices.  When each variable is placed, it will be
    placed on the least loaded gpu. All other Ops, which will be the computation
    Ops, will be placed on the worker_device.
  &quot;&quot;&quot;

  def __init__(self, worker_device, ps_devices):
    &quot;&quot;&quot;Initializer for GpuParamServerDeviceSetter.
    Args:
      worker_device: the device to use for computation Ops.
      ps_devices: a list of devices to use for Variable Ops. Each variable is
      assigned to the least loaded device.
    &quot;&quot;&quot;
    self.ps_devices = ps_devices
    self.worker_device = worker_device
    self.ps_sizes = [0] * len(self.ps_devices)

  def __call__(self, op):
    if op.device:
      return op.device
    if op.type not in ['Variable', 'VariableV2', 'VarHandleOp']:
      return self.worker_device

    # Gets the least loaded ps_device
    device_index, _ = min(enumerate(self.ps_sizes), key=operator.itemgetter(1))
    device_name = self.ps_devices[device_index]
    var_size = op.outputs[0].get_shape().num_elements()
    self.ps_sizes[device_index] += var_size

    return device_name

def _create_device_setter(is_cpu_ps, worker, num_gpus):
  &quot;&quot;&quot;Create device setter object.&quot;&quot;&quot;
  if is_cpu_ps:
    # tf.train.replica_device_setter supports placing variables on the CPU, all
    # on one GPU, or on ps_servers defined in a cluster_spec.
    return tf.train.replica_device_setter(
        worker_device=worker, ps_device='/cpu:0', ps_tasks=1)
  else:
    gpus = ['/gpu:%d' % i for i in range(num_gpus)]
    return ParamServerDeviceSetter(worker, gpus)

# The method below is a modified snippet from the full example.
def _resnet_model_fn():
    # When set to False, variables are placed on the least loaded GPU. If set
    # to True, the variables will be placed on the CPU.
    is_cpu_ps = False

    # Loops over the number of GPUs and creates a copy (&quot;tower&quot;) of the model on
    # each GPU.
    for i in range(num_gpus):
      worker = '/gpu:%d' % i
      # Creates a device setter used to determine where Ops are to be placed.
      device_setter = _create_device_setter(is_cpu_ps, worker, FLAGS.num_gpus)
      # Creates variables on the first loop.  On subsequent loops reuse is set
      # to True, which results in the &quot;towers&quot; sharing variables.
      with tf.variable_scope('resnet', reuse=bool(i != 0)):
        with tf.name_scope('tower_%d' % i) as name_scope:
          # tf.device calls the device_setter for each Op that is created.
          # device_setter returns the device the Op is to be placed on.
          with tf.device(device_setter):
            # Creates the &quot;tower&quot;.
            _tower_fn(is_training, weight_decay, tower_features[i],
                      tower_labels[i], tower_losses, tower_gradvars,
                      tower_preds, False)

</code></pre>

<p>In the near future the above code will be for illustration purposes only as
there will be easy to use high level methods to support a wide range of popular
approaches. This
<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator">example</a>
will continue to get updated as the API expands and evolves to address multi-GPU
scenarios.</p>
<h2 id="optimizing_for_cpu">Optimizing for CPU</h2>
<p>CPUs, which includes Intel® Xeon Phi™, achieve optimal performance when
TensorFlow is <a href="../install/install_sources">built from source</a> with all of the instructions
supported by the target CPU.</p>
<p>Beyond using the latest instruction sets, Intel® has added support for the
Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) to
TensorFlow. While the name is not completely accurate, these optimizations are
often simply referred to as 'MKL' or 'TensorFlow with MKL'. <a href="performance_guide#tensorflow_with_intel_mkl_dnn">TensorFlow
with Intel® MKL-DNN</a> contains details on the
MKL optimizations.</p>
<p>The two configurations listed below are used to optimize CPU performance by
adjusting the thread pools.</p>
<ul>
<li><code>intra_op_parallelism_threads</code>: Nodes that can use multiple threads to
    parallelize their execution will schedule the individual pieces into this
    pool.</li>
<li><code>inter_op_parallelism_threads</code>: All ready nodes are scheduled in this pool.</li>
</ul>
<p>These configurations are set via the <code>tf.ConfigProto</code> and passed to <code>tf.Session</code>
in the <code>config</code> attribute as shown in the snippet below.  For both configuration
options, if they are unset or set to 0, will default to the number of logical
CPU cores. Testing has shown that the default is effective for systems ranging
from one CPU with 4 cores to multiple CPUs with 70+ combined logical cores.
A common alternative optimization is to set the number of threads in both pools
equal to the number of physical cores rather than logical cores.</p>
<pre class="prettyprint lang-python"><code>
  config = tf.ConfigProto()
  config.intra_op_parallelism_threads = 44
  config.inter_op_parallelism_threads = 44
  tf.session(config=config)

</code></pre>

<p>The <a href="performance_guide#comparing_compiler_optimizations">Comparing compiler optimizations</a>
section contains the results of tests that used different compiler
optimizations.</p>
<h3 id="tensorflow_with_intel_mkl_dnn">TensorFlow with Intel® MKL DNN</h3>
<p>Intel® has added optimizations to TensorFlow for Intel® Xeon® and Intel® Xeon
Phi™ though the use of Intel® Math Kernel Library for Deep Neural Networks
(Intel® MKL-DNN) optimized primitives. The optimizations also provide speedups
for the consumer line of processors, e.g. i5 and i7 Intel processors. The Intel
published paper
<a href="https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture">TensorFlow* Optimizations on Modern Intel® Architecture</a>
contains additional details on the implementation.</p>
<blockquote>
<aside class="note"><strong>Note:</strong><span> MKL was added as of TensorFlow 1.2 and currently only works on Linux. It
also does not work when also using <code>--config=cuda</code>.</span></aside>
</blockquote>
<p>In addition to providing significant performance improvements for training CNN
based models, compiling with the MKL creates a binary that is optimized for AVX
and AVX2. The result is a single binary that is optimized and compatible with
most modern (post-2011) processors.</p>
<p>TensorFlow can be compiled with the MKL optimizations using the following
commands that depending on the version of the TensorFlow source used.</p>
<p>For TensorFlow source versions after 1.3.0:</p>
<pre class="prettyprint lang-bash"><code>./configure
# Pick the desired options
bazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package

</code></pre>

<p>For TensorFlow versions 1.2.0 through 1.3.0:</p>
<pre class="prettyprint lang-bash"><code>./configure
Do you wish to build TensorFlow with MKL support? [y/N] Y
Do you wish to download MKL LIB from the web? [Y/n] Y
# Select the defaults for the rest of the options.

bazel build --config=mkl --copt=&quot;-DEIGEN_USE_VML&quot; -c opt //tensorflow/tools/pip_package:build_pip_package

</code></pre>

<h4 id="tuning_mkl_for_the_best_performance">Tuning MKL for the best performance</h4>
<p>This section details the different configurations and environment variables that
can be used to tune the MKL to get optimal performance. Before tweaking various
environment variables make sure the model is using the <code>NCHW</code> (<code>channels_first</code>)
<a href="performance_guide#data_formats">data format</a>. The MKL is optimized for <code>NCHW</code> and Intel is
working to get near performance parity when using <code>NHWC</code>.</p>
<p>MKL uses the following environment variables to tune performance:</p>
<ul>
<li>KMP_BLOCKTIME - Sets the time, in milliseconds, that a thread should wait,
    after completing the execution of a parallel region, before sleeping.</li>
<li>KMP_AFFINITY - Enables the run-time library to bind threads to physical
    processing units.</li>
<li>KMP_SETTINGS - Enables (true) or disables (false) the printing of OpenMP*
    run-time library environment variables during program execution.</li>
<li>OMP_NUM_THREADS - Specifies the number of threads to use.</li>
</ul>
<p>More details on the KMP variables are on
<a href="https://software.intel.com/en-us/node/522775">Intel's</a> site and the OMP
variables on
<a href="https://gcc.gnu.org/onlinedocs/libgomp/Environment-Variables.html">gnu.org</a></p>
<p>While there can be substantial gains from adjusting the environment variables,
which is discussed below, the simplified advice is to set the
<code>inter_op_parallelism_threads</code> equal to the number of physical CPUs and to set
the following environment variables:</p>
<ul>
<li>KMP_BLOCKTIME=0</li>
<li>KMP_AFFINITY=granularity=fine,verbose,compact,1,0</li>
</ul>
<p>Example setting MKL variables with command-line arguments:</p>
<pre class="prettyprint lang-bash"><code>KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \
KMP_SETTINGS=1 python your_python_script.py
</code></pre>

<p>Example setting MKL variables with python <code>os.environ</code>:</p>
<pre class="prettyprint lang-python"><code>os.environ[&quot;KMP_BLOCKTIME&quot;] = str(FLAGS.kmp_blocktime)
os.environ[&quot;KMP_SETTINGS&quot;] = str(FLAGS.kmp_settings)
os.environ[&quot;KMP_AFFINITY&quot;]= FLAGS.kmp_affinity
if FLAGS.num_intra_threads &gt; 0:
  os.environ[&quot;OMP_NUM_THREADS&quot;]= str(FLAGS.num_intra_threads)

</code></pre>

<p>There are models and hardware platforms that benefit from different settings.
Each variable that impacts performance is discussed below.</p>
<ul>
<li>
<p><strong>KMP_BLOCKTIME</strong>: The MKL default is 200ms, which was not optimal in our
    testing. 0 (0ms) was a good default for CNN based models that were tested.
    The best performance for AlexNex was achieved at 30ms and both GoogleNet and
    VGG11 performed best set at 1ms.</p>
</li>
<li>
<p><strong>KMP_AFFINITY</strong>: The recommended setting is
    <code>granularity=fine,verbose,compact,1,0</code>.</p>
</li>
<li>
<p><strong>OMP_NUM_THREADS</strong>: This defaults to the number of physical cores.
    Adjusting this parameter beyond matching the number of cores can have an
    impact when using Intel® Xeon Phi™ (Knights Landing) for some models. See
    <a href="https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture">TensorFlow* Optimizations on Modern Intel® Architecture</a>
    for optimal settings.</p>
</li>
<li>
<p><strong>intra_op_parallelism_threads</strong>: Setting this equal to the number of
    physical cores is recommended. Setting the value to 0, which is the default
    and will result in the value being set to the number of logical cores, is an
    option to try for some architectures.  This value and <code>OMP_NUM_THREADS</code>
    should be equal.</p>
</li>
<li>
<p><strong>inter_op_parallelism_threads</strong>: Setting this equal to the number of
    sockets is recommended. Setting the value to 0, which is the default,
    results in the value being set to the number of logical cores.</p>
</li>
</ul>
<h3 id="comparing_compiler_optimizations">Comparing compiler optimizations</h3>
<p>Collected below are performance results running training and inference on
different types of CPUs on different platforms with various compiler
optimizations.  The models used were ResNet-50
(<a href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>) and
InceptionV3 (<a href="https://arxiv.org/abs/1512.00567">arXiv:1512.00567</a>).</p>
<p>For each test, when the MKL optimization was used the environment variable
KMP_BLOCKTIME was set to 0 (0ms) and KMP_AFFINITY to
<code>granularity=fine,verbose,compact,1,0</code>.</p>
<h4 id="inference_inceptionv3">Inference InceptionV3</h4>
<p><strong>Environment</strong></p>
<ul>
<li>Instance Type: AWS EC2 m4.xlarge</li>
<li>CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)</li>
<li>Dataset: ImageNet</li>
<li>TensorFlow Version: 1.2.0 RC2</li>
<li>Test Script: <a href="https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py">tf_cnn_benchmarks.py</a></li>
</ul>
<p><strong>Batch Size: 1</strong></p>
<p>Command executed for the MKL test:</p>
<pre class="prettyprint lang-bash"><code>python tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \
--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \
--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \
--data_dir=&lt;path to ImageNet TFRecords&gt;
</code></pre>

<table>
<thead>
<tr>
<th>Optimization</th>
<th>Data Format</th>
<th>Images/Sec  (step time)</th>
<th>Intra threads</th>
<th>Inter Threads</th>
</tr>
</thead>
<tbody>
<tr>
<td>AVX2</td>
<td>NHWC</td>
<td>7.0 (142ms)</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>MKL</td>
<td>NCHW</td>
<td>6.6 (152ms)</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>AVX</td>
<td>NHWC</td>
<td>5.0 (202ms)</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>SSE3</td>
<td>NHWC</td>
<td>2.8 (361ms)</td>
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<p><strong>Batch Size: 32</strong></p>
<p>Command executed for the MKL test:</p>
<pre class="prettyprint lang-bash"><code>python tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \
--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \
--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \
--data_dir=&lt;path to ImageNet TFRecords&gt;
</code></pre>

<table>
<thead>
<tr>
<th>Optimization</th>
<th>Data Format</th>
<th>Images/Sec  (step time)</th>
<th>Intra threads</th>
<th>Inter Threads</th>
</tr>
</thead>
<tbody>
<tr>
<td>MKL</td>
<td>NCHW</td>
<td>10.3  (3,104ms)</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>AVX2</td>
<td>NHWC</td>
<td>7.5 (4,255ms)</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>AVX</td>
<td>NHWC</td>
<td>5.1 (6,275ms)</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>SSE3</td>
<td>NHWC</td>
<td>2.8 (11,428ms)</td>
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<h4 id="inference_resnet-50">Inference ResNet-50</h4>
<p><strong>Environment</strong></p>
<ul>
<li>Instance Type: AWS EC2 m4.xlarge</li>
<li>CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)</li>
<li>Dataset: ImageNet</li>
<li>TensorFlow Version: 1.2.0 RC2</li>
<li>Test Script: <a href="https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py">tf_cnn_benchmarks.py</a></li>
</ul>
<p><strong>Batch Size: 1</strong></p>
<p>Command executed for the MKL test:</p>
<pre class="prettyprint lang-bash"><code>python tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \
--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \
--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \
--data_dir=&lt;path to ImageNet TFRecords&gt;
</code></pre>

<table>
<thead>
<tr>
<th>Optimization</th>
<th>Data Format</th>
<th>Images/Sec  (step time)</th>
<th>Intra threads</th>
<th>Inter Threads</th>
</tr>
</thead>
<tbody>
<tr>
<td>AVX2</td>
<td>NHWC</td>
<td>8.8 (113ms)</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>MKL</td>
<td>NCHW</td>
<td>8.5 (120ms)</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>AVX</td>
<td>NHWC</td>
<td>6.4 (157ms)</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>SSE3</td>
<td>NHWC</td>
<td>3.7 (270ms)</td>
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<p><strong>Batch Size: 32</strong></p>
<p>Command executed for the MKL test:</p>
<pre class="prettyprint lang-bash"><code>python tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \
--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \
--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \
--data_dir=&lt;path to ImageNet TFRecords&gt;
</code></pre>

<table>
<thead>
<tr>
<th>Optimization</th>
<th>Data Format</th>
<th>Images/Sec  (step time)</th>
<th>Intra threads</th>
<th>Inter Threads</th>
</tr>
</thead>
<tbody>
<tr>
<td>MKL</td>
<td>NCHW</td>
<td>12.4  (2,590ms)</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>AVX2</td>
<td>NHWC</td>
<td>10.4 (3,079ms)</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>AVX</td>
<td>NHWC</td>
<td>7.3 (4,4416ms)</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>SSE3</td>
<td>NHWC</td>
<td>4.0 (8,054ms)</td>
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<h4 id="training_inceptionv3">Training InceptionV3</h4>
<p><strong>Environment</strong></p>
<ul>
<li>Instance Type: Dedicated AWS EC2 r4.16xlarge (Broadwell)</li>
<li>CPU: Intel Xeon E5-2686 v4 (Broadwell) Processors</li>
<li>Dataset: ImageNet</li>
<li>TensorFlow Version: 1.2.0 RC2</li>
<li>Test Script: <a href="https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py">tf_cnn_benchmarks.py</a></li>
</ul>
<p>Command executed for MKL test:</p>
<pre class="prettyprint lang-bash"><code>python tf_cnn_benchmarks.py --device=cpu --mkl=True --kmp_blocktime=0 \
--nodistortions --model=resnet50 --data_format=NCHW --batch_size=32 \
--num_inter_threads=2 --num_intra_threads=36 \
--data_dir=&lt;path to ImageNet TFRecords&gt;
</code></pre>

<table>
<thead>
<tr>
<th>Optimization</th>
<th>Data Format</th>
<th>Images/Sec</th>
<th>Intra threads</th>
<th>Inter Threads</th>
</tr>
</thead>
<tbody>
<tr>
<td>MKL</td>
<td>NCHW</td>
<td>20.8</td>
<td>36</td>
<td>2</td>
</tr>
<tr>
<td>AVX2</td>
<td>NHWC</td>
<td>6.2</td>
<td>36</td>
<td>0</td>
</tr>
<tr>
<td>AVX</td>
<td>NHWC</td>
<td>5.7</td>
<td>36</td>
<td>0</td>
</tr>
<tr>
<td>SSE3</td>
<td>NHWC</td>
<td>4.3</td>
<td>36</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>ResNet and <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a>
were also run on this configuration but in an ad hoc manner. There were not
enough runs executed to publish a coherent table of results. The incomplete
results strongly indicated the final result would be similar to the table above
with MKL providing significant 3x+ gains over AVX2.</p>

  </div>
  

  
        
  







        
<div class="devsite-content-footer nocontent">
  
  
    <p>Except as otherwise noted, the content of this page is licensed under the <a href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution 3.0 License</a>, and code samples are licensed under the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 License</a>. For details, see our <a href="https://developers.google.cn/terms/site-policies">Site Policies</a>. Java is a registered trademark of Oracle and/or its affiliates.</p>
  

  
    
    <p class="devsite-content-footer-date" itemprop="datePublished"
       content="2018-01-27T01:30:38.287860">
      
      上次更新日期：一月 27, 2018
    </p>
  

</div>

        </article>
      </article>
  

        </div>
      

<footer class="devsite-footer-linkboxes nocontent
               devsite-footer-linkboxes-all-backup
               "><nav class="devsite-full-site-width"><ul class="devsite-footer-linkboxes-list"><li class="devsite-footer-linkbox devsite-footer-linkbox-backup"><h3 class="devsite-footer-linkbox-heading">把握动态</h3><ul class="devsite-footer-linkbox-list"><li class="devsite-footer-linkbox-item"><a href="https://research.googleblog.com/search/label/TensorFlow" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Blog Link">
                博客
            </a></li><li class="devsite-footer-linkbox-item"><a href="https://github.com/tensorflow/" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer GitHub Link">
                GitHub
            </a></li><li class="devsite-footer-linkbox-item"><a href="https://twitter.com/tensorflow" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Twitter Link">
                Twitter
            </a></li></ul></li><li class="devsite-footer-linkbox devsite-footer-linkbox-backup"><h3 class="devsite-footer-linkbox-heading">支持</h3><ul class="devsite-footer-linkbox-list"><li class="devsite-footer-linkbox-item"><a href="https://github.com/tensorflow/tensorflow/issues" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Issue Tracker Link">
                问题跟踪器
            </a></li><li class="devsite-footer-linkbox-item"><a href="https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Release Notes Link">
                版本说明
            </a></li><li class="devsite-footer-linkbox-item"><a href="https://stackoverflow.com/questions/tagged/tensorflow" class="gc-analytics-event"
               data-category="Site-Wide Custom Events"
               data-label="Footer Stack Overflow Link">
                Stack Overflow
            </a></li></ul></li></ul></nav></footer><footer class="devsite-utility-footer"><nav class="devsite-utility-footer-nav devsite-nav devsite-full-site-width"><div class="devsite-utility-footer-nav-left"><form class="devsite-utility-footer-language" action="https://tensorflow.google.cn/i18n/setlang/" method="post"><input type="hidden" name="xsrf_token" value="wCA8ZahBW5ET-08ILBrdvET0sYOOw2d1GoChWSRe_to6MTUxNzg3MjY0NDU5NDMxMA" /><input type="hidden" name="next" value="/performance/performance_guide"><select class="devsite-utility-footer-language-select kd-select" name="language"
                track-type="languageSelector" track-name="click"><option value="en"
                
                track-type="languageSelector" track-name="changed"
                track-metadata-original-language="zh-cn"
                track-metadata-selected-language="en">
            English
          </option><option value="zh-cn"
                 selected="selected" 
                track-type="languageSelector" track-name="changed"
                track-metadata-original-language="zh-cn"
                track-metadata-selected-language="zh-cn">
            简体中文
          </option></select></form><span class="devsite-utility-footer-links"><a class="devsite-utility-footer-link gc-analytics-event"
           href="http://www.google.cn/policies/terms/"
           data-category="Site-Wide Custom Events"
           data-label="Footer terms link"
           data-footer-link-id="terms"
           >条款
         </a><a class="devsite-utility-footer-link gc-analytics-event"
           href="http://www.google.cn/policies/privacy/"
           data-category="Site-Wide Custom Events"
           data-label="Footer privacy link"
           data-footer-link-id="privacy"
           
           data-cookie-policy="//www.google.cn/policies/technologies/cookies/"
           >隐私权
         </a></span></div></nav></footer></div><script async defer src="https://www.gstatic.com/feedback/api.js"></script><script src="../_static2/18908e5f48/jsi18n/index.html"></script><script src="../_static/18908e5f48/js/script_foot_closure__zh_cn.js"></script><script src="../_static/18908e5f48/js/script_foot.js"></script><script>
        (function($) {
          
          devsite.devsite.Init($, {'FULL_SITE_SEARCH_ENABLED': 0, 'ENABLE_BLOCKED_VIDEO_PLACEHOLDER': 1, 'VERSION_HASH': '18908e5f48', 'SITE_NAME': 'tensorflow_china', 'HISTORY_ENABLED': 0, 'ENABLE_BLOCKED_LINK_TOOLTIP': 1, 'ALLOWED_HOSTS': ['.android.com', '.anvato.com', '.apigee.com', '.appspot.com', '.dialogflow.com', '.gonglchuangl.net', '.google.cn', '.google.com', '.googleplex.com', '.nest.com', '.openthread.io', '.openweave.io', '.orbitera.com', '.tensorflow.org'], 'BLOCK_RSS_FEEDS': 1, 'SCRIPTSAFE_DOMAIN': 'tensorflow-dot-google-developers.gonglchuangl.net'},
                               '[]','zh-cn',
                               true, '',
                               {"f62218c009ec029abef196bba5aa34cf": true, "098dafe57affddc137df300142652cfd": false, "8e03e230de0bd8a6fe173fdf172e8b3f": true, "cb025a64a50094835616312f4774a53d": true, "51470233c56fc1fde50f00b73c52b216": false, "d169d485cf24243a263783dbe42029b1": true, "039e5d84b87fd75807ffb37b7f1bbf2c": true, "752953480de00a336d911a46966cc16d": false, "700def1a83e356c06c0925afb05de4b0": false, "6749dcb526ce9bde6993550c7d928d24": true}, '/',
                               'https://tensorflow.google.cn/');
        })(jQuery);

        
        devsite.localInit = function() {
          
        };

      </script><script>
      $('.devsite-utility-footer-language-select').each(function() {
        $(this).change(function(){$('.devsite-utility-footer-language').submit();});
      });
      </script></div><span id="devsite-request-elapsed" data-request-elapsed="269.469976425"></span></body></html>
